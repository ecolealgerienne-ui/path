# Ground Truth Evaluation Scripts

Scripts pour Ã©valuer la fidÃ©litÃ© clinique d'Optimus-Gate en comparant ses prÃ©dictions avec des annotations expertes (Ground Truth).

## ðŸ“‹ Vue d'ensemble

| Script | Description | Usage |
|--------|-------------|-------|
| `download_evaluation_datasets.py` | TÃ©lÃ©charge les datasets d'Ã©valuation | [Docs](#1-tÃ©lÃ©chargement-des-datasets) |
| `convert_annotations.py` | Convertit les annotations au format unifiÃ© | [Docs](#2-conversion-des-annotations) |
| `evaluate_ground_truth.py` | Ã‰value le modÃ¨le contre le GT | [Docs](#3-Ã©valuation) |

## ðŸŽ¯ Workflow complet

```bash
# 1. TÃ©lÃ©charger les datasets
python scripts/evaluation/download_evaluation_datasets.py --dataset consep

# 2. Convertir au format unifiÃ© (.npz)
python scripts/evaluation/convert_annotations.py \
    --dataset consep \
    --input_dir data/evaluation/consep/Test \
    --output_dir data/evaluation/consep_converted

# 3. Ã‰valuer le modÃ¨le
python scripts/evaluation/evaluate_ground_truth.py \
    --dataset_dir data/evaluation/consep_converted \
    --output_dir results/consep \
    --dataset consep

# 4. Consulter le rapport
cat results/consep/clinical_report_consep_*.txt
```

## 1. TÃ©lÃ©chargement des Datasets

### Afficher les datasets disponibles

```bash
python scripts/evaluation/download_evaluation_datasets.py --info
```

### TÃ©lÃ©charger CoNSeP (rapide, 70 MB)

```bash
python scripts/evaluation/download_evaluation_datasets.py --dataset consep
```

### TÃ©lÃ©charger PanNuke (lent, ~1.5 GB)

```bash
# Tous les folds
python scripts/evaluation/download_evaluation_datasets.py --dataset pannuke

# Seulement Fold 2 (pour validation)
python scripts/evaluation/download_evaluation_datasets.py \
    --dataset pannuke \
    --folds 2
```

## 2. Conversion des Annotations

### CoNSeP (.mat â†’ .npz)

```bash
python scripts/evaluation/convert_annotations.py \
    --dataset consep \
    --input_dir data/evaluation/consep/Test \
    --output_dir data/evaluation/consep_converted
```

### PanNuke (.npy â†’ .npz)

```bash
python scripts/evaluation/convert_annotations.py \
    --dataset pannuke \
    --input_dir "data/evaluation/pannuke/Fold 2" \
    --output_dir data/evaluation/pannuke_fold2_converted
```

### VÃ©rifier une conversion

```bash
python scripts/evaluation/convert_annotations.py \
    --verify data/evaluation/consep_converted/image_001.npz
```

## 3. Ã‰valuation

### Ã‰valuation complÃ¨te sur un dataset

```bash
python scripts/evaluation/evaluate_ground_truth.py \
    --dataset_dir data/evaluation/consep_converted \
    --output_dir results/consep \
    --dataset consep
```

### Ã‰valuation sur un sous-ensemble

```bash
# 100 images de PanNuke
python scripts/evaluation/evaluate_ground_truth.py \
    --dataset_dir data/evaluation/pannuke_fold2_converted \
    --output_dir results/pannuke_fold2 \
    --num_samples 100
```

### Ã‰valuation d'une seule image

```bash
python scripts/evaluation/evaluate_ground_truth.py \
    --image data/evaluation/consep_converted/test_001.npz \
    --output_dir results/single \
    --verbose
```

## ðŸ“Š RÃ©sultats

### Fichiers gÃ©nÃ©rÃ©s

| Fichier | Description |
|---------|-------------|
| `clinical_report_*.txt` | Rapport de fidÃ©litÃ© clinique (format texte) |
| `metrics_*.json` | MÃ©triques dÃ©taillÃ©es (format JSON) |
| `confusion_matrix_*.npy` | Matrice de confusion (format NumPy) |

### Exemple de rapport clinique

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               RAPPORT DE FIDÃ‰LITÃ‰ CLINIQUE                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Dice Global: 0.9601  |  AJI: 0.8234  |  PQ: 0.7891           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ DÃ‰TECTION                                                    â•‘
â•‘   TP:  180  |  FP:   12  |  FN:    8                        â•‘
â•‘   PrÃ©cision: 93.75%  |  Rappel: 95.74%                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ FIDÃ‰LITÃ‰ PAR TYPE CELLULAIRE                                 â•‘
â•‘   ðŸ”´ Neoplastic  : Expert= 20 â†’ ModÃ¨le= 19 â†’ 95.0%           â•‘
â•‘   ðŸŸ¢ Inflammatory: Expert= 15 â†’ ModÃ¨le= 14 â†’ 93.3%           â•‘
â•‘   ðŸ”µ Connective  : Expert=  8 â†’ ModÃ¨le=  8 â†’ 100.0%          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ CLASSIFICATION ACCURACY: 91.25%                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ðŸ“ MÃ©triques ExpliquÃ©es

| MÃ©trique | Formule | Ce qu'elle mesure | Cible |
|----------|---------|-------------------|-------|
| **Dice** | 2Ã—\|Pâˆ©GT\| / (\|P\|+\|GT\|) | Chevauchement binaire | > 0.95 |
| **AJI** | Î£ IoU_matched / (TP+FP+FN) | QualitÃ© des instances | > 0.80 |
| **PQ** | DQ Ã— SQ | Panoptic Quality | > 0.70 |
| **F1** | 2Ã—PrecisionÃ—Recall / (Prec+Rec) | Ã‰quilibre Prec/Rec | > 0.90 |

### Seuils de QualitÃ©

| Niveau | Dice | AJI | PQ | Statut |
|--------|------|-----|----|----|
| **Excellent** | â‰¥ 0.95 | â‰¥ 0.80 | â‰¥ 0.70 | âœ… Cible |
| **Acceptable** | â‰¥ 0.90 | â‰¥ 0.70 | â‰¥ 0.60 | ðŸŸ¡ OK |
| **Sous-optimal** | â‰¥ 0.85 | â‰¥ 0.60 | â‰¥ 0.50 | ðŸŸ  AmÃ©liorer |
| **Critique** | < 0.85 | < 0.60 | < 0.50 | ðŸ”´ ProblÃ¨me |

## ðŸŽ“ RÃ©fÃ©rences

- **PanNuke**: Gamper et al. (2019) - [Paper](https://arxiv.org/abs/2003.10778)
- **CoNSeP**: Graham et al. (2019) - [HoVer-Net](https://github.com/vqdang/hover_net)
- **AJI**: Kumar et al. (2017) - [Paper](https://ieeexplore.ieee.org/document/7872382)
- **Panoptic Quality**: Kirillov et al. (2019) - [Paper](https://arxiv.org/abs/1801.00868)

## ðŸ› DÃ©pannage

### Erreur: "No .npz files found"

VÃ©rifiez que la conversion a Ã©tÃ© effectuÃ©e :

```bash
ls -la data/evaluation/consep_converted/*.npz
```

### Erreur: "No 'inst_map' found in .mat"

Le fichier .mat doit contenir les clÃ©s `inst_map`, `type_map`, `inst_centroid`. VÃ©rifiez le format :

```python
import scipy.io as sio
data = sio.loadmat("file.mat")
print(data.keys())
```

### MÃ©moire insuffisante

RÃ©duisez le nombre d'images Ã©valuÃ©es :

```bash
python scripts/evaluation/evaluate_ground_truth.py \
    --dataset_dir ... \
    --num_samples 10  # Ã‰valuer seulement 10 images
```

## ðŸ“ Notes

- **Seuil IoU**: Par dÃ©faut 0.5 (norme communautÃ©). Ne pas changer sans raison.
- **Indexation**: `inst_map` commence Ã  1, pas 0 (0 = background).
- **Classes**: PanNuke utilise 1-5 pour les types (0 = background).
- **Mapping**: CoNSeP et MoNuSAC sont automatiquement mappÃ©s vers PanNuke.
