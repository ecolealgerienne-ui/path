#!/usr/bin/env python3
"""
Evaluate V13 Smart Crops Model on AJI Metric.

This script evaluates the trained V13 Smart Crops model on validation samples
and computes instance segmentation metrics (AJI, PQ) to verify improvement:
- V13 POC baseline: AJI = 0.57 (measured on TRAIN data - invalidated)
- V13 Smart Crops target: AJI ‚â• 0.68 (+18% improvement on VAL data)

The script uses HV-guided watershed for instance segmentation post-processing
with optimized parameters per organ (loaded from organ_config.py).

Usage:
    # Evaluate family with default params
    python scripts/evaluation/test_v13_smart_crops_aji.py \
        --checkpoint models/checkpoints_v13_smart_crops/hovernet_epidermal_v13_smart_crops_best.pth \
        --family epidermal \
        --n_samples 50

    # Evaluate organ with AUTO-LOADED optimized params
    python scripts/evaluation/test_v13_smart_crops_aji.py \
        --checkpoint models/checkpoints_v13_smart_crops/hovernet_respiratory_v13_smart_crops_hybrid_fpn_best.pth \
        --family respiratory \
        --organ Liver \
        --use_organ_params \
        --n_samples 100

Author: CellViT-Optimus Project
Date: 2025-12-27
"""

import sys
from pathlib import Path

# Add project root to PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import argparse
import numpy as np
import torch
import torch.nn as nn
import cv2
from typing import Dict
import json
from datetime import datetime
from tqdm import tqdm

from src.models.hovernet_decoder import HoVerNetDecoder
from src.metrics.ground_truth_metrics import compute_aji  # Centralized AJI
from src.postprocessing import hv_guided_watershed  # Single source of truth
from src.ui.organ_config import ORGAN_WATERSHED_PARAMS, ORGAN_TO_FAMILY


def compute_pq(pred_inst: np.ndarray, gt_inst: np.ndarray, iou_threshold: float = 0.5) -> Dict[str, float]:
    """
    Compute Panoptic Quality (PQ) metric.

    PQ = Detection Quality (DQ) √ó Segmentation Quality (SQ)
    """
    pred_ids = np.unique(pred_inst)
    pred_ids = pred_ids[pred_ids > 0]

    gt_ids = np.unique(gt_inst)
    gt_ids = gt_ids[gt_ids > 0]

    if len(gt_ids) == 0:
        return {"PQ": 1.0 if len(pred_ids) == 0 else 0.0, "DQ": 1.0, "SQ": 1.0, "TP": 0, "FP": len(pred_ids), "FN": 0}

    if len(pred_ids) == 0:
        return {"PQ": 0.0, "DQ": 0.0, "SQ": 0.0, "TP": 0, "FP": 0, "FN": len(gt_ids)}

    # Compute IoU matrix
    iou_matrix = np.zeros((len(gt_ids), len(pred_ids)))

    for i, gt_id in enumerate(gt_ids):
        gt_mask = (gt_inst == gt_id)

        for j, pred_id in enumerate(pred_ids):
            pred_mask = (pred_inst == pred_id)

            intersection = np.logical_and(gt_mask, pred_mask).sum()
            union = np.logical_or(gt_mask, pred_mask).sum()

            if union > 0:
                iou_matrix[i, j] = intersection / union

    # Match GT and predicted instances
    TP = 0
    sum_iou = 0.0
    matched_gt = set()
    matched_pred = set()

    for i in range(len(gt_ids)):
        for j in range(len(pred_ids)):
            if iou_matrix[i, j] >= iou_threshold:
                if i not in matched_gt and j not in matched_pred:
                    TP += 1
                    sum_iou += iou_matrix[i, j]
                    matched_gt.add(i)
                    matched_pred.add(j)

    FP = len(pred_ids) - TP
    FN = len(gt_ids) - TP

    # Detection Quality
    if TP + 0.5 * FP + 0.5 * FN == 0:
        DQ = 0.0
    else:
        DQ = TP / (TP + 0.5 * FP + 0.5 * FN)

    # Segmentation Quality
    SQ = (sum_iou / TP) if TP > 0 else 0.0

    # Panoptic Quality
    PQ = DQ * SQ

    return {
        "PQ": PQ,
        "DQ": DQ,
        "SQ": SQ,
        "TP": TP,
        "FP": FP,
        "FN": FN
    }


def compute_dice(pred: np.ndarray, gt: np.ndarray) -> float:
    """Compute Dice coefficient for binary masks."""
    pred_binary = (pred > 0.5).astype(bool)
    gt_binary = (gt > 0.5).astype(bool)

    intersection = np.logical_and(pred_binary, gt_binary).sum()
    union = pred_binary.sum() + gt_binary.sum()

    if union == 0:
        return 1.0

    return (2.0 * intersection / union)


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate V13 Smart Crops model on AJI metric"
    )
    parser.add_argument(
        "--checkpoint",
        required=True,
        type=Path,
        help="Path to trained V13 Smart Crops checkpoint"
    )
    parser.add_argument(
        "--family",
        required=True,
        choices=["glandular", "digestive", "urologic", "epidermal", "respiratory"],
        help="Family to evaluate"
    )
    parser.add_argument(
        "--organ",
        type=str,
        default=None,
        help="Specific organ (optional). Dynamic filtering from family data (no regeneration needed). Ex: Lung, Colon, Breast"
    )
    parser.add_argument(
        "--use_organ_params",
        action="store_true",
        help="Auto-load optimized watershed params from organ_config.py (requires --organ)"
    )
    parser.add_argument(
        "--n_samples",
        type=int,
        default=50,
        help="Number of validation samples to evaluate"
    )
    # === WATERSHED PARAMETERS (Optimized via grid search 2025-12-29) ===
    # Respiratory optimal: np_threshold=0.40, min_size=30, min_distance=5
    # Epidermal optimal:   np_threshold=0.45, min_size=40, min_distance=5
    parser.add_argument(
        "--np_threshold",
        type=float,
        default=0.40,
        help="NP binarization threshold (Respiratory=0.40, Epidermal=0.45)"
    )
    parser.add_argument(
        "--beta",
        type=float,
        default=0.50,
        help="HV magnitude exponent (optimal=0.50 for all families)"
    )
    parser.add_argument(
        "--min_size",
        type=int,
        default=30,
        help="Minimum instance size in pixels (Respiratory=30, Epidermal=40)"
    )
    parser.add_argument(
        "--min_distance",
        type=int,
        default=5,
        help="Minimum distance between peak markers (optimal=5 for all families)"
    )
    parser.add_argument(
        "--device",
        default="cuda",
        choices=["cuda", "cpu"]
    )
    parser.add_argument(
        "--use_hybrid",
        action="store_true",
        help="Use hybrid mode (RGB+H-channel injection)"
    )
    parser.add_argument(
        "--use_fpn_chimique",
        action="store_true",
        help="Use FPN Chimique (multi-scale H-injection at 5 levels)"
    )
    parser.add_argument(
        "--use_h_alpha",
        action="store_true",
        help="Use learnable alpha for H-channel amplification"
    )
    parser.add_argument(
        "--diagnose_outliers",
        action="store_true",
        help="Enable detailed outlier analysis for samples with low AJI"
    )
    parser.add_argument(
        "--outlier_threshold",
        type=float,
        default=0.50,
        help="AJI threshold below which samples are considered outliers (default: 0.50)"
    )
    args = parser.parse_args()

    # =========================================================================
    # AUTO-LOAD ORGAN-SPECIFIC WATERSHED PARAMS
    # =========================================================================
    if args.use_organ_params:
        if not args.organ:
            print("‚ùå ERROR: --use_organ_params requires --organ to be specified")
            return 1

        if args.organ not in ORGAN_WATERSHED_PARAMS:
            print(f"‚ùå ERROR: No optimized params found for organ '{args.organ}'")
            print(f"  Available organs: {list(ORGAN_WATERSHED_PARAMS.keys())}")
            return 1

        # Override CLI params with optimized organ-specific params
        organ_params = ORGAN_WATERSHED_PARAMS[args.organ]
        args.np_threshold = organ_params["np_threshold"]
        args.min_size = organ_params["min_size"]
        args.beta = organ_params["beta"]
        args.min_distance = organ_params["min_distance"]

        # Validate organ belongs to family
        expected_family = ORGAN_TO_FAMILY.get(args.organ)
        if expected_family and expected_family != args.family:
            print(f"‚ö†Ô∏è  WARNING: Organ '{args.organ}' belongs to family '{expected_family}'")
            print(f"  But you specified --family {args.family}")

    device = torch.device(args.device)
    n_classes = 5

    print("=" * 80)
    print("EVALUATION V13 SMART CROPS - AJI METRIC")
    print("=" * 80)
    print(f"\nConfiguration:")
    print(f"  Checkpoint: {args.checkpoint}")
    print(f"  Family: {args.family}")
    print(f"  N samples: {args.n_samples}")
    print(f"  NP threshold: {args.np_threshold}")
    print(f"  Watershed beta: {args.beta}")
    print(f"  Watershed min_size: {args.min_size}")
    print(f"  Watershed min_distance: {args.min_distance}")
    if args.use_organ_params:
        print(f"  ‚úÖ Using OPTIMIZED organ params from organ_config.py")
    print(f"  Hybrid mode: {args.use_hybrid} (H-channel injection)")
    print(f"  FPN Chimique: {args.use_fpn_chimique} (multi-scale H @ 16,32,64,112,224)")
    print(f"  Device: {args.device}")
    if args.organ:
        print(f"  Organ filter: {args.organ} (dynamic filtering)")
    if args.diagnose_outliers:
        print(f"  Outlier diagnosis: ENABLED (threshold={args.outlier_threshold})")

    # Load model
    print("\n" + "=" * 80)
    print("LOADING MODEL")
    print("=" * 80)

    # Check checkpoint for hybrid/fpn mode consistency
    checkpoint = torch.load(args.checkpoint, map_location=device)
    checkpoint_use_hybrid = checkpoint.get('use_hybrid', False)
    checkpoint_use_fpn = checkpoint.get('use_fpn_chimique', False)

    if args.use_hybrid != checkpoint_use_hybrid:
        print(f"  ‚ö†Ô∏è  WARNING: Checkpoint trained with use_hybrid={checkpoint_use_hybrid}")
        print(f"  ‚ö†Ô∏è  But evaluation requested use_hybrid={args.use_hybrid}")
        print(f"  ‚ö†Ô∏è  Using checkpoint setting: use_hybrid={checkpoint_use_hybrid}")
        args.use_hybrid = checkpoint_use_hybrid

    if args.use_fpn_chimique != checkpoint_use_fpn:
        print(f"  ‚ö†Ô∏è  WARNING: Checkpoint trained with use_fpn_chimique={checkpoint_use_fpn}")
        print(f"  ‚ö†Ô∏è  But evaluation requested use_fpn_chimique={args.use_fpn_chimique}")
        print(f"  ‚ö†Ô∏è  Using checkpoint setting: use_fpn_chimique={checkpoint_use_fpn}")
        args.use_fpn_chimique = checkpoint_use_fpn

    # Detect use_h_alpha from checkpoint (check if h_alphas params exist)
    checkpoint_use_h_alpha = any('h_alphas' in k for k in checkpoint['model_state_dict'].keys())
    if checkpoint_use_h_alpha:
        print(f"  ‚úÖ Checkpoint contains h_alphas parameters (use_h_alpha=True)")
        args.use_h_alpha = True

    model = HoVerNetDecoder(
        embed_dim=1536,
        n_classes=n_classes,
        dropout=0.1,
        use_hybrid=args.use_hybrid,
        use_fpn_chimique=args.use_fpn_chimique,
        use_h_alpha=args.use_h_alpha
    ).to(device)

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"  ‚úÖ Checkpoint loaded (epoch {checkpoint['epoch']})")
    print(f"  Best Dice: {checkpoint.get('best_dice', 'N/A')}")
    if args.use_hybrid:
        print(f"  ‚úÖ Mode HYBRID activ√©: injection H-channel via RuifrokExtractor")
    if args.use_fpn_chimique:
        print(f"  ‚úÖ Mode FPN CHIMIQUE activ√©: injection H multi-√©chelle (16‚Üí32‚Üí64‚Üí112‚Üí224)")

    # Load validation data (targets + inst_maps)
    print("\n" + "=" * 80)
    print("LOADING VALIDATION DATA")
    print("=" * 80)

    # Toujours charger depuis le fichier famille
    val_data_path = Path(f"data/family_data_v13_smart_crops/{args.family}_val_v13_smart_crops.npz")
    if not val_data_path.exists():
        print(f"‚ùå ERROR: {val_data_path} not found")
        print(f"\nRun first:")
        print(f"  python scripts/preprocessing/prepare_v13_smart_crops.py --family {args.family}")
        return 1

    print(f"Loading {val_data_path.name}...")
    val_data_raw = np.load(val_data_path, allow_pickle=True)

    # Load PRE-EXTRACTED features (always from family file)
    features_path = Path(f"data/cache/family_data/{args.family}_rgb_features_v13_smart_crops_val.npz")
    if not features_path.exists():
        print(f"‚ùå ERROR: {features_path} not found")
        print(f"\nRun first:")
        print(f"  python scripts/preprocessing/extract_features_v13_smart_crops.py --family {args.family} --split val")
        return 1

    print(f"Loading {features_path.name}...")
    features_data = np.load(features_path)
    all_features_raw = features_data['features']  # (N_val, 261, 1536)

    # =========================================================================
    # FILTRAGE DYNAMIQUE PAR ORGANE (sans r√©g√©n√©ration des donn√©es)
    # =========================================================================
    if args.organ:
        organ_names = val_data_raw.get('organ_names', None)
        if organ_names is None:
            print(f"‚ùå ERROR: organ_names not found in {val_data_path}")
            print(f"  Regenerate data with latest prepare_v13_smart_crops.py")
            return 1

        # Normaliser les noms (g√©rer bytes vs str)
        organ_names = np.array([
            name.decode('utf-8') if isinstance(name, bytes) else name
            for name in organ_names
        ])

        # Cr√©er le masque de filtre
        organ_mask = organ_names == args.organ
        n_organ = organ_mask.sum()

        if n_organ == 0:
            available_organs = np.unique(organ_names)
            print(f"‚ùå ERROR: No samples found for organ '{args.organ}'")
            print(f"  Available organs in {args.family}: {list(available_organs)}")
            return 1

        print(f"  üî¨ Filtrage organe: {args.organ} ‚Üí {n_organ}/{len(organ_names)} samples")

        # Appliquer le masque
        np_targets = val_data_raw['np_targets'][organ_mask]
        hv_targets = val_data_raw['hv_targets'][organ_mask]
        inst_maps = val_data_raw['inst_maps'][organ_mask]
        all_features = all_features_raw[organ_mask]

        if args.use_hybrid:
            if 'images' in val_data_raw.files:
                all_images = val_data_raw['images'][organ_mask]
                print(f"  ‚úÖ Images RGB filtr√©es: shape {all_images.shape}")
            else:
                print(f"‚ùå ERROR: Mode hybride activ√© mais 'images' non trouv√©es")
                return 1
        else:
            all_images = None
    else:
        # Mode famille: utiliser toutes les donn√©es
        np_targets = val_data_raw['np_targets']
        hv_targets = val_data_raw['hv_targets']
        inst_maps = val_data_raw['inst_maps']
        all_features = all_features_raw

        if args.use_hybrid:
            if 'images' in val_data_raw.files:
                all_images = val_data_raw['images']
                print(f"  ‚úÖ Images RGB charg√©es: shape {all_images.shape}, dtype {all_images.dtype}")
            else:
                print(f"‚ùå ERROR: Mode hybride activ√© mais 'images' non trouv√©es dans {val_data_path}")
                return 1
        else:
            all_images = None

    n_total = len(all_features)
    n_to_eval = min(args.n_samples, n_total)

    print(f"  ‚Üí {n_total} validation crops available")
    print(f"  ‚Üí Evaluating first {n_to_eval} samples")
    print(f"  ‚úÖ Using PRE-EXTRACTED features (same as training)")
    print(f"  ‚úÖ Using TRUE instance maps (not watershed reconstruction)")

    # Use true instance maps from data
    gt_instances = inst_maps[:n_to_eval]

    print(f"\n‚úÖ Data loaded: {len(gt_instances)} samples")

    # Evaluation
    print("\n" + "=" * 80)
    print("EVALUATION")
    print("=" * 80)

    all_dice = []
    all_aji = []
    all_aji_fair = []  # Fair AJI using same watershed for both
    all_pq = []
    n_pred_instances = []
    n_gt_instances = []

    # Per-sample details for outlier analysis
    sample_details = []

    for i in tqdm(range(n_to_eval), desc="Evaluating"):
        gt_inst = gt_instances[i]

        # Use PRE-EXTRACTED features (same as training!)
        features = torch.from_numpy(all_features[i]).unsqueeze(0).float().to(device)  # (1, 261, 1536)

        # Image RGB pour mode hybride
        if args.use_hybrid:
            # Convertir image HWC uint8 ‚Üí CHW float32 [0, 255]
            image = torch.from_numpy(all_images[i]).permute(2, 0, 1).unsqueeze(0).float().to(device)
        else:
            image = None

        # Forward through decoder
        with torch.no_grad():
            np_out, hv_out, nt_out = model(features, images_rgb=image)  # (1, 2, 224, 224), (1, 2, 224, 224), (1, 5, 224, 224)

            # Convert to numpy - USE SOFTMAX (not sigmoid!) for CrossEntropyLoss
            np_probs = torch.softmax(np_out, dim=1).cpu().numpy()[0]  # (2, 224, 224)
            np_pred = np_probs[1]  # Canal 1 = Noyaux (224, 224)
            hv_pred = hv_out.cpu().numpy()[0]  # (2, 224, 224)

        # Watershed post-processing
        pred_inst = hv_guided_watershed(
            np_pred,
            hv_pred,
            np_threshold=args.np_threshold,
            beta=args.beta,
            min_size=args.min_size,
            min_distance=args.min_distance
        )

        # Metrics
        dice = compute_dice(np_pred, np_targets[i])
        aji = compute_aji(pred_inst, gt_inst)
        pq_metrics = compute_pq(pred_inst, gt_inst)

        # DEBUG: Compute "fair AJI" using same method for both
        # Reconstruct GT instances using SAME watershed method
        gt_hv = hv_targets[i]  # (2, 224, 224)
        gt_inst_watershed = hv_guided_watershed(
            np_targets[i],  # Use GT NP as binary mask
            gt_hv,          # Use GT HV for guidance
            np_threshold=0.5,  # GT is already binary
            beta=args.beta,
            min_size=args.min_size,
            min_distance=args.min_distance
        )
        aji_fair = compute_aji(pred_inst, gt_inst_watershed)

        # Count instances
        n_pred = len(np.unique(pred_inst)) - 1  # -1 for background
        n_gt = len(np.unique(gt_inst)) - 1

        # Compute HV magnitude on predicted nuclei regions
        hv_magnitude = np.sqrt(hv_pred[0]**2 + hv_pred[1]**2)
        nuclei_mask = np_pred > args.np_threshold
        hv_mag_mean = float(hv_magnitude[nuclei_mask].mean()) if nuclei_mask.sum() > 0 else 0.0

        # Nuclear coverage (% of image that is nuclei)
        np_coverage = float(nuclei_mask.sum()) / (224 * 224) * 100

        all_dice.append(dice)
        all_aji.append(aji)
        all_aji_fair.append(aji_fair)
        all_pq.append(pq_metrics['PQ'])
        n_pred_instances.append(n_pred)
        n_gt_instances.append(n_gt)

        # Store sample details for outlier analysis
        sample_details.append({
            'index': i,
            'aji': aji,
            'dice': dice,
            'pq': pq_metrics['PQ'],
            'n_gt': n_gt,
            'n_pred': n_pred,
            'hv_mag_mean': hv_mag_mean,
            'np_coverage': np_coverage,
            'tp': pq_metrics['TP'],
            'fp': pq_metrics['FP'],
            'fn': pq_metrics['FN']
        })

    # Results
    print("\n" + "=" * 80)
    print("RESULTS")
    print("=" * 80)

    mean_dice = np.mean(all_dice)
    std_dice = np.std(all_dice)
    mean_aji = np.mean(all_aji)
    std_aji = np.std(all_aji)
    median_aji = np.median(all_aji)
    mean_aji_fair = np.mean(all_aji_fair)
    std_aji_fair = np.std(all_aji_fair)
    mean_pq = np.mean(all_pq)
    std_pq = np.std(all_pq)
    mean_n_pred = np.mean(n_pred_instances)
    mean_n_gt = np.mean(n_gt_instances)
    over_seg_ratio = mean_n_pred / mean_n_gt if mean_n_gt > 0 else 0.0

    print(f"\nüìä M√âTRIQUES GLOBALES (n={n_to_eval}):")
    print(f"\n  Dice:        {mean_dice:.4f} ¬± {std_dice:.4f}")
    print(f"  AJI:         {mean_aji:.4f} ¬± {std_aji:.4f}")
    print(f"  AJI Median:  {median_aji:.4f}")
    print(f"  AJI FAIR:    {mean_aji_fair:.4f} ¬± {std_aji_fair:.4f}  ‚Üê Same watershed for GT & Pred")
    print(f"  PQ:          {mean_pq:.4f} ¬± {std_pq:.4f}")
    print(f"\n  Instances pred: {mean_n_pred:.1f}")
    print(f"  Instances GT:   {mean_n_gt:.1f}")
    print(f"  Over-seg ratio: {over_seg_ratio:.2f}√ó")

    # Diagnostic
    if mean_aji_fair > mean_aji + 0.05:
        print(f"\n‚ö†Ô∏è  DIAGNOSTIC: AJI FAIR (+{mean_aji_fair - mean_aji:.2f}) > AJI standard")
        print(f"   ‚Üí Le probl√®me vient de la M√âTHODE de construction des GT instances")
        print(f"   ‚Üí scipy.ndimage.label() produit des fronti√®res diff√©rentes du watershed")
        print(f"   ‚Üí Consid√©rez utiliser watershed pour les GT aussi (coh√©rence)")

    # Outlier Analysis (if enabled)
    if args.diagnose_outliers:
        print("\n" + "=" * 80)
        print("üîç OUTLIER ANALYSIS")
        print("=" * 80)

        # Find outliers (samples below threshold)
        outliers = [s for s in sample_details if s['aji'] < args.outlier_threshold]
        outliers_sorted = sorted(outliers, key=lambda x: x['aji'])

        print(f"\nThreshold: AJI < {args.outlier_threshold}")
        print(f"Outliers found: {len(outliers)} / {n_to_eval} samples ({len(outliers)/n_to_eval*100:.1f}%)")

        if outliers:
            # Compute impact on mean
            non_outlier_ajis = [s['aji'] for s in sample_details if s['aji'] >= args.outlier_threshold]
            mean_without_outliers = np.mean(non_outlier_ajis) if non_outlier_ajis else 0
            print(f"Mean AJI without outliers: {mean_without_outliers:.4f} (vs {mean_aji:.4f} with)")

            print(f"\n{'‚îÄ'*80}")
            print(f"{'Idx':<5} {'AJI':<7} {'Dice':<6} {'GT':<5} {'Pred':<5} {'Œî':<6} {'Type':<12} {'HV_mag':<7} {'NP%':<6}")
            print(f"{'‚îÄ'*80}")

            for s in outliers_sorted[:15]:  # Show top 15 worst
                delta = s['n_pred'] - s['n_gt']
                if delta < -2:
                    seg_type = "UNDER-SEG"
                elif delta > 2:
                    seg_type = "OVER-SEG"
                else:
                    seg_type = "boundary"

                print(f"{s['index']:<5} {s['aji']:<7.4f} {s['dice']:<6.3f} {s['n_gt']:<5} {s['n_pred']:<5} {delta:+<5} {seg_type:<12} {s['hv_mag_mean']:<7.3f} {s['np_coverage']:<6.1f}")

            print(f"{'‚îÄ'*80}")

            # Analyze patterns
            under_seg = sum(1 for s in outliers if s['n_pred'] < s['n_gt'] - 2)
            over_seg = sum(1 for s in outliers if s['n_pred'] > s['n_gt'] + 2)
            boundary = len(outliers) - under_seg - over_seg

            print(f"\nüìä PATTERN ANALYSIS:")
            print(f"  Under-segmentation (pred << gt): {under_seg} ({under_seg/len(outliers)*100:.0f}%)")
            print(f"  Over-segmentation (pred >> gt):  {over_seg} ({over_seg/len(outliers)*100:.0f}%)")
            print(f"  Boundary issues (|Œî| ‚â§ 2):       {boundary} ({boundary/len(outliers)*100:.0f}%)")

            # HV magnitude analysis
            outlier_hv_mags = [s['hv_mag_mean'] for s in outliers]
            good_samples = [s for s in sample_details if s['aji'] >= 0.65]
            good_hv_mags = [s['hv_mag_mean'] for s in good_samples] if good_samples else [0]

            print(f"\nüìä HV MAGNITUDE COMPARISON:")
            print(f"  Outliers mean:     {np.mean(outlier_hv_mags):.4f}")
            print(f"  Good samples mean: {np.mean(good_hv_mags):.4f}")

            if np.mean(outlier_hv_mags) < np.mean(good_hv_mags) * 0.8:
                print(f"  ‚ö†Ô∏è  Outliers have weak HV gradients ‚Üí Watershed struggles to separate")

            # Nuclear density analysis
            outlier_coverage = [s['np_coverage'] for s in outliers]
            good_coverage = [s['np_coverage'] for s in good_samples] if good_samples else [0]

            print(f"\nüìä NUCLEAR DENSITY:")
            print(f"  Outliers mean coverage:     {np.mean(outlier_coverage):.1f}%")
            print(f"  Good samples mean coverage: {np.mean(good_coverage):.1f}%")

            if np.mean(outlier_coverage) > np.mean(good_coverage) * 1.3:
                print(f"  ‚ö†Ô∏è  Outliers are dense patches ‚Üí Consider reducing min_distance")

        else:
            print(f"\n‚úÖ No outliers found! All samples have AJI ‚â• {args.outlier_threshold}")

    # Verdict
    print("\n" + "=" * 80)
    print("VERDICT")
    print("=" * 80)

    target_aji = 0.68
    if mean_aji >= target_aji:
        verdict = "‚úÖ OBJECTIF ATTEINT"
        print(f"\n{verdict}")
        print(f"  AJI = {mean_aji:.4f} ‚â• {target_aji} (+{((mean_aji / 0.57) - 1) * 100:.1f}% vs baseline 0.57)")
    else:
        verdict = "‚ö†Ô∏è OBJECTIF NON ATTEINT"
        gap = target_aji - mean_aji
        print(f"\n{verdict}")
        print(f"  AJI = {mean_aji:.4f} < {target_aji} (√©cart: -{gap:.4f})")
        print(f"  Progress: {(mean_aji / target_aji) * 100:.1f}% of target")

    if median_aji > 0.80:
        print(f"\n‚úÖ Note positive: M√©diane AJI = {median_aji:.4f} d√©montre excellente capacit√©")

    # Save results
    results = {
        "checkpoint": str(args.checkpoint),
        "family": args.family,
        "organ": args.organ,
        "n_samples": n_to_eval,
        "use_hybrid": args.use_hybrid,
        "watershed_params": {
            "np_threshold": args.np_threshold,
            "beta": args.beta,
            "min_size": args.min_size,
            "min_distance": args.min_distance
        },
        "metrics": {
            "dice_mean": float(mean_dice),
            "dice_std": float(std_dice),
            "aji_mean": float(mean_aji),
            "aji_std": float(std_aji),
            "aji_median": float(median_aji),
            "pq_mean": float(mean_pq),
            "pq_std": float(std_pq),
            "over_seg_ratio": float(over_seg_ratio),
            "n_pred_mean": float(mean_n_pred),
            "n_gt_mean": float(mean_n_gt)
        },
        "verdict": verdict,
        "target_aji": target_aji,
        "timestamp": datetime.now().isoformat()
    }

    # Add outlier analysis to results if enabled
    if args.diagnose_outliers:
        outliers = [s for s in sample_details if s['aji'] < args.outlier_threshold]
        results["outlier_analysis"] = {
            "threshold": args.outlier_threshold,
            "n_outliers": len(outliers),
            "outlier_indices": [s['index'] for s in outliers],
            "outliers": sorted(outliers, key=lambda x: x['aji']),
            "all_samples": sample_details
        }

    results_dir = Path("results/v13_smart_crops")
    results_dir.mkdir(parents=True, exist_ok=True)

    # Nom du fichier: organe si sp√©cifi√©, sinon famille
    output_prefix = args.organ.lower() if args.organ else args.family
    suffix = "_hybrid" if args.use_hybrid else ""
    results_file = results_dir / f"{output_prefix}{suffix}_aji_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nüìÑ Results saved: {results_file}")

    print("\n" + "=" * 80)
    print("‚úÖ EVALUATION COMPLETE")
    print("=" * 80)

    return 0


if __name__ == "__main__":
    sys.exit(main())
