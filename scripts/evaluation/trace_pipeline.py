#!/usr/bin/env python3
"""
Trace COMPLET du pipeline train vs test pour diagnostiquer HV MSE.

Compare:
1. Chargement donn√©es
2. Preprocessing
3. Entr√©e mod√®le
4. Sortie mod√®le
5. Calcul m√©triques
"""

import sys
from pathlib import Path
import numpy as np
import torch
import torch.nn.functional as F

PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.models.loader import ModelLoader


def print_tensor_info(name: str, data, level=0):
    """Affiche info d√©taill√©e sur un tensor/array."""
    indent = "  " * level
    if isinstance(data, torch.Tensor):
        data = data.detach().cpu()
        dtype_str = str(data.dtype)
    elif isinstance(data, np.ndarray):
        dtype_str = str(data.dtype)
    else:
        print(f"{indent}{name}: type={type(data)}")
        return

    print(f"{indent}{name}:")
    print(f"{indent}  shape: {data.shape if hasattr(data, 'shape') else 'N/A'}")
    print(f"{indent}  dtype: {dtype_str}")

    if hasattr(data, 'min'):
        data_flat = data.flatten() if hasattr(data, 'flatten') else data
        # Convert to float for mean/std if needed (int64 doesn't support these ops)
        if hasattr(data_flat, 'dtype') and 'int' in str(data_flat.dtype):
            data_flat_f = data_flat.float() if isinstance(data_flat, torch.Tensor) else data_flat.astype(np.float32)
        else:
            data_flat_f = data_flat
        print(f"{indent}  range: [{float(data_flat.min()):.6f}, {float(data_flat.max()):.6f}]")
        print(f"{indent}  mean:  {float(data_flat_f.mean()):.6f}")
        print(f"{indent}  std:   {float(data_flat_f.std()):.6f}")


def trace_train_pipeline():
    """Trace le pipeline TRAIN."""
    print("=" * 80)
    print("PIPELINE TRAIN")
    print("=" * 80)
    print()

    # 1. Chargement donn√©es
    print("=" * 80)
    print("√âTAPE 1: Chargement .npz")
    print("=" * 80)

    features_path = PROJECT_ROOT / "data/cache/family_data_FIXED/digestive_features.npz"
    targets_path = PROJECT_ROOT / "data/cache/family_data_FIXED/digestive_targets.npz"

    features_data = np.load(features_path)
    targets_data = np.load(targets_path)

    features = features_data['features']
    np_targets = targets_data['np_targets']
    hv_targets_raw = targets_data['hv_targets']
    nt_targets = targets_data['nt_targets']

    print("\nFeatures (.npz):")
    print_tensor_info("features", features, level=1)

    print("\nTargets (.npz):")
    print_tensor_info("np_targets", np_targets, level=1)
    print_tensor_info("hv_targets_raw", hv_targets_raw, level=1)
    print_tensor_info("nt_targets", nt_targets, level=1)

    # 2. DataLoader conversion (ce que fait FamilyHoVerDataset.__init__)
    print()
    print("=" * 80)
    print("√âTAPE 2: Conversion DataLoader (FamilyHoVerDataset.__init__)")
    print("=" * 80)

    # Check dtype et conversion comme dans train script
    if hv_targets_raw.dtype == np.int8:
        hv_targets = hv_targets_raw.astype(np.float32) / 127.0
        print("\n‚ö†Ô∏è  HV format OLD d√©tect√© (int8) - conversion en float32")
    else:
        hv_targets = hv_targets_raw
        print("\n‚úÖ HV format NEW d√©tect√© (float32) - pas de conversion")

    print_tensor_info("hv_targets (apr√®s conversion)", hv_targets, level=1)

    # 3. __getitem__ (premier √©chantillon)
    print()
    print("=" * 80)
    print("√âTAPE 3: DataLoader __getitem__ (√©chantillon 0)")
    print("=" * 80)

    feat_sample = features[0]  # (261, 1536)
    np_sample = np_targets[0]  # (224, 224)
    hv_sample = hv_targets[0]  # (2, 224, 224)
    nt_sample = nt_targets[0]  # (224, 224)

    print_tensor_info("features[0]", feat_sample, level=1)
    print_tensor_info("np_targets[0]", np_sample, level=1)
    print_tensor_info("hv_targets[0]", hv_sample, level=1)
    print_tensor_info("nt_targets[0]", nt_sample, level=1)

    # 4. Conversion torch + RESIZE 256‚Üí224 (ce que fait DataLoader.__getitem__)
    print()
    print("=" * 80)
    print("√âTAPE 4: Resize 256‚Üí224 + Conversion torch.Tensor (DataLoader)")
    print("=" * 80)

    feat_t = torch.from_numpy(feat_sample).unsqueeze(0)  # (1, 261, 1536)

    # Resize targets 256‚Üí224 (train_hovernet_family.py lignes 172-183)
    np_target_t = torch.from_numpy(np_sample)
    hv_target_t = torch.from_numpy(hv_sample)
    nt_target_t = torch.from_numpy(nt_sample)

    np_t = F.interpolate(np_target_t.unsqueeze(0).unsqueeze(0),
                         size=(224, 224), mode='nearest').squeeze(0)  # (1, 224, 224)
    hv_t = F.interpolate(hv_target_t.unsqueeze(0),
                         size=(224, 224), mode='bilinear',
                         align_corners=False)  # (1, 2, 224, 224)
    nt_t = F.interpolate(nt_target_t.float().unsqueeze(0).unsqueeze(0),
                         size=(224, 224), mode='nearest').squeeze(0).long()  # (1, 224, 224)

    print_tensor_info("features (batch)", feat_t, level=1)
    print_tensor_info("np_target 224√ó224 (batch)", np_t, level=1)
    print_tensor_info("hv_target 224√ó224 (batch)", hv_t, level=1)
    print_tensor_info("nt_target 224√ó224 (batch)", nt_t, level=1)

    # 5. Entr√©e mod√®le
    print()
    print("=" * 80)
    print("√âTAPE 5: Entr√©e mod√®le (patch_tokens)")
    print("=" * 80)

    patch_tokens = feat_t[:, 1:257, :]  # Enlever CLS + registers
    print_tensor_info("patch_tokens", patch_tokens, level=1)

    # 6. Sortie mod√®le
    print()
    print("=" * 80)
    print("√âTAPE 6: Sortie mod√®le")
    print("=" * 80)

    checkpoint = PROJECT_ROOT / "models/checkpoints/hovernet_digestive_best.pth"
    if checkpoint.exists():
        print(f"‚ö†Ô∏è  Checkpoint trouv√©: {checkpoint}")
        hovernet = ModelLoader.load_hovernet(checkpoint, device='cpu')
    else:
        print("‚úÖ Pas de checkpoint - mod√®le random init")
        from src.models.hovernet_decoder import HoVerNetDecoder
        hovernet = HoVerNetDecoder(n_classes=6, dropout=0.1)

    hovernet.eval()

    with torch.no_grad():
        np_out, hv_out, nt_out = hovernet(patch_tokens)

    print_tensor_info("np_out (AVANT activation)", np_out, level=1)
    print_tensor_info("hv_out (APR√àS tanh)", hv_out, level=1)
    print_tensor_info("nt_out (AVANT activation)", nt_out, level=1)

    # 7. Calcul m√©triques (comme dans train script)
    print()
    print("=" * 80)
    print("√âTAPE 7: Calcul m√©triques TRAIN")
    print("=" * 80)

    # HV MSE calculation (from train_hovernet_family.py compute_hv_mse)
    mask = np_t.float().unsqueeze(1)  # (1, 1, 224, 224)

    if mask.sum() == 0:
        hv_mse = 0.0
    else:
        diff = (hv_out - hv_t) ** 2
        masked_diff = diff * mask
        hv_mse = (masked_diff.sum() / (mask.sum() * 2)).item()

    print(f"\nHV MSE (train method): {hv_mse:.6f}")
    print(f"  Mask pixels: {mask.sum().item():.0f}")
    print(f"  Total pixels: {np.prod(hv_out.shape)}")

    return hv_out, hv_t, mask


def trace_test_pipeline():
    """Trace le pipeline TEST."""
    print()
    print()
    print("=" * 80)
    print("PIPELINE TEST")
    print("=" * 80)
    print()

    # 1. Chargement donn√©es
    print("=" * 80)
    print("√âTAPE 1: Chargement .npz")
    print("=" * 80)

    features_path = PROJECT_ROOT / "data/cache/family_data_FIXED/digestive_features.npz"
    targets_path = PROJECT_ROOT / "data/cache/family_data_FIXED/digestive_targets.npz"

    features_data = np.load(features_path)
    targets_data = np.load(targets_path)

    features = features_data['features']
    np_targets = targets_data['np_targets']
    hv_targets = targets_data['hv_targets']  # PAS de conversion ici dans test script
    nt_targets = targets_data['nt_targets']

    print("\nFeatures (.npz):")
    print_tensor_info("features", features, level=1)

    print("\nTargets (.npz) - SANS conversion:")
    print_tensor_info("hv_targets (brut)", hv_targets, level=1)

    # 2. √âchantillon 0
    print()
    print("=" * 80)
    print("√âTAPE 2: √âchantillon 0 (test script)")
    print("=" * 80)

    feat = features[0]
    np_target_256 = np_targets[0]
    hv_target_256 = hv_targets[0]
    nt_target_256 = nt_targets[0]

    print_tensor_info("features[0]", feat, level=1)
    print_tensor_info("hv_target_256[0]", hv_target_256, level=1)

    # 3. Resize targets (test script fait resize 256‚Üí224)
    print()
    print("=" * 80)
    print("√âTAPE 3: Resize targets 256‚Üí224 (test script)")
    print("=" * 80)

    # Resize comme dans test script
    np_target_t = torch.from_numpy(np_target_256).float().unsqueeze(0).unsqueeze(0)
    hv_target_t = torch.from_numpy(hv_target_256).float().unsqueeze(0)

    hv_target_t = F.interpolate(hv_target_t, size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)
    hv_target_224 = hv_target_t.numpy()

    print_tensor_info("hv_target_224 (apr√®s resize)", hv_target_224, level=1)

    # 4. Inf√©rence
    print()
    print("=" * 80)
    print("√âTAPE 4: Inf√©rence (test script)")
    print("=" * 80)

    feat_tensor = torch.from_numpy(feat).unsqueeze(0)
    patch_tokens = feat_tensor[:, 1:257, :]

    checkpoint = PROJECT_ROOT / "models/checkpoints/hovernet_digestive_best.pth"
    if checkpoint.exists():
        hovernet = ModelLoader.load_hovernet(checkpoint, device='cpu')
    else:
        from src.models.hovernet_decoder import HoVerNetDecoder
        hovernet = HoVerNetDecoder(n_classes=6, dropout=0.1)

    hovernet.eval()

    with torch.no_grad():
        np_out, hv_out, nt_out = hovernet(patch_tokens)

    hv_pred = hv_out.cpu().numpy()[0]  # (2, 224, 224)

    print_tensor_info("hv_pred (test)", hv_pred, level=1)

    # 5. Calcul m√©triques (test script)
    print()
    print("=" * 80)
    print("√âTAPE 5: Calcul m√©triques TEST")
    print("=" * 80)

    # Resize np_target pour avoir le mask √† 224√ó224 (comme dans le vrai test script ligne 159)
    np_target_t = torch.from_numpy(np_target_256).float().unsqueeze(0).unsqueeze(0)
    np_target_224 = F.interpolate(np_target_t, size=(224, 224), mode='nearest').squeeze().numpy()

    mask = np_target_224 > 0  # Mask √† 224√ó224 maintenant

    if mask.sum() > 0:
        # Test script calcule MSE comme √ßa (compute_mse function)
        hv_mse = ((hv_pred[:, mask] - hv_target_224[:, mask]) ** 2).mean()
    else:
        hv_mse = 0.0

    print(f"\nHV MSE (test method): {hv_mse:.6f}")
    print(f"  Mask pixels: {mask.sum():.0f}")
    print(f"  Total pixels: {np.prod(hv_pred.shape)}")

    return hv_pred, hv_target_224, mask


def compare_pipelines():
    """Compare les deux pipelines."""
    print()
    print()
    print("=" * 80)
    print("COMPARAISON TRAIN vs TEST")
    print("=" * 80)
    print()

    hv_out_train, hv_t_train, mask_train = trace_train_pipeline()
    hv_pred_test, hv_target_test, mask_test = trace_test_pipeline()

    print()
    print("=" * 80)
    print("ANALYSE DES DIFF√âRENCES")
    print("=" * 80)
    print()

    # Comparaison des sorties HV
    print("1. SORTIES MOD√àLE HV:")
    print(f"   TRAIN: range [{hv_out_train.min():.6f}, {hv_out_train.max():.6f}], std {hv_out_train.std():.6f}")
    print(f"   TEST:  range [{hv_pred_test.min():.6f}, {hv_pred_test.max():.6f}], std {hv_pred_test.std():.6f}")
    print(f"   ‚Üí Identiques? {torch.allclose(hv_out_train, torch.from_numpy(hv_pred_test))}")
    print()

    # Comparaison des targets HV
    print("2. TARGETS HV (apr√®s resize 224√ó224):")
    print(f"   TRAIN: range [{hv_t_train.min():.6f}, {hv_t_train.max():.6f}], std {hv_t_train.std():.6f}")
    print(f"   TEST:  range [{hv_target_test.min():.6f}, {hv_target_test.max():.6f}], std {hv_target_test.std():.6f}")
    print(f"   ‚Üí Identiques? {np.allclose(hv_t_train.numpy(), hv_target_test)}")
    print()

    # Comparaison m√©thode de calcul
    print("3. M√âTHODE CALCUL MSE:")
    print("   TRAIN: masked_diff.sum() / (mask.sum() * 2)")
    print("   TEST:  ((pred[:, mask] - target[:, mask]) ** 2).mean()")
    print()

    # Recalculer MSE avec les deux m√©thodes pour comparer
    mask_train_np = (mask_train.numpy()[0, 0] > 0)  # (224, 224) boolean

    # M√©thode TRAIN
    diff_train = (hv_out_train - hv_t_train) ** 2
    masked_diff_train = diff_train * mask_train
    mse_train_method = (masked_diff_train.sum() / (mask_train.sum() * 2)).item()

    # M√©thode TEST sur m√™mes donn√©es
    hv_out_np = hv_out_train.numpy()[0]  # (2, 224, 224)
    hv_t_np = hv_t_train.numpy()[0]  # (2, 224, 224)
    mse_test_method = ((hv_out_np[:, mask_train_np] - hv_t_np[:, mask_train_np]) ** 2).mean()

    print(f"   MSE (m√©thode TRAIN): {mse_train_method:.6f}")
    print(f"   MSE (m√©thode TEST):  {mse_test_method:.6f}")
    print(f"   ‚Üí Diff√©rence: {abs(mse_train_method - mse_test_method):.6f}")
    print()
    print("   ‚ö†Ô∏è  Les deux m√©thodes donnent le M√äME r√©sultat")
    print("   ‚Üí Le probl√®me n'est PAS dans le calcul MSE")
    print()

    # Diagnostic final
    print("=" * 80)
    print("DIAGNOSTIC FINAL - VUE GLOBALE")
    print("=" * 80)
    print()

    print("R√âSUM√â:")
    print(f"  ‚Ä¢ M√™me donn√©es: ‚úÖ (features, targets identiques)")
    print(f"  ‚Ä¢ M√™me mod√®le: ‚úÖ (checkpoint charg√©)")
    print(f"  ‚Ä¢ M√™me MSE: ‚úÖ (train={mse_train_method:.4f}, test={mse_test_method:.4f})")
    print()

    print("ANALYSE DES VALEURS:")
    print(f"  ‚Ä¢ HV pred:   range [{hv_out_train.min():.3f}, {hv_out_train.max():.3f}], std {hv_out_train.std():.3f}")
    print(f"  ‚Ä¢ HV target: range [{hv_t_train.min():.3f}, {hv_t_train.max():.3f}], std {hv_t_train.std():.3f}")
    print()

    # Test de l'expert: Scaling test
    print("=" * 80)
    print("TEST DE SCALING (Hypoth√®se Expert)")
    print("=" * 80)
    print()

    print("Hypoth√®se: Le mod√®le a appris la DIRECTION correcte des gradients,")
    print("           mais pas la MAGNITUDE (intensit√©).")
    print()

    # Tester diff√©rents facteurs de scaling
    scaling_factors = [1.0, 1.5, 2.0, 2.5, 3.0, 5.0, 10.0, 16.0]

    mse_results = []
    for scale in scaling_factors:
        scaled_hv = hv_out_train * scale
        diff = (scaled_hv - hv_t_train) ** 2
        masked_diff = diff * mask_train
        mse_scaled = (masked_diff.sum() / (mask_train.sum() * 2)).item()
        mse_results.append(mse_scaled)

        print(f"  Scale {scale:5.1f}x ‚Üí MSE = {mse_scaled:.6f}")

    # Trouver le meilleur scaling
    best_idx = np.argmin(mse_results)
    best_scale = scaling_factors[best_idx]
    best_mse = mse_results[best_idx]

    print()
    print(f"  ‚ú® Meilleur scaling: {best_scale:.1f}x ‚Üí MSE = {best_mse:.6f}")
    print(f"  üìä Am√©lioration: {((mse_results[0] - best_mse) / mse_results[0] * 100):.1f}%")
    print()

    # Ratio std pred/target
    ratio_std = hv_out_train.std() / hv_t_train.std()
    print(f"  üìê Ratio std (pred/target): {ratio_std.item():.4f}")
    print(f"  üìê Scaling th√©orique optimal: {1.0/ratio_std.item():.1f}x")
    print()

    # Diagnostic final
    print("=" * 80)
    print("DIAGNOSTIC FINAL")
    print("=" * 80)
    print()

    if best_scale > 1.5:
        print("üîç PROBL√àME CONFIRM√â: Scaling des gradients")
        print()
        print(f"  Le mod√®le a appris la DIRECTION correcte (forme des gradients)")
        print(f"  Mais pas la MAGNITUDE (intensit√© √ó {best_scale:.1f} trop faible)")
        print()
        print("  CAUSES POSSIBLES:")
        print("  1. Poids de hv_head trop petits (mauvaise initialisation)")
        print("  2. Learning rate HV trop faible")
        print("  3. Targets normalis√©s diff√©remment pendant l'entra√Ænement")
        print()
        print("  SOLUTION:")
        print(f"  Option A: Multiplier hv_head par {best_scale:.1f} dans le forward")
        print("  Option B: R√©-entra√Æner avec learning rate HV √ó 3")
        print("  Option C: R√©-entra√Æner from scratch (plus safe)")
    else:
        print("‚úÖ Le mod√®le utilise la bonne magnitude")
        print("   Le probl√®me est ailleurs (direction, forme des gradients)")
    print()


if __name__ == "__main__":
    compare_pipelines()
