#!/usr/bin/env python3
"""
V√©rification CRITIQUE des features H-optimus-0 utilis√©es pour training.

Suite √† d√©couverte checkpoint entra√Æn√© APR√àS Sobel fix (24 d√©c > 23 d√©c),
mais HV magnitude quand m√™me catastrophique (0.022).

Hypoth√®se: Features training corrompues (Bug #1 ToPILImage ou Bug #2 LayerNorm).

Ce script v√©rifie:
1. CLS std dans [0.70, 0.90] (signature features correctes)
2. Shape (N, 261, 1536) - 1 CLS + 256 patches
3. Mean proche de 0 (normalis√©)
4. Comparaison train vs inference (ratio proche 1.0)
"""

import sys
from pathlib import Path
import numpy as np
import argparse

PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.constants import DEFAULT_FAMILY_DATA_DIR


def verify_features_training(family: str = "epidermal", compare_inference: bool = False):
    """
    V√©rifie features H-optimus-0 utilis√©es durant training.

    Args:
        family: Famille √† v√©rifier
        compare_inference: Si True, compare avec features inference fra√Æches
    """
    print("\n" + "="*80)
    print(f"V√âRIFICATION CRITIQUE: FEATURES H-OPTIMUS-0 TRAINING - {family.upper()}")
    print("="*80)
    print("\nCrit√®re: CLS std doit √™tre dans [0.70, 0.90]")
    print("Si hors plage ‚Üí Features corrompues (Bug #1 ou Bug #2)")
    print("\n" + "‚îÄ"*80)

    # Charger features training
    data_dir = Path(DEFAULT_FAMILY_DATA_DIR)
    features_path = data_dir / f"{family}_features.npz"

    if not features_path.exists():
        print(f"\n‚ùå ERREUR: Fichier introuvable: {features_path}")
        print(f"\nCherche dans:")
        print(f"  ‚Ä¢ {data_dir}")
        print(f"  ‚Ä¢ {data_dir.parent}")

        # Chercher r√©cursivement
        possible_paths = list(Path(PROJECT_ROOT).rglob(f"{family}_features.npz"))
        if possible_paths:
            print(f"\nüí° Fichiers trouv√©s ailleurs:")
            for p in possible_paths:
                print(f"   {p}")
        return 1

    print(f"\nüìÅ Fichier: {features_path}")
    print(f"   Taille: {features_path.stat().st_size / 1024**2:.1f} MB")

    # Charger avec mmap
    data = np.load(features_path, mmap_mode='r')

    print(f"\nüìä Contenu .npz:")
    for key in data.keys():
        arr = data[key]
        print(f"   ‚Ä¢ {key:20s}: shape={arr.shape}, dtype={arr.dtype}")

    # Extraire features
    if 'features' in data:
        features = data['features']
    elif 'layer_24' in data:
        features = data['layer_24']
        print(f"\n‚ö†Ô∏è WARNING: Cl√© 'layer_24' trouv√©e (anciennes features)")
        print(f"   Pr√©f√©rer 'features' (nouvelles)")
    else:
        print(f"\n‚ùå ERREUR: Ni 'features' ni 'layer_24' trouv√©!")
        print(f"   Cl√©s disponibles: {list(data.keys())}")
        return 1

    # Statistiques COMPL√àTES
    print(f"\n" + "="*80)
    print("STATISTIQUES FEATURES H-OPTIMUS-0")
    print("="*80)

    print(f"\n1Ô∏è‚É£ FORMAT")
    print(f"   Shape:  {features.shape}")
    print(f"   Dtype:  {features.dtype}")
    print(f"   Memory: {features.nbytes / 1024**2:.1f} MB")

    # V√©rifier shape
    expected_shape = (None, 261, 1536)  # N samples, 1 CLS + 256 patches, 1536-dim
    if features.ndim != 3:
        print(f"\n‚ùå ERREUR SHAPE: {features.ndim}D au lieu de 3D")
        return 1

    if features.shape[1] != 261:
        print(f"\n‚ùå ERREUR TOKENS: {features.shape[1]} au lieu de 261")
        print(f"   Attendu: 1 CLS + 256 patches = 261 tokens")
        return 1

    if features.shape[2] != 1536:
        print(f"\n‚ùå ERREUR DIM: {features.shape[2]} au lieu de 1536")
        print(f"   Attendu: H-optimus-0 embedding dimension = 1536")
        return 1

    print(f"\n‚úÖ Shape correcte: {features.shape}")
    print(f"   ‚Ä¢ Samples: {features.shape[0]}")
    print(f"   ‚Ä¢ Tokens: {features.shape[1]} (1 CLS + 256 patches)")
    print(f"   ‚Ä¢ Dim: {features.shape[2]} (H-optimus-0)")

    # Extraire CLS tokens
    cls_tokens = features[:, 0, :]  # (N, 1536)

    print(f"\n2Ô∏è‚É£ CLS TOKEN STATISTICS (CRITIQUE)")

    cls_mean = float(cls_tokens.mean())
    cls_std = float(cls_tokens.std())
    cls_min = float(cls_tokens.min())
    cls_max = float(cls_tokens.max())

    print(f"   Mean:   {cls_mean:+.6f}")
    print(f"   Std:    {cls_std:.6f}")
    print(f"   Min:    {cls_min:+.6f}")
    print(f"   Max:    {cls_max:+.6f}")

    # DIAGNOSTIC CLS STD (CRITIQUE)
    print(f"\n" + "="*80)
    print("DIAGNOSTIC CLS STD")
    print("="*80)

    if cls_std < 0.40:
        print(f"\n‚ùå ERREUR CRITIQUE: CLS std trop bas ({cls_std:.4f})")
        print(f"   Attendu: [0.70, 0.90]")
        print(f"\nüí° DIAGNOSTIC: Bug #2 (LayerNorm Mismatch)")
        print(f"   Cause probable: Features extraites avec blocks[23] (sans LayerNorm)")
        print(f"   Au lieu de: forward_features() (avec LayerNorm)")
        print(f"\nüîß SOLUTION:")
        print(f"   1. R√©g√©n√©rer features avec forward_features()")
        print(f"   2. V√©rifier CLS std apr√®s r√©g√©n√©ration")
        print(f"   3. R√©-entra√Æner avec features correctes")
        return 1

    elif cls_std > 1.50:
        print(f"\n‚ùå ERREUR CRITIQUE: CLS std trop haut ({cls_std:.4f})")
        print(f"   Attendu: [0.70, 0.90]")
        print(f"\nüí° DIAGNOSTIC: Normalisation incorrecte")
        print(f"   Cause probable: Mean/Std HOPTIMUS incorrects")
        print(f"   Ou: Pas de normalisation appliqu√©e")
        return 1

    elif 0.70 <= cls_std <= 0.90:
        print(f"\n‚úÖ CLS STD CORRECT: {cls_std:.4f} (dans [0.70, 0.90])")
        print(f"   Features H-optimus-0 VALIDES ‚úÖ")

    else:
        print(f"\n‚ö†Ô∏è WARNING: CLS std l√©g√®rement hors plage ({cls_std:.4f})")
        print(f"   Attendu: [0.70, 0.90]")
        print(f"   Acceptable si proche (0.65-0.95)")

    # Statistiques par √©chantillon
    print(f"\n3Ô∏è‚É£ CLS STD PAR √âCHANTILLON (Premiers 10)")

    sample_stds = []
    for i in range(min(10, features.shape[0])):
        sample_cls = features[i, 0, :]
        sample_std = float(sample_cls.std())
        sample_stds.append(sample_std)
        print(f"   Sample {i:2d}: std={sample_std:.4f}")

    # Distribution CLS std
    all_sample_stds = [features[i, 0, :].std() for i in range(features.shape[0])]
    std_mean = np.mean(all_sample_stds)
    std_std = np.std(all_sample_stds)

    print(f"\n   Distribution CLS std:")
    print(f"      Mean: {std_mean:.4f}")
    print(f"      Std:  {std_std:.4f}")
    print(f"      Min:  {np.min(all_sample_stds):.4f}")
    print(f"      Max:  {np.max(all_sample_stds):.4f}")

    # Comparaison inference (optionnel)
    if compare_inference:
        print(f"\n" + "="*80)
        print("COMPARAISON TRAIN VS INFERENCE")
        print("="*80)

        print(f"\n‚ö†Ô∏è Fonctionnalit√© non impl√©ment√©e (n√©cessite image test)")
        print(f"   Utiliser: compare_train_vs_inference.py")

    # VERDICT FINAL
    print(f"\n" + "="*80)
    print("VERDICT FINAL")
    print("="*80)

    issues = []

    if features.shape[1] != 261:
        issues.append(f"Shape tokens incorrect: {features.shape[1]} au lieu de 261")

    if features.shape[2] != 1536:
        issues.append(f"Embedding dim incorrect: {features.shape[2]} au lieu de 1536")

    if cls_std < 0.70 or cls_std > 0.90:
        issues.append(f"CLS std hors plage: {cls_std:.4f} (attendu [0.70, 0.90])")

    if issues:
        print(f"\n‚ùå PROBL√àMES D√âTECT√âS ({len(issues)}):")
        for i, issue in enumerate(issues, 1):
            print(f"   {i}. {issue}")

        print(f"\nüîß ACTIONS REQUISES:")
        print(f"   1. R√©g√©n√©rer features H-optimus-0 avec preprocessing correct")
        print(f"   2. V√©rifier CLS std apr√®s r√©g√©n√©ration")
        print(f"   3. R√©-entra√Æner avec features correctes")

        return 1
    else:
        print(f"\n‚úÖ FEATURES H-OPTIMUS-0 CORRECTES!")
        print(f"   ‚Ä¢ Shape: {features.shape}")
        print(f"   ‚Ä¢ CLS std: {cls_std:.4f} (dans [0.70, 0.90])")
        print(f"   ‚Ä¢ Mean: {cls_mean:+.4f} (centr√©)")

        print(f"\nüí° CONCLUSION:")
        print(f"   Les features training sont valides.")
        print(f"   Le probl√®me HV magnitude faible (0.022) vient donc:")
        print(f"   ‚Üí Du MOD√àLE (convergence insuffisante)")
        print(f"   ‚Üí Ou des HYPERPARAM√àTRES (lambda_hv trop faible)")

        print(f"\nüîç PROCHAINE √âTAPE:")
        print(f"   V√©rifier logs training pour:")
        print(f"   1. Sobel gradient loss actif?")
        print(f"   2. HV MSE convergence?")
        print(f"   3. Nombre epochs suffisant?")

        return 0


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="V√©rifier features H-optimus-0 training"
    )
    parser.add_argument('--family', type=str, default='epidermal',
                       choices=['glandular', 'digestive', 'urologic', 'epidermal', 'respiratory'],
                       help='Famille √† v√©rifier')
    parser.add_argument('--compare', action='store_true',
                       help='Comparer avec features inference (non impl√©ment√©)')

    args = parser.parse_args()

    sys.exit(verify_features_training(args.family, args.compare))
