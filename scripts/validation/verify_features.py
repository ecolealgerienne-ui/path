#!/usr/bin/env python3
"""
Script de v√©rification des features H-optimus-0 extraites.

OBJECTIF:
=========
V√©rifier que les features extraites sont coh√©rentes avec la m√©thode
forward_features() qui inclut le LayerNorm final.

CRIT√àRES DE VALIDATION:
=======================
- CLS token std: 0.70 - 0.90 (avec LayerNorm)
- Si std ~0.28, les features sont corrompues (sans LayerNorm)

Usage:
    # V√©rifier les features d'un fold
    python scripts/validation/verify_features.py --features_dir data/cache/pannuke_features

    # V√©rifier avec comparaison fresh (plus lent, charge H-optimus-0)
    python scripts/validation/verify_features.py --features_dir data/cache/pannuke_features \
        --data_dir /home/amar/data/PanNuke --verify_fresh
"""

import argparse
import sys
from pathlib import Path
import numpy as np

# Constantes de validation
EXPECTED_CLS_STD_MIN = 0.70
EXPECTED_CLS_STD_MAX = 0.90
CORRUPTED_CLS_STD_MAX = 0.40  # Features sans LayerNorm ont std ~0.28


def verify_features_file(features_path: Path, verbose: bool = True) -> dict:
    """
    V√©rifie un fichier de features.

    Returns:
        dict avec: valid (bool), cls_std, cls_mean, shape, issues (list)
    """
    result = {
        "path": str(features_path),
        "valid": False,
        "issues": []
    }

    if not features_path.exists():
        result["issues"].append(f"Fichier non trouv√©: {features_path}")
        return result

    try:
        data = np.load(features_path)
    except Exception as e:
        result["issues"].append(f"Erreur de chargement: {e}")
        return result

    # D√©terminer la cl√© des features
    if 'features' in data.files:
        features = data['features']
        result["key"] = "features"
    elif 'layer_24' in data.files:
        features = data['layer_24']
        result["key"] = "layer_24"
    else:
        result["issues"].append(f"Cl√© de features non trouv√©e. Cl√©s disponibles: {data.files}")
        return result

    result["shape"] = features.shape
    result["dtype"] = str(features.dtype)

    # V√©rifier la shape
    if len(features.shape) != 3:
        result["issues"].append(f"Shape invalide: {features.shape}, attendu (N, 261, 1536)")
        return result

    n_images, n_tokens, embed_dim = features.shape

    if n_tokens != 261:
        result["issues"].append(f"Nombre de tokens invalide: {n_tokens}, attendu 261")

    if embed_dim != 1536:
        result["issues"].append(f"Dimension d'embedding invalide: {embed_dim}, attendu 1536")

    # Extraire les CLS tokens
    cls_tokens = features[:, 0, :]  # (N, 1536)

    result["cls_std"] = float(cls_tokens.std())
    result["cls_mean"] = float(cls_tokens.mean())
    result["cls_min"] = float(cls_tokens.min())
    result["cls_max"] = float(cls_tokens.max())
    result["n_images"] = n_images

    # V√©rifier le std
    if result["cls_std"] < CORRUPTED_CLS_STD_MAX:
        result["issues"].append(
            f"CLS std={result['cls_std']:.4f} < {CORRUPTED_CLS_STD_MAX} "
            f"‚Üí Features CORROMPUES (LayerNorm manquant)!"
        )
    elif result["cls_std"] < EXPECTED_CLS_STD_MIN:
        result["issues"].append(
            f"CLS std={result['cls_std']:.4f} < {EXPECTED_CLS_STD_MIN} "
            f"‚Üí Features suspectes"
        )
    elif result["cls_std"] > EXPECTED_CLS_STD_MAX:
        result["issues"].append(
            f"CLS std={result['cls_std']:.4f} > {EXPECTED_CLS_STD_MAX} "
            f"‚Üí Features anormalement √©lev√©es"
        )

    # V√©rifier les NaN/Inf
    if np.isnan(features).any():
        result["issues"].append("Features contiennent des NaN!")
    if np.isinf(features).any():
        result["issues"].append("Features contiennent des Inf!")

    result["valid"] = len(result["issues"]) == 0

    if verbose:
        print(f"\n{'='*60}")
        print(f"üìÅ {features_path.name}")
        print(f"{'='*60}")
        print(f"  Shape: {result['shape']}")
        print(f"  Images: {n_images}")
        print(f"  CLS token stats:")
        print(f"    std:  {result['cls_std']:.4f} (attendu: {EXPECTED_CLS_STD_MIN}-{EXPECTED_CLS_STD_MAX})")
        print(f"    mean: {result['cls_mean']:.4f}")
        print(f"    range: [{result['cls_min']:.4f}, {result['cls_max']:.4f}]")

        if result["valid"]:
            print(f"\n  ‚úÖ VALIDE")
        else:
            print(f"\n  ‚ùå INVALIDE:")
            for issue in result["issues"]:
                print(f"    ‚Üí {issue}")

    return result


def verify_fresh_extraction(
    features_path: Path,
    data_dir: Path,
    n_samples: int = 10
) -> dict:
    """
    Compare les features cach√©es avec une extraction fra√Æche.

    Permet de d√©tecter les diff√©rences de preprocessing.
    """
    import torch
    from torchvision import transforms

    try:
        import timm
    except ImportError:
        return {"valid": False, "issues": ["timm non install√©"]}

    result = {"valid": False, "issues": []}

    # Charger les features cach√©es
    data = np.load(features_path)
    if 'features' in data.files:
        cached_features = data['features']
    elif 'layer_24' in data.files:
        cached_features = data['layer_24']
    else:
        result["issues"].append("Cl√© de features non trouv√©e")
        return result

    # D√©terminer le fold depuis le nom du fichier
    fold = int(features_path.stem.replace("fold", "").replace("_features", ""))

    # Charger les images
    images_path = data_dir / f"fold{fold}" / "images.npy"
    if not images_path.exists():
        result["issues"].append(f"Images non trouv√©es: {images_path}")
        return result

    images = np.load(images_path, mmap_mode='r')

    # S√©lectionner des √©chantillons al√©atoires
    indices = np.random.choice(len(images), min(n_samples, len(images)), replace=False)

    print(f"\nüî¨ V√©rification fresh extraction ({n_samples} √©chantillons)...")

    # Charger H-optimus-0
    print("  Chargement H-optimus-0...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = timm.create_model(
        "hf-hub:bioptimus/H-optimus-0",
        pretrained=True,
        init_values=1e-5,
        dynamic_img_size=False
    )
    model.eval().to(device)

    # Transform
    HOPTIMUS_MEAN = (0.707223, 0.578729, 0.703617)
    HOPTIMUS_STD = (0.211883, 0.230117, 0.177517)

    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=HOPTIMUS_MEAN, std=HOPTIMUS_STD),
    ])

    # Extraire les features fra√Æches
    differences = []

    with torch.no_grad():
        for idx in indices:
            img = images[idx]
            if img.dtype != np.uint8:
                img = img.clip(0, 255).astype(np.uint8)

            tensor = transform(img).unsqueeze(0).to(device)
            fresh_features = model.forward_features(tensor).cpu().numpy()

            cached = cached_features[idx:idx+1]

            # Calculer la diff√©rence
            diff = np.abs(fresh_features - cached).mean()
            differences.append(diff)

            print(f"    Image {idx}: diff={diff:.6f}")

    mean_diff = np.mean(differences)
    max_diff = np.max(differences)

    print(f"\n  Diff√©rence moyenne: {mean_diff:.6f}")
    print(f"  Diff√©rence max: {max_diff:.6f}")

    # Les features doivent √™tre identiques (ou tr√®s proches √† cause de la pr√©cision float)
    if mean_diff > 0.001:
        result["issues"].append(
            f"Diff√©rence significative entre features cach√©es et fresh: {mean_diff:.6f}"
        )
    else:
        print(f"  ‚úÖ Features coh√©rentes avec extraction fra√Æche")

    result["mean_diff"] = mean_diff
    result["max_diff"] = max_diff
    result["valid"] = len(result["issues"]) == 0

    return result


def main():
    parser = argparse.ArgumentParser(description="V√©rification features H-optimus-0")
    parser.add_argument("--features_dir", type=str, required=True,
                        help="R√©pertoire des features (ex: data/cache/pannuke_features)")
    parser.add_argument("--data_dir", type=str, default=None,
                        help="R√©pertoire PanNuke pour v√©rification fresh")
    parser.add_argument("--verify_fresh", action="store_true",
                        help="V√©rifier avec extraction fra√Æche (lent)")
    parser.add_argument("--n_samples", type=int, default=10,
                        help="Nombre d'√©chantillons pour v√©rification fresh")
    args = parser.parse_args()

    features_dir = Path(args.features_dir)

    if not features_dir.exists():
        print(f"‚ùå R√©pertoire non trouv√©: {features_dir}")
        sys.exit(1)

    # Trouver tous les fichiers de features
    feature_files = sorted(features_dir.glob("fold*_features.npz"))

    if not feature_files:
        print(f"‚ùå Aucun fichier de features trouv√© dans {features_dir}")
        sys.exit(1)

    print("=" * 60)
    print("V√âRIFICATION DES FEATURES H-OPTIMUS-0")
    print("=" * 60)
    print(f"R√©pertoire: {features_dir}")
    print(f"Fichiers trouv√©s: {len(feature_files)}")
    print(f"Crit√®res de validation:")
    print(f"  - CLS std attendu: {EXPECTED_CLS_STD_MIN} - {EXPECTED_CLS_STD_MAX}")
    print(f"  - CLS std corrompu: < {CORRUPTED_CLS_STD_MAX}")

    # V√©rifier chaque fichier
    all_results = []
    for features_path in feature_files:
        result = verify_features_file(features_path)
        all_results.append(result)

        # V√©rification fresh si demand√©e
        if args.verify_fresh and args.data_dir and result["valid"]:
            fresh_result = verify_fresh_extraction(
                features_path,
                Path(args.data_dir),
                n_samples=args.n_samples
            )
            result["fresh_verification"] = fresh_result
            if not fresh_result["valid"]:
                result["valid"] = False
                result["issues"].extend(fresh_result["issues"])

    # R√©sum√© final
    print("\n" + "=" * 60)
    print("R√âSUM√â")
    print("=" * 60)

    valid_count = sum(1 for r in all_results if r["valid"])
    total_count = len(all_results)

    for result in all_results:
        status = "‚úÖ" if result["valid"] else "‚ùå"
        print(f"{status} {Path(result['path']).name}: CLS std={result.get('cls_std', 'N/A'):.4f}")
        if not result["valid"]:
            for issue in result["issues"]:
                print(f"   ‚Üí {issue}")

    print(f"\nTotal: {valid_count}/{total_count} valides")

    if valid_count == total_count:
        print("\nüéâ TOUTES LES FEATURES SONT VALIDES!")
        print("   ‚Üí Pr√™t pour l'entra√Ænement OrganHead/HoVerNet")
        sys.exit(0)
    else:
        print("\n‚ùå FEATURES INVALIDES D√âTECT√âES!")
        print("   ‚Üí R√©-extraire les features avec le script corrig√©:")
        print("     python scripts/preprocessing/extract_features.py --data_dir /path/to/PanNuke --fold 0")
        sys.exit(1)


if __name__ == "__main__":
    main()
