# Fix Sobel Gradient Loss ‚Äî R√©solution Probl√®me AJI/PQ

**Date:** 2025-12-23
**Probl√®me:** AJI 0.07 vs cible 0.80 (√©cart +1000%), PQ 0.10 vs cible 0.70 (√©cart +600%)
**Cause racine:** Signal de gradient_loss trop faible (0.01) ‚Üí mod√®le apprend HV maps "douces" ‚Üí watershed √©choue

---

## Diagnostic Expert Externe

### Observation 1 : HV MSE Bon mais AJI Catastrophique

```
M√©triques apr√®s r√©-entra√Ænement MSE:
  NP Dice:  0.9527  ‚úÖ Excellent
  HV MSE:   0.0520  ‚úÖ Excellent
  NT Acc:   0.8731  ‚úÖ Bon

Ground Truth Evaluation:
  AJI:      0.0701  ‚ùå Catastrophique (cible: >0.80)
  PQ:       0.1060  ‚ùå Catastrophique (cible: >0.70)
  Rappel:   6.93%   ‚ùå D√©tecte seulement 50/721 cellules
```

**Paradoxe:** Comment HV MSE peut-il √™tre excellent (0.05) mais AJI catastrophique ?

### Observation 2 : Visualisation R√©v√©latrice

**Expert externe (diagnostic image):**
> "Les Cartes HV: Le 'bruit' des Gradients
> - Tes gradients sont 'mous'
> - L'image HV Gradient (edges) montre des lignes rouges tr√®s fines et fragment√©es
> - Pour que le Watershed fonctionne, ces lignes devraient √™tre des enceintes ferm√©es et solides"

**Explication:**
- HV MSE mesure l'**erreur moyenne** sur les valeurs HV
- Watershed a besoin de **gradients nets** (fortes variations spatiales)
- Un HV map "doux" (liss√©) peut avoir bon MSE mais gradients faibles

### Observation 3 : Analyse Math√©matique du Code

**Expert externe (diagnostic code):**
> "Le 'Loup' est dans la gradient_loss (MSGE)
>
> ```python
> pred_h = pred[:, :, :, 1:] - pred[:, :, :, :-1]
> ```
>
> Si tes pixels sont distants de 1, leur diff√©rence de valeur est minuscule (ex: 0.01).
>
> **Impact:** Ton hv_gradient (la MSGE) devient une valeur extr√™mement petite (proche de 0).
> M√™me avec ton multiplicateur 0.5 * hv_gradient, cette perte est 'invisible' pour l'optimiseur."

**Exemple concret:**

```python
# HV map [-1, 1] avec transition douce sur 10 pixels
HV = [-0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4]

# Gradient avec diff√©rence finie simple
grad_simple = HV[i+1] - HV[i] = 0.1  (constant)
grad_loss_simple = (0.1)¬≤ = 0.01  ‚ùå Signal faible

# Gradient avec Sobel (3√ó3)
# Sobel kernel: [-1, 0, 1] / 2 = moyenne pond√©r√©e sur 3 pixels
grad_sobel = (HV[i+2] - HV[i]) / 2 = 0.2
grad_loss_sobel = (0.2)¬≤ = 0.04  ‚úÖ Signal 4√ó plus fort
```

---

## Solution Impl√©ment√©e : Op√©rateur Sobel

### Avant (Diff√©rences Finies Simples)

```python
# src/models/hovernet_decoder.py (ancien)
def gradient_loss(self, pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
    # Gradients horizontal et vertical (diff√©rences finies)
    pred_grad_h = pred[:, :, :, 1:] - pred[:, :, :, :-1]
    pred_grad_v = pred[:, :, 1:, :] - pred[:, :, :-1, :]

    target_grad_h = target[:, :, :, 1:] - target[:, :, :, :-1]
    target_grad_v = target[:, :, 1:, :] - target[:, :, :-1, :]

    # Signal typique: ~0.01 ‚Üí gradient_loss n√©gligeable
    grad_loss = F.mse_loss(pred_grad_h, target_grad_h) + F.mse_loss(pred_grad_v, target_grad_v)
```

**Probl√®me:**
- Diff√©rence entre pixels voisins dans HV [-1, 1] : ~0.01
- Gradient loss : ~0.0001 (n√©gligeable devant NP loss ~1.0)
- Optimiseur ignore cette perte ‚Üí pas de pression pour cr√©er fronti√®res nettes

### Apr√®s (Op√©rateur Sobel)

```python
# src/models/hovernet_decoder.py (nouveau)
def gradient_loss(self, pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
    """
    MSGE avec op√©rateur Sobel pour signal amplifi√©.

    Sobel kernel horizontal: [[-1, 0, 1],
                              [-2, 0, 2],
                              [-1, 0, 1]]

    Sobel kernel vertical:   [[-1, -2, -1],
                              [ 0,  0,  0],
                              [ 1,  2,  1]]
    """
    # Noyaux Sobel pour gradients horizontal et vertical
    sobel_h = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=pred.dtype, device=pred.device).view(1, 1, 3, 3)
    sobel_v = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=pred.dtype, device=pred.device).view(1, 1, 3, 3)

    B, C, H, W = pred.shape

    # Reshape pour convolution: (B*C, 1, H, W)
    pred_reshaped = pred.view(B * C, 1, H, W)
    target_reshaped = target.view(B * C, 1, H, W)

    # Gradients Sobel avec padding pour garder la taille
    pred_grad_h = F.conv2d(pred_reshaped, sobel_h, padding=1)
    pred_grad_v = F.conv2d(pred_reshaped, sobel_v, padding=1)

    target_grad_h = F.conv2d(target_reshaped, sobel_h, padding=1)
    target_grad_v = F.conv2d(target_reshaped, sobel_v, padding=1)

    # Reshape back: (B, C, H, W)
    pred_grad_h = pred_grad_h.view(B, C, H, W)
    pred_grad_v = pred_grad_v.view(B, C, H, W)
    target_grad_h = target_grad_h.view(B, C, H, W)
    target_grad_v = target_grad_v.view(B, C, H, W)

    if mask is not None:
        # Masquer les gradients (uniquement sur les noyaux)
        grad_loss_h = F.mse_loss(pred_grad_h * mask, target_grad_h * mask, reduction='sum')
        grad_loss_v = F.mse_loss(pred_grad_v * mask, target_grad_v * mask, reduction='sum')

        # Normaliser par le nombre de pixels masqu√©s
        n_pixels = mask.sum() * C
        grad_loss = (grad_loss_h + grad_loss_v) / (n_pixels + 1e-8)
    else:
        grad_loss = F.mse_loss(pred_grad_h, target_grad_h) + F.mse_loss(pred_grad_v, target_grad_v)

    return grad_loss
```

**Avantages:**
- Sobel amplifie gradients 2-3√ó (convolution sur 3√ó3 voisinage)
- Signal gradient_loss ~0.04 au lieu de ~0.01 (4√ó plus fort)
- Optimiseur re√ßoit pression significative pour cr√©er fronti√®res nettes
- Les contours deviennent des "enceintes ferm√©es" n√©cessaires au watershed

---

## Validation du Fix

### √âtape 1 : V√©rifier Normalisation HV Targets

**Pourquoi ?** L'expert a sugg√©r√© de v√©rifier que les targets ne sont pas en [0, 255] au lieu de [-1, 1].

**Script cr√©√©:**

```bash
python scripts/validation/verify_hv_targets.py
```

**Sortie attendue:**

```
üîç V√©rification: epidermal_targets.npz
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HV Targets:
  Dtype:  float32
  Range:  [-1.0000, 1.0000]
  Mean:   0.0000
  Std:    0.5350

‚úÖ VALIDATION OK
```

**Si validation √©choue:**
- R√©g√©n√©rer donn√©es avec `prepare_family_data_FIXED.py`
- V√©rifier que `compute_hv_maps()` normalise bien avec `/ max_dist`

### √âtape 2 : R√©-entra√Æner avec Sobel Gradient Loss

**Commande:**

```bash
conda activate cellvit

python scripts/training/train_hovernet_family.py \
    --family epidermal \
    --epochs 50 \
    --augment \
    --lambda_np 1.0 \
    --lambda_hv 2.0 \
    --lambda_nt 1.0
```

**Checkpoint sauvegard√©:**
- `models/checkpoints/hovernet_epidermal_best.pth`

**M√©triques attendues (entra√Ænement):**
| M√©trique | Avant (simple grad) | Apr√®s (Sobel) | Explication |
|----------|---------------------|---------------|-------------|
| NP Dice | 0.9527 | ~0.95 (stable) | Segmentation binaire peu affect√©e |
| HV MSE | 0.0520 | ~0.05-0.08 | Peut augmenter l√©g√®rement (MSE ‚â† sharpness) |
| NT Acc | 0.8731 | ~0.87 (stable) | Classification ind√©pendante |

**‚ö†Ô∏è Important:** HV MSE peut l√©g√®rement **augmenter** avec Sobel car le mod√®le optimise maintenant pour des **gradients nets** (sharpness) plut√¥t que MSE minimale. C'est **normal et souhait√©**.

### √âtape 3 : √âvaluer sur Ground Truth

**Commande:**

```bash
python scripts/evaluation/evaluate_ground_truth.py \
    --dataset_dir data/evaluation/pannuke_fold2_converted \
    --num_samples 100 \
    --output_dir results/epidermal_sobel_eval \
    --checkpoint models/checkpoints/hovernet_epidermal_best.pth \
    --family epidermal
```

**M√©triques cibles (Ground Truth):**
| M√©trique | Avant (simple grad) | Cible (Sobel) | Am√©lioration |
|----------|---------------------|---------------|--------------|
| **AJI** | 0.0701 | **>0.60** | **+756%** |
| **PQ** | 0.1060 | **>0.70** | **+560%** |
| Dice | 0.9441 | ~0.94 (stable) | Stable |
| Rappel | 6.93% | **>80%** | **+1054%** |

**Si AJI/PQ s'am√©liorent significativement (>0.60):**
- ‚úÖ Hypoth√®se confirm√©e ‚Üí Sobel r√©sout le probl√®me
- R√©-entra√Æner les 4 autres familles (glandular, digestive, urologic, respiratory)

**Si AJI/PQ restent faibles (<0.30):**
- V√©rifier visualisations HV gradients (devraient montrer contours ferm√©s)
- Augmenter poids gradient_loss (0.5 ‚Üí 1.0)
- V√©rifier post-processing watershed (seuils, param√®tres)

---

## Explication Scientifique : Pourquoi Sobel Fonctionne

### Probl√®me Fondamental

Le watershed a besoin de **minima locaux** dans la magnitude du gradient HV :

```
Gradient Magnitude = ‚àö(grad_h¬≤ + grad_v¬≤)

Pour s√©parer 2 noyaux touchants:
  - Au centre de chaque noyau: gradient magnitude faible (noyau homog√®ne)
  - √Ä la fronti√®re entre noyaux: gradient magnitude √âLEV√âE (transition nette)
  - Watershed suit les cr√™tes (high gradient) pour tracer les fronti√®res
```

### Diff√©rences Finies vs Sobel

**Diff√©rences finies simples:**
```
grad[i] = pixel[i+1] - pixel[i]

Sensible au bruit:
  HV = [0.5, 0.52, 0.48, 0.51]
  grad = [0.02, -0.04, 0.03]  ‚Üê Oscillations bruit√©es
```

**Sobel (moyenne pond√©r√©e 3√ó3):**
```
grad[i] = (pixel[i-1] - pixel[i+1]) / 2 + poids voisins

Liss√© et amplifi√©:
  HV = [0.5, 0.52, 0.48, 0.51]
  grad_sobel = [0.00, 0.01, 0.00]  ‚Üê Liss√©, signal net √† la fronti√®re
```

**R√©sultat:**
- Sobel cr√©e des contours **ferm√©s et nets** autour des noyaux
- Watershed peut suivre ces cr√™tes pour s√©parer les instances
- AJI/PQ s'am√©liorent drastiquement

---

## Timeline Compl√®te du Debugging

### 2025-12-21 : Instance Mismatch (Bug #3)
- D√©couverte : connectedComponents fusionne cellules qui se touchent
- Solution : Utiliser vraies instances PanNuke (canaux 1-4)
- Impact : Recall passe de 7.69% √† ~60%

### 2025-12-22 : SmoothL1 vs MSE (Partial Fix)
- D√©couverte : SmoothL1 plafonne gradients √† ¬±1 pour fortes erreurs
- Solution : Remplacer par MSE dans gradient_loss
- Impact : L√©ger (NT Acc 0.9061 ‚Üí 0.8731)

### 2025-12-23 : Weak Gradient Signal (ROOT CAUSE)
- D√©couverte : Diff√©rences finies produisent signal ~0.01 ‚Üí n√©gligeable
- Solution : Op√©rateur Sobel pour signal 2-3√ó plus fort
- Impact attendu : AJI 0.07 ‚Üí >0.60 (+756%)

---

## Prochaines √âtapes

1. **V√©rification HV targets** (5 min)
   ```bash
   python scripts/validation/verify_hv_targets.py
   ```

2. **R√©-entra√Ænement epidermal** (~1h)
   ```bash
   python scripts/training/train_hovernet_family.py --family epidermal --epochs 50 --augment
   ```

3. **√âvaluation Ground Truth** (5 min)
   ```bash
   python scripts/evaluation/evaluate_ground_truth.py \
       --dataset_dir data/evaluation/pannuke_fold2_converted \
       --num_samples 100 \
       --output_dir results/epidermal_sobel_eval \
       --checkpoint models/checkpoints/hovernet_epidermal_best.pth \
       --family epidermal
   ```

4. **Si succ√®s (AJI >0.60) : Expansion 4 familles** (~4h)
   ```bash
   for family in glandular digestive urologic respiratory; do
       python scripts/training/train_hovernet_family.py --family $family --epochs 50 --augment
   done
   ```

---

## R√©f√©rences

**Op√©rateur Sobel:**
- Sobel, I., & Feldman, G. (1973). "A 3√ó3 isotropic gradient operator for image processing"
- Utilis√© comme d√©tecteur d'ar√™tes standard en vision par ordinateur

**MSGE (Mean Squared Gradient Error):**
- Graham et al. (2019). "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei"
- Section 3.2: "We enforce smooth gradients with MSGE loss"

**Watershed Segmentation:**
- Meyer, F. (1994). "Topographic distance and watershed lines"
- Principe : Suivre les cr√™tes de gradient pour s√©parer bassins versants (instances)

---

## Commit

```
fix: Replace simple gradients with Sobel operator in gradient_loss for sharper HV boundaries

PROBL√àME IDENTIFI√â (Expert externe):
- Diff√©rences finies simples (pixel[i+1] - pixel[i]) produisent signal ~0.01
- Dans HV maps [-1, 1], gradient_loss devient n√©gligeable
- Mod√®le n'a pas de pression pour cr√©er fronti√®res nettes
‚Üí Watershed √©choue √† s√©parer instances (AJI 0.07 vs 0.80 cible)

SOLUTION:
- Remplacer finite differences par op√©rateur Sobel (3√ó3)
- Sobel amplifie gradients 2-3√ó (convolution avec poids [-1,0,1])
- Force mod√®le √† cr√©er contours ferm√©s autour des noyaux

IMPACT ATTENDU:
- HV gradients plus nets ‚Üí watershed plus efficace
- AJI 0.07 ‚Üí >0.60 (gain +700%)
- PQ 0.10 ‚Üí >0.70 (gain +600%)
```
