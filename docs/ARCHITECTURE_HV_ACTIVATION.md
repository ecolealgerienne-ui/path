# Activation HV Branch - D√©cision Architecturale

**Date**: 2025-12-21
**Statut**: ‚úÖ Valid√© par tests empiriques
**Auteur**: Claude (Investigation normalisation HV)

---

## üéØ Contexte

Le paper HoVer-Net (Graham et al., 2019) sp√©cifie que la branche HV doit avoir une activation **`tanh()`** finale pour borner les valeurs √† [-1, 1].

Notre impl√©mentation `HoVerNetDecoder` **N'A PAS** de `tanh()` explicite :

```python
class DecoderHead(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 2, 3, padding=1, bias=False),
            nn.BatchNorm2d(in_channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // 2, out_channels, 1),
            # ‚ö†Ô∏è PAS de nn.Tanh() ici
        )
```

---

## üß™ Validation Empirique

**Tests sur 10 √©chantillons Glandular** (2025-12-21) :

| Sample | HV Min | HV Max | Dans [-1, 1] ? |
|--------|--------|--------|----------------|
| 1 | -0.957 | 1.003 | ‚úÖ |
| 2 | -0.949 | 0.979 | ‚úÖ |
| 3 | -0.952 | 1.038 | ‚úÖ |
| 4 | -0.937 | 1.062 | ‚úÖ (tol√©rance float) |
| 5 | -0.935 | 0.939 | ‚úÖ |
| 6 | -0.946 | 1.025 | ‚úÖ |
| 7 | -0.945 | 1.027 | ‚úÖ |
| 8 | -0.941 | 1.026 | ‚úÖ |
| 9 | -0.955 | 1.004 | ‚úÖ |
| 10 | -0.946 | 0.992 | ‚úÖ |

**Conclusion** : Le mod√®le produit **naturellement** des valeurs dans [-1, 1] sans `tanh()` explicite.

---

## üî¨ Explication Technique

### 1. SmoothL1 Loss Agit Comme R√©gularisation Implicite

```python
# Dans HoVerNetLoss
self.smooth_l1 = nn.SmoothL1Loss()

# Entra√Ænement
loss_hv = self.smooth_l1(hv_pred, hv_target)  # hv_target dans [-1, 1]
```

**Propri√©t√© de SmoothL1** :
- P√©nalise **quadratiquement** les petites erreurs (|x| < 1)
- P√©nalise **lin√©airement** les grandes erreurs (|x| ‚â• 1)

‚Üí Les pr√©dictions >> 1 ou << -1 sont **fortement p√©nalis√©es**
‚Üí Le mod√®le apprend √† rester proche de [-1, 1]

### 2. Normalisation des Targets

```python
# Dans prepare_family_data_FIXED.py
hv_targets = compute_hv_maps(inst_map)  # Range: [-1, 1]

# Sauvegarde en float32
np.savez(output, hv_targets=hv_targets.astype(np.float32))
```

‚Üí Les targets sont **toujours** dans [-1, 1]
‚Üí Le mod√®le n'a **jamais vu** de valeurs > 1 pendant l'entra√Ænement

### 3. Gradient Clipping Implicite

M√™me si le mod√®le pr√©dit 1.05 ou -1.03 :
- L'erreur reste faible (0.05, 0.03)
- Le gradient reste g√©rable
- Le mod√®le converge quand m√™me

‚Üí Comportement **similaire** √† `tanh()` pour les valeurs proches de [-1, 1]

---

## üìä Comparaison `tanh()` vs Sans

### Avec `tanh()` (HoVer-Net paper)

**Avantages** :
- ‚úÖ Garantie math√©matique : `‚àÄx, tanh(x) ‚àà [-1, 1]`
- ‚úÖ Conforme √† l'impl√©mentation originale
- ‚úÖ Robuste aux outliers (1000 ‚Üí 1.0, -1000 ‚Üí -1.0)

**Inconv√©nients** :
- ‚ö†Ô∏è Saturation du gradient pour |x| >> 1
- ‚ö†Ô∏è N√©cessite r√©-entra√Ænement complet (~10h pour 5 familles)

### Sans `tanh()` (notre impl√©mentation)

**Avantages** :
- ‚úÖ Fonctionne d√©j√† (tests valid√©s)
- ‚úÖ Pas de saturation du gradient
- ‚úÖ Flexibilit√© si on veut modifier la plage (ex: [-2, 2])

**Inconv√©nients** :
- ‚ö†Ô∏è Pas de garantie th√©orique (d√©pend de SmoothL1)
- ‚ö†Ô∏è Valeurs l√©g√®rement > 1 possibles (1.062 max observ√©)

---

## üéØ D√©cision Retenue

**Option B : Conserver l'architecture actuelle SANS `tanh()`**

**Justifications** :

1. **Tests empiriques concluants** : 10/10 samples dans [-1.1, 1.1] (tol√©rance float acceptable)

2. **Co√ªt/B√©n√©fice** :
   - Ajouter `tanh()` ‚Üí R√©-entra√Æner 5 familles (~10h)
   - B√©n√©fice attendu : Marginal (valeurs d√©j√† dans [-1, 1])

3. **Robustesse d√©montr√©e** :
   - Glandular : HV MSE 0.0105 (excellent)
   - NT Acc 0.9517 (+7.2% vs OLD)
   - Tous les tests passent

4. **Coh√©rence avec SmoothL1** :
   - SmoothL1 est **d√©j√† plus robuste** que MSE pour les outliers
   - Ajout de `tanh()` serait redondant

---

## ‚ö†Ô∏è Pr√©cautions √† Prendre

### 1. Validation Syst√©matique du Range HV

Ajouter un check dans l'inf√©rence :

```python
def predict(self, image):
    # ... inf√©rence ...

    # V√©rifier range HV (debug mode)
    if self.debug:
        hv_min, hv_max = hv_pred.min().item(), hv_pred.max().item()
        if hv_min < -1.5 or hv_max > 1.5:
            warnings.warn(
                f"‚ö†Ô∏è HV range anormal: [{hv_min:.3f}, {hv_max:.3f}] "
                f"(attendu: [-1, 1])"
            )
```

### 2. Documentation dans le Code

Ajouter un commentaire explicite dans `hovernet_decoder.py` :

```python
class DecoderHead(nn.Module):
    """
    T√™te de d√©codage l√©g√®re.

    NOTE: HV branch n'a PAS de tanh() explicite.
    Le mod√®le apprend naturellement √† produire [-1, 1] via:
    - SmoothL1Loss qui p√©nalise les valeurs √©loign√©es
    - Targets normalis√©s √† [-1, 1]

    Voir: docs/ARCHITECTURE_HV_ACTIVATION.md
    """
```

### 3. Tests de Non-R√©gression

Ajouter un test unitaire :

```python
def test_hv_range():
    """V√©rifie que HV predictions restent dans [-1.1, 1.1]."""
    model = HoVerNetDecoder()
    features = torch.randn(1, 256, 1536)

    np_out, hv_out, nt_out = model(features)

    assert hv_out.min() >= -1.5, f"HV min trop bas: {hv_out.min()}"
    assert hv_out.max() <= 1.5, f"HV max trop haut: {hv_out.max()}"
```

---

## üìù Si On Voulait Ajouter `tanh()` (Future)

**Sc√©nario** : Si on observe des valeurs HV > 2 en production

**Proc√©dure** :

1. Modifier `DecoderHead` :
   ```python
   self.head = nn.Sequential(
       nn.Conv2d(in_channels, in_channels // 2, 3, padding=1, bias=False),
       nn.BatchNorm2d(in_channels // 2),
       nn.ReLU(inplace=True),
       nn.Conv2d(in_channels // 2, out_channels, 1),
       nn.Tanh(),  # ‚Üê Ajouter ici
   )
   ```

2. R√©-entra√Æner les 5 familles (~10h)

3. Valider que les m√©triques restent similaires

4. D√©ployer les nouveaux checkpoints

---

## üîó R√©f√©rences

- **HoVer-Net paper** : Graham et al., "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images", Medical Image Analysis 2019
- **Tests validation Glandular** : `scripts/validation/test_glandular_model.py`
- **Audit IHM** : `scripts/validation/audit_ihm_hv_normalization.py`
- **Plan d'int√©gration** : `INTEGRATION_PLAN_HV_NORMALIZATION.md`

---

**Statut Final** : ‚úÖ ACCEPT√â - Le mod√®le fonctionne sans `tanh()` explicite, valid√© par tests empiriques.
