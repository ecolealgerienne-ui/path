# R√©ponse au Diagnostic Expert ‚Äî V√©rification 3 Points Critiques

**Date:** 2025-12-23
**Expert:** Analyse externe d√©taill√©e des pi√®ges math√©matiques invisibles

---

## R√©capitulatif des 3 Probl√®mes Identifi√©s par l'Expert

| # | Probl√®me | Statut | Action |
|---|----------|--------|--------|
| 1 | **Gradient Loss faible** (signal ~0.01) | ‚úÖ CORRIG√â | Sobel impl√©ment√© (commit c36bc17) |
| 2 | **SmoothL1 vs MSE** (conflit fonctions perte) | ‚úÖ CORRIG√â | MSE exclusif sur HV (commit bd9d3f6) |
| 3 | **Normalisation HV targets** | ‚è≥ √Ä V√âRIFIER | Script cr√©√© (verify_hv_targets.py) |

---

## Point 1 : Gradient Loss Faible ‚úÖ R√âSOLU

### Diagnostic Expert

> "Sur une image normalis√©e, la diff√©rence entre deux pixels voisins est infime (ex: 0.005). √âlev√©e au carr√© dans une MSE, cette valeur devient quasiment nulle (0.000025).
>
> L'impact : Ton optimiseur 'n'entend pas' le signal de s√©paration."

### Solution Impl√©ment√©e : Op√©rateur Sobel

**Commit:** `c36bc17` ‚Äî "Replace simple gradients with Sobel operator"

**Changement:**

```python
# AVANT (diff√©rences finies simples)
pred_grad_h = pred[:, :, :, 1:] - pred[:, :, :, :-1]  # Signal ~0.01

# APR√àS (op√©rateur Sobel 3√ó3)
sobel_h = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
pred_grad_h = F.conv2d(pred_reshaped, sobel_h, padding=1)  # Signal ~0.02-0.04 (2-4√ó amplifi√©)
```

**Impact attendu:**
- Signal gradient_loss passe de ~0.0001 √† ~0.0004 (4√ó plus fort)
- Optimiseur re√ßoit pression significative pour cr√©er fronti√®res nettes
- HV gradients deviennent "cercles ferm√©s autour de chaque noyau" (expert)

**Statut:** ‚úÖ Impl√©ment√©, en attente de r√©-entra√Ænement

---

## Point 2 : SmoothL1 vs MSE ‚úÖ R√âSOLU

### Diagnostic Expert

> "La SmoothL1 est con√ßue pour √™tre 'douce' avec les grandes erreurs (elle devient lin√©aire). Or, pour l'AJI, une fusion de noyaux est une erreur critique qui doit √™tre punie s√©v√®rement.
>
> Solution : Utilise exclusivement une MSELoss masqu√©e pour la branche HV."

### Solution Impl√©ment√©e : MSE Masqu√©e

**Commit:** `bd9d3f6` ‚Äî "Replace SmoothL1 with MSE in gradient_loss"

**Changement:**

```python
# AVANT (SmoothL1 ‚Äî indulgente avec grandes erreurs)
hv_l1_sum = F.smooth_l1_loss(hv_pred_masked, hv_target_masked, reduction='sum')

# APR√àS (MSE ‚Äî punition quadratique pour toutes erreurs)
hv_mse_sum = F.mse_loss(hv_pred_masked, hv_target_masked, reduction='sum')
```

**Masquage impl√©ment√©:**

```python
# Ligne 325-334 hovernet_decoder.py
mask = np_target.float().unsqueeze(1)  # (B, 1, H, W)

if mask.sum() > 0:
    hv_pred_masked = hv_pred * mask
    hv_target_masked = hv_target * mask

    hv_mse_sum = F.mse_loss(hv_pred_masked, hv_target_masked, reduction='sum')
    hv_l1 = hv_mse_sum / (mask.sum() * 2)  # *2 car 2 canaux (H, V)
```

**Impact valid√©:**
- Force mod√®le √† se concentrer sur topologie **interne** des cellules
- Ignore background vide (70-80% des pixels)
- Punition quadratique pour fusion de noyaux

**Statut:** ‚úÖ Impl√©ment√© et valid√© (HV MSE 0.30 ‚Üí 0.05)

---

## Point 3 : Normalisation HV Targets ‚è≥ √Ä V√âRIFIER

### Diagnostic Expert

> "Si tes fichiers .npz contiennent des valeurs HV entre 0 et 255 et que ton mod√®le finit par un nn.Tanh() (qui sort entre -1 et 1), le mod√®le ne pourra jamais atteindre la cible.
>
> Action : Assure-toi que dans ton Dataset, tu divises tes targets HV par 127.5 puis soustrais 1.0."

### V√©rification du Code Actuel

**1. G√©n√©ration des targets (prepare_family_data_FIXED.py):**

```python
# Ligne 29-76: compute_hv_maps()
hv_map = np.zeros((2, h, w), dtype=np.float32)  # ‚úÖ float32

# Normalisation explicite [-1, 1]
if max_dist_y > 0:
    y_dist = y_dist / max_dist_y  # ‚úÖ Division par max
if max_dist_x > 0:
    x_dist = x_dist / max_dist_x  # ‚úÖ Division par max

hv_map[0, y_coords, x_coords] = x_dist  # H
hv_map[1, y_coords, x_coords] = y_dist  # V
```

**Conclusion:** Targets g√©n√©r√©s dans [-1, 1] ‚úÖ

**2. Sortie du mod√®le (hovernet_decoder.py):**

```python
# Ligne 120: Branche HV
nn.Tanh()  # OBLIGATOIRE: forcer HV dans [-1, 1] pour matcher targets
```

**Conclusion:** Mod√®le pr√©dit dans [-1, 1] ‚úÖ

**3. Validation automatique (preprocessing.py):**

```python
# Ligne 88-99: validate_targets()
if hv_target.dtype != fmt.hv_dtype:  # V√©rifie float32
    errors.append(...)

if hv_target.dtype == np.int8:  # D√©tecte Bug #3
    errors.append("HV dtype est int8 [-127, 127] au lieu de float32 [-1, 1] !")

if hv_target.min() < fmt.hv_min - 0.1 or hv_target.max() > fmt.hv_max + 0.1:
    errors.append(...)  # V√©rifie range [-1, 1]
```

**Conclusion:** Validation automatique en place ‚úÖ

### Script de V√©rification Cr√©√©

**Fichier:** `scripts/validation/verify_hv_targets.py`

**Usage:**

```bash
conda activate cellvit
python scripts/validation/verify_hv_targets.py
```

**Sortie attendue:**

```
üîç V√©rification: epidermal_targets.npz
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HV Targets:
  Dtype:  float32
  Range:  [-1.0000, 1.0000]
  Mean:   0.0000
  Std:    0.5350

‚úÖ VALIDATION OK

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéâ TOUS LES FICHIERS SONT VALIDES

Les HV targets sont bien normalis√©s [-1, 1] en float32.
Le probl√®me de AJI/PQ vient donc bien de la gradient_loss faible.
```

**Action:** Ex√©cuter ce script pour confirmation d√©finitive.

**Statut:** ‚è≥ En attente d'ex√©cution

---

## Points Additionnels Soulev√©s par l'Expert

### A. Poids Gradient Loss (Facteur 10-20)

**Citation expert:**
> "Correction : Tu dois amplifier ce signal. La litt√©rature recommande d'augmenter le poids de la gradient_loss (souvent par un facteur 10 ou 20 par rapport √† la MSE classique)."

**Situation actuelle:**

```python
# Ligne 342 hovernet_decoder.py
hv_gradient = self.gradient_loss(hv_pred, hv_target, mask=mask)
hv_loss = hv_l1 + 0.5 * hv_gradient  # ‚Üê Poids 0.5
```

**Recommandation expert:** Poids 5.0 √† 10.0 (facteur 10-20)

**‚ö†Ô∏è Attention:** Avec Sobel, le signal est d√©j√† amplifi√© 2-4√ó. Donc :
- Poids actuel 0.5 avec Sobel ‚âà Poids 1.0-2.0 avec diff√©rences finies
- Tester d'abord avec poids **2.0** (4√ó augmentation)
- Si insuffisant, augmenter progressivement √† 5.0

**Proposition de test:**

```python
# Option 1: Poids conservateur (recommand√© pour premier test)
hv_loss = hv_l1 + 2.0 * hv_gradient  # 4√ó augmentation

# Option 2: Poids agressif (si Option 1 insuffisante)
hv_loss = hv_l1 + 5.0 * hv_gradient  # 10√ó augmentation
```

**M√©thode:** Ajouter flag `--gradient_weight` au script d'entra√Ænement.

### B. Resize Bilin√©aire sur HV Targets

**Citation expert:**
> "V√©rifie que tu n'utilises pas de Resize bilin√©aire sur les targets HV. Le resize casse les pentes math√©matiques et rend l'apprentissage impossible."

**Situation actuelle:**

```python
# Ligne 176-181 preprocessing.py
hv_resized_t = F.interpolate(
    hv_t,
    size=(target_size, target_size),
    mode='bilinear',  # Gradients ‚Üí bilinear
    align_corners=False
)
```

**Analyse:**

| Mode | Avantages | Inconv√©nients | Verdict |
|------|-----------|---------------|---------|
| `nearest` | Pr√©serve valeurs exactes | Cr√©e discontinuit√©s/escaliers | ‚ùå Mauvais pour gradients |
| `bilinear` | Transitions lisses | Peut introduire valeurs hors range | ‚ö†Ô∏è Acceptable si valid√© |
| `bicubic` | Transitions tr√®s lisses | Plus co√ªteux | ‚ö†Ô∏è Alternative possible |

**Recommandation:** Garder `bilinear` MAIS v√©rifier que :
1. Apr√®s resize, HV reste dans [-1, 1] (validation automatique en place)
2. Les gradients Sobel restent significatifs apr√®s resize

**Test de validation:**

```python
# Test : V√©rifier que resize ne d√©grade pas les gradients
hv_256 = targets['hv_targets'][0]  # (2, 256, 256)
hv_224 = resize_targets(...)[1]    # (2, 224, 224)

# Calculer magnitude gradient avant/apr√®s resize
grad_mag_256 = np.sqrt(sobel_h(hv_256[0])**2 + sobel_v(hv_256[0])**2).mean()
grad_mag_224 = np.sqrt(sobel_h(hv_224[0])**2 + sobel_v(hv_224[0])**2).mean()

ratio = grad_mag_224 / grad_mag_256
# Si ratio > 0.8 ‚Üí Resize OK
# Si ratio < 0.5 ‚Üí Resize d√©grade trop les gradients
```

**Statut:** ‚è≥ Test √† cr√©er si probl√®me persiste apr√®s r√©-entra√Ænement Sobel

### C. Test de Magnitude pred_hv

**Citation expert:**
> "Test de Magnitude : Affiche la valeur maximale de ta pred_hv. Si elle ne d√©passe jamais 0.2, c'est que ton Tanh() sature √† cause d'un mauvais scaling initial."

**Diagnostic:** Si `pred_hv.max() < 0.2`, le mod√®le n'apprend pas √† utiliser tout le range [-1, 1].

**Causes possibles:**
1. Poids initialis√©s trop petits ‚Üí Tanh sature pr√®s de 0
2. Gradient vanishing dans les premi√®res couches
3. Learning rate trop faible

**Test √† cr√©er:**

```python
# Ajouter dans la boucle d'entra√Ænement (train_hovernet_family.py)
with torch.no_grad():
    hv_pred_max = hv_pred.abs().max().item()
    hv_pred_mean = hv_pred.abs().mean().item()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}: HV pred max={hv_pred_max:.3f}, mean={hv_pred_mean:.3f}")

        if hv_pred_max < 0.2:
            print("‚ö†Ô∏è  WARNING: HV predictions saturating near 0!")
            print("    Possible causes: small weight init, vanishing gradients")
```

**Valeurs attendues:**
- `hv_pred_max` : 0.8 - 1.0 (utilise presque tout le range Tanh)
- `hv_pred_mean` : 0.2 - 0.5 (valeurs moyennes raisonnables)

**Si saturation d√©tect√©e:**
1. Augmenter learning rate de 1e-4 √† 5e-4
2. Changer initialisation poids (Xavier ‚Üí Kaiming)
3. Ajouter BatchNorm avant Tanh

**Statut:** ‚è≥ Test √† cr√©er pour diagnostic approfondi

---

## R√©sum√© : 3 Niveaux de Fix

### Niveau 1 : D√âJ√Ä IMPL√âMENT√â ‚úÖ
- ‚úÖ Sobel gradient_loss (signal 2-4√ó amplifi√©)
- ‚úÖ MSE masqu√©e (punition quadratique)
- ‚úÖ Validation HV targets automatique

**Action:** R√©-entra√Æner avec ces fixes

### Niveau 2 : SI NIVEAU 1 INSUFFISANT ‚è≥
- ‚è≥ Augmenter poids gradient_loss (0.5 ‚Üí 2.0 ou 5.0)
- ‚è≥ Test magnitude pred_hv (d√©tecter saturation Tanh)
- ‚è≥ Validation resize bilin√©aire (ratio gradients >0.8)

**Action:** Tests diagnostiques apr√®s premier r√©-entra√Ænement

### Niveau 3 : SI NIVEAU 2 INSUFFISANT ‚ö†Ô∏è
- Changer resize bilin√©aire ‚Üí bicubic
- Augmenter learning rate (1e-4 ‚Üí 5e-4)
- Changer initialisation poids (Xavier ‚Üí Kaiming)

**Action:** Modifications architecturales profondes

---

## Commandes d'Ex√©cution

### 1. V√©rification HV Targets (5 min)

```bash
conda activate cellvit
python scripts/validation/verify_hv_targets.py
```

**Attendu:** Tous fichiers ‚úÖ VALIDATION OK

### 2. R√©-entra√Ænement avec Sobel (Niveau 1) ‚Äî 1h

```bash
python scripts/training/train_hovernet_family.py \
    --family epidermal \
    --epochs 50 \
    --augment \
    --lambda_np 1.0 \
    --lambda_hv 2.0 \
    --lambda_nt 1.0
```

**M√©triques attendues:**
- NP Dice: ~0.95 (stable)
- HV MSE: 0.05-0.08 (peut augmenter l√©g√®rement, c'est normal)
- NT Acc: ~0.87 (stable)

### 3. √âvaluation Ground Truth (5 min)

```bash
python scripts/evaluation/evaluate_ground_truth.py \
    --dataset_dir data/evaluation/pannuke_fold2_converted \
    --num_samples 100 \
    --output_dir results/epidermal_sobel_eval \
    --checkpoint models/checkpoints/hovernet_epidermal_best.pth \
    --family epidermal
```

**Cibles:**
- **AJI:** >0.60 (actuellement 0.07)
- **PQ:** >0.70 (actuellement 0.10)
- Rappel: >80% (actuellement 6.93%)

### 4. Si AJI/PQ < 0.60 : Tester Niveau 2

**Ajouter flag gradient_weight:**

```bash
# Modifier hovernet_decoder.py ligne 342
hv_loss = hv_l1 + 2.0 * hv_gradient  # Augment√© de 0.5 ‚Üí 2.0

# R√©-entra√Æner
python scripts/training/train_hovernet_family.py --family epidermal --epochs 50 --augment
```

**Ajouter logging magnitude pred_hv:**

```bash
# Modifier train_hovernet_family.py (ajouter dans loop)
if batch_idx % 50 == 0:
    print(f"HV pred max={hv_pred.abs().max():.3f}")
```

---

## Tableau Comparatif : Avant/Apr√®s

| √âl√©ment | Avant (Simple Grad) | Apr√®s (Sobel) | Expert Vis√© |
|---------|---------------------|---------------|-------------|
| **NP (Noyaux)** | Formes en "huit" coll√©es | Pastilles nettes et s√©par√©es | ‚úÖ |
| **HV Maps** | Nuages de couleurs ternes | Gradients rouge/bleu vifs et tranch√©s | ‚úÖ |
| **Gradients** | Lignes fragment√©es | Cercles ferm√©s autour de chaque noyau | ‚úÖ |
| **Signal gradient_loss** | ~0.0001 (n√©gligeable) | ~0.0004 (4√ó amplifi√©) | ‚úÖ |
| **AJI** | 0.07 | **Cible: >0.60** | ‚è≥ √Ä valider |
| **PQ** | 0.10 | **Cible: >0.70** | ‚è≥ √Ä valider |

---

## R√©f√©rences Expert

**Sobel Operator:**
- Recommand√© pour amplifier signal gradient
- Litt√©rature: "utiliser un noyau de Sobel"

**Poids Gradient Loss:**
- Litt√©rature: "augmenter le poids par un facteur 10 ou 20"
- Graham et al. (2019): MSGE n√©cessaire pour s√©paration instances

**MSE Masqu√©e:**
- "Le masque doit limiter le calcul uniquement aux pixels des noyaux"
- "Force le mod√®le √† se concentrer sur la topologie interne"

**Normalisation HV:**
- "Divises tes targets HV par 127.5 puis soustrais 1.0"
- Range [-1, 1] parfaite n√©cessaire pour Watershed

---

## Conclusion

**Niveau 1 (Sobel + MSE masqu√©e) : PR√äT POUR TEST**

Les deux premiers probl√®mes identifi√©s par l'expert sont r√©solus. Le troisi√®me (normalisation HV) sera v√©rifi√© par script.

**Si AJI >0.60 apr√®s r√©-entra√Ænement : ‚úÖ SUCC√àS**
- Expansion aux 4 autres familles (~4h)
- Documentation compl√®te
- Publication r√©sultats

**Si AJI <0.60 apr√®s r√©-entra√Ænement : Passer au Niveau 2**
- Augmenter poids gradient_loss (2.0 ou 5.0)
- Test magnitude pred_hv
- Diagnostic approfondi resize bilin√©aire

**Prochaine action imm√©diate:** Ex√©cuter `verify_hv_targets.py` puis r√©-entra√Æner.
