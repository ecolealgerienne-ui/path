# PLAN D'IMPL√âMENTATION: Magnitude Loss (Solution A)

**Date:** 2025-12-24
**Objectif:** Forcer le mod√®le √† pr√©dire des gradients HV FORTS (magnitude >0.4)
**Gain attendu:** AJI 0.09 ‚Üí 0.50-0.70 (gain 5-7√ó)

---

## üìã MODIFICATIONS REQUISES

### Fichier: `src/models/hovernet_decoder.py`

---

### √âTAPE 1: Ajouter m√©thode magnitude_loss()

**Localisation:** Apr√®s `gradient_loss()` (ligne ~300)

**Code √† ajouter:**

```python
def magnitude_loss(
    self,
    hv_pred: torch.Tensor,
    hv_target: torch.Tensor,
    mask: torch.Tensor = None
) -> torch.Tensor:
    """
    Force le mod√®le √† pr√©dire des gradients FORTS aux fronti√®res.

    PROBL√àME R√âSOLU:
    - La loss actuelle (MSE + gradient) ne p√©nalise PAS magnitude faible
    - Le mod√®le apprend √† pr√©dire des HV maps LISSES (magnitude 0.04 vs targets 0.77)
    - Watershed ne peut pas s√©parer instances (AJI 0.09 catastrophique)

    SOLUTION:
    - MSE sur la MAGNITUDE (sqrt(H¬≤ + V¬≤)) au lieu des composantes s√©par√©es
    - Force le mod√®le √† pr√©dire des valeurs √âLEV√âES (proches des targets)
    - Gain attendu: magnitude 0.04 ‚Üí 0.40-0.60 (10-15√ó)

    Args:
        hv_pred: Pr√©dictions HV (B, 2, H, W) - float [-1, 1]
        hv_target: Targets HV (B, 2, H, W) - float [-1, 1]
        mask: Masque noyaux (B, 1, H, W) - binary [0, 1]

    Returns:
        Scalar loss (MSE sur magnitudes)

    Example:
        >>> hv_pred = torch.randn(1, 2, 224, 224)  # Magnitude faible ~0.05
        >>> hv_target = torch.randn(1, 2, 224, 224) * 0.8  # Magnitude forte ~0.8
        >>> mask = torch.ones(1, 1, 224, 224)
        >>> loss = criterion.magnitude_loss(hv_pred, hv_target, mask)
        >>> # loss √©lev√© car √©cart de magnitude important
    """
    # Calculer magnitude (norme L2 des composantes H et V)
    # sqrt(H¬≤ + V¬≤) ‚àà [0, sqrt(2)] ‚âà [0, 1.41]
    mag_pred = torch.sqrt((hv_pred ** 2).sum(dim=1, keepdim=True) + 1e-8)  # (B, 1, H, W)
    mag_target = torch.sqrt((hv_target ** 2).sum(dim=1, keepdim=True) + 1e-8)

    # Masquer (calcul UNIQUEMENT sur pixels de noyaux)
    if mask is not None and mask.sum() > 0:
        mag_pred_masked = mag_pred * mask
        mag_target_masked = mag_target * mask

        # MSE avec normalisation par nombre de pixels masqu√©s
        mag_loss_sum = F.mse_loss(mag_pred_masked, mag_target_masked, reduction='sum')
        n_pixels = mask.sum()
        mag_loss = mag_loss_sum / (n_pixels + 1e-8)
    else:
        # Sans masque (fallback, ne devrait jamais arriver)
        mag_loss = F.mse_loss(mag_pred, mag_target)

    return mag_loss
```

---

### √âTAPE 2: Modifier calcul loss totale HV

**Localisation:** M√©thode `forward()`, ligne ~348

**Code AVANT:**

```python
# Gradient loss (MSGE - Graham et al.)
hv_gradient = self.gradient_loss(hv_pred, hv_target, mask=mask)
hv_loss = hv_l1 + 2.0 * hv_gradient  # √âquilibr√©: MSE + 2√ó gradient
```

**Code APR√àS:**

```python
# Gradient loss (MSGE - Graham et al.)
hv_gradient = self.gradient_loss(hv_pred, hv_target, mask=mask)

# Magnitude loss (NOUVEAU - 2025-12-24)
# Force le mod√®le √† pr√©dire gradients FORTS (magnitude >0.4)
# Targets ont magnitude 0.77 mais pr√©dictions seulement 0.04 ‚Üí ratio 0.05 (20√ó trop faible!)
# Solution: MSE sur magnitude pour forcer le mod√®le √† "muscler" ses pr√©dictions
hv_magnitude = self.magnitude_loss(hv_pred, hv_target, mask=mask)

# Loss totale HV (3 termes)
hv_loss = hv_l1 + 2.0 * hv_gradient + 1.0 * hv_magnitude
#                                      ^^^^^^^^^^^^^^^^^^
#                                      NOUVEAU terme
```

---

### √âTAPE 3: Ajouter monitoring magnitude loss

**Localisation:** Retour `forward()`, lignes ~369-385

**Modifications:**

#### Mode adaptive (ligne 369):

```python
return total, {
    'np': np_loss.item(),
    'hv': hv_loss.item(),
    'hv_l1': hv_l1.item(),           # AJOUTER (d√©tail MSE)
    'hv_gradient': hv_gradient.item(),  # AJOUTER (d√©tail gradient)
    'hv_magnitude': hv_magnitude.item(),  # AJOUTER (d√©tail magnitude)
    'nt': nt_loss.item(),
    'w_np': w_np,
    'w_hv': w_hv,
    'w_nt': w_nt,
}
```

#### Mode poids fixes (ligne 381):

```python
return total, {
    'np': np_loss.item(),
    'hv': hv_loss.item(),
    'hv_l1': hv_l1.item(),           # AJOUTER
    'hv_gradient': hv_gradient.item(),  # AJOUTER
    'hv_magnitude': hv_magnitude.item(),  # AJOUTER
    'nt': nt_loss.item(),
}
```

---

## üìä POIDS RECOMMAND√âS

```python
hv_loss = 1.0 * hv_l1 + 2.0 * hv_gradient + 1.0 * hv_magnitude
#         ^^^^^^^       ^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^
#         MSE base      Force variations    Force magnitude
#         (accuracy)    (sharpness)         (strength)
```

**Justification:**
- `1.0 √ó hv_l1`: Assure pr√©dictions pr√©cises (accuracy)
- `2.0 √ó hv_gradient`: Force variations spatiales (sharpness) - d√©j√† pr√©sent
- `1.0 √ó hv_magnitude`: Force gradients forts (strength) - NOUVEAU

**Alternative (si magnitude loss domine trop):**
```python
hv_loss = 1.0 * hv_l1 + 2.0 * hv_gradient + 0.5 * hv_magnitude
```

**Test apr√®s premier training:** Si magnitude reste <0.2, augmenter √† `2.0 √ó hv_magnitude`

---

## üî¨ VALIDATION POST-IMPL√âMENTATION

### Test 1: V√©rifier magnitude loss fonctionne

**Script de test (√† cr√©er):** `scripts/validation/test_magnitude_loss.py`

```python
import torch
from src.models.hovernet_decoder import HoVerNetLoss

# Cr√©er loss
criterion = HoVerNetLoss(lambda_np=1.0, lambda_hv=2.0, lambda_nt=1.0)

# Cas 1: Magnitude faible (pred) vs forte (target)
hv_pred_weak = torch.randn(1, 2, 224, 224) * 0.1  # Magnitude ~0.1
hv_target_strong = torch.randn(1, 2, 224, 224) * 0.8  # Magnitude ~0.8
mask = torch.ones(1, 1, 224, 224)

mag_loss_high = criterion.magnitude_loss(hv_pred_weak, hv_target_strong, mask)
print(f"Magnitude loss (faible‚Üíforte): {mag_loss_high:.4f}")  # Attendu: >0.5

# Cas 2: Magnitude forte (pred) vs forte (target)
hv_pred_strong = torch.randn(1, 2, 224, 224) * 0.8
mag_loss_low = criterion.magnitude_loss(hv_pred_strong, hv_target_strong, mask)
print(f"Magnitude loss (forte‚Üíforte): {mag_loss_low:.4f}")  # Attendu: <0.1

assert mag_loss_high > mag_loss_low * 5, "Magnitude loss ne p√©nalise pas assez!"
print("‚úÖ Magnitude loss fonctionne correctement")
```

---

### Test 2: R√©-entra√Æner epidermal

**Commande:**

```bash
python scripts/training/train_hovernet_family.py \
    --family epidermal \
    --epochs 50 \
    --augment \
    --lambda_hv 2.0 \
    --batch_size 16 \
    --lr 1e-4
```

**M√©triques √† surveiller (logs training):**

```
Epoch 10/50
  hv_l1:        0.05   (MSE base - doit rester ~0.05)
  hv_gradient:  0.08   (Gradient loss - doit rester ~0.08)
  hv_magnitude: 0.30   (NOUVEAU - doit DIMINUER au fil des epochs)
  hv_loss:      0.51   (Somme: 0.05 + 2√ó0.08 + 1√ó0.30)

Epoch 50/50
  hv_l1:        0.04
  hv_gradient:  0.06
  hv_magnitude: 0.10   ‚Üê DIMINUTION = mod√®le apprend √† pr√©dire magnitude forte!
  hv_loss:      0.26
```

**Bon signe:** hv_magnitude diminue au fil des epochs (mod√®le pr√©dit des magnitudes plus proches des targets)

**Mauvais signe:** hv_magnitude stagne ou augmente (poids trop faible, augmenter √† 2.0)

---

### Test 3: V√©rifier magnitude pr√©dictions

**Commande:**

```bash
python scripts/evaluation/compute_hv_magnitude.py \
    --family epidermal \
    --checkpoint models/checkpoints/hovernet_epidermal_best.pth \
    --n_samples 10
```

**R√©sultat attendu:**

```
AVANT (sans magnitude loss):
  Magnitude moyenne: 0.0423
  Status: FAIL (<0.05)

APR√àS (avec magnitude loss):
  Magnitude moyenne: 0.40-0.60   ‚Üê OBJECTIF ATTEINT
  Status: SUCCESS (>0.15)
```

---

### Test 4: V√©rifier AJI Ground Truth

**Commande:**

```bash
python scripts/evaluation/test_on_training_data.py \
    --family epidermal \
    --checkpoint models/checkpoints/hovernet_epidermal_best.pth \
    --n_samples 10
```

**R√©sultat attendu:**

```
AVANT (sans magnitude loss):
  NP Dice: 0.95
  AJI:     0.09   ‚Üê CATASTROPHIQUE

APR√àS (avec magnitude loss):
  NP Dice: 0.92-0.95  (peut l√©g√®rement diminuer, acceptable)
  AJI:     0.50-0.70  ‚Üê OBJECTIF ATTEINT (gain 5-7√ó)
```

---

## üìà PROGRESSION ATTENDUE

| Phase | Magnitude Pred | HV MSE | AJI | Statut |
|-------|----------------|--------|-----|--------|
| **Baseline** (Œª_hv=2.0) | 0.022 | 0.16 | 0.09 | ‚ùå Giant Blob |
| **Œª_hv=3.0** | 0.053 | 0.16 | ~0.15 | ‚ö†Ô∏è Partiel |
| **Œª_hv=5.0** | 0.042 | 0.16 | ~0.09 | ‚ùå Plateau |
| **+ Magnitude Loss** | **0.40-0.60** | **0.20-0.25** | **0.50-0.70** | ‚úÖ **SUCC√àS** |

**Notes:**
- HV MSE peut augmenter (0.16 ‚Üí 0.20-0.25) ‚Äî C'EST NORMAL
- Pr√©dire des gradients forts est plus difficile ‚Üí MSE plus √©lev√©
- Mais AJI s'am√©liore drastiquement ‚Üí C'est ce qui compte!

---

## ‚ö†Ô∏è POINTS D'ATTENTION

### 1. Si magnitude loss domine trop

**Sympt√¥me:** HV MSE explose (>0.50), NP Dice chute (<0.85)

**Solution:** R√©duire poids magnitude loss
```python
hv_loss = hv_l1 + 2.0 * hv_gradient + 0.3 * hv_magnitude  # Au lieu de 1.0
```

---

### 2. Si magnitude stagne malgr√© la loss

**Sympt√¥me:** Apr√®s 50 epochs, magnitude reste <0.20

**Solutions possibles:**
- Augmenter poids: `2.0 * hv_magnitude` au lieu de `1.0`
- V√©rifier tanh activation (ligne 118) ‚Äî doit √™tre pr√©sent
- V√©rifier gradient clipping dans optimizer

---

### 3. Si AJI ne s'am√©liore pas malgr√© magnitude √©lev√©e

**Sympt√¥me:** Magnitude pred >0.40 mais AJI toujours ~0.15

**Diagnostic:** Probl√®me post-processing watershed
- V√©rifier param√®tres watershed (dist_threshold, min_size)
- Voir `scripts/evaluation/test_watershed_params.py`

---

## üéØ CRIT√àRES DE SUCC√àS

| M√©trique | Avant | Cible | Seuil Succ√®s |
|----------|-------|-------|--------------|
| **Magnitude pred** | 0.04 | 0.50 | **>0.40** ‚úÖ |
| **AJI** | 0.09 | 0.65 | **>0.50** ‚úÖ |
| HV MSE | 0.16 | 0.25 | <0.30 (acceptable) |
| NP Dice | 0.95 | 0.93 | >0.90 (tol√©rance -2%) |

**Si AJI >0.50 ET magnitude >0.40:** ‚úÖ **SUCC√àS COMPLET**

**Si magnitude >0.40 MAIS AJI <0.40:** ‚ö†Ô∏è **SUCC√àS PARTIEL** (probl√®me post-processing)

**Si magnitude <0.30:** ‚ùå **√âCHEC** (augmenter poids magnitude loss)

---

## üìã CHECKLIST D'IMPL√âMENTATION

- [ ] √âTAPE 1: Ajouter m√©thode `magnitude_loss()` dans hovernet_decoder.py
- [ ] √âTAPE 2: Modifier calcul `hv_loss` (ligne 348) pour inclure magnitude
- [ ] √âTAPE 3: Ajouter monitoring dans retour `forward()` (lignes 369, 381)
- [ ] √âTAPE 4: Cr√©er `test_magnitude_loss.py` pour valider fonction
- [ ] √âTAPE 5: Ex√©cuter test unitaire magnitude loss
- [ ] √âTAPE 6: R√©-entra√Æner epidermal (50 epochs)
- [ ] √âTAPE 7: V√©rifier magnitude pr√©dictions (>0.40)
- [ ] √âTAPE 8: V√©rifier AJI (>0.50)
- [ ] √âTAPE 9: Documenter r√©sultats dans CLAUDE.md

**Temps estim√© total:** 2 heures

---

**Derni√®re mise √† jour:** 2025-12-24
**Statut:** Pr√™t pour impl√©mentation
**Prochaine action:** Impl√©menter √âTAPE 1 (ajouter magnitude_loss)
