# V15.2 Cytology Architecture â€” Specification

> **Version:** 15.2-Lite
> **Date:** 2026-01-22
> **Statut:** Draft â€” En revue
> **Auteurs:** Equipe CellViT-Optimus + Expert Review
> **Timeline:** 12 semaines

---

## Executive Summary

V15.2 est une refonte majeure du pipeline cytologie, passant d'une approche "foundation model brut" (V14) Ã  une architecture **industrielle validÃ©e** combinant:

1. **Stain Normalization** â€” Robustesse inter-laboratoires
2. **YOLO Detection** â€” Localisation cellulaire rapide
3. **HoVerNet-lite** â€” Segmentation nucleus sur clusters
4. **Gated Feature Fusion** â€” Fusion adaptative visual + morpho
5. **Couche SÃ©curitÃ©** â€” Conformal Prediction + OOD

### Pourquoi V15.2?

| Limitation V14 | Solution V15.2 |
|----------------|----------------|
| H-Optimus jamais vu Pap-stain | Fine-tuning + benchmark UNI |
| 99% SIPaKMeD â‰  performance LBC | Validation sur APCData/LBC rÃ©els |
| CellPose inadaptÃ© clusters 3D | HoVerNet-lite avec HV maps |
| Concat simple features | Gated Feature Fusion |
| Pas de gestion incertitude | Conformal + OOD + Reject option |

---

## Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         V15.2 CYTOLOGY PIPELINE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         IMAGE LBC / PAP SMEAR
                        (ex: 2048Ã—1532 pixels)
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 0: STAIN NORMALIZATION (Macenko/Reinhard Pap)                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Normalisation couleur inter-laboratoires                                 â”‚
â”‚  â€¢ Correction illumination                                                  â”‚
â”‚  â€¢ Target: reference Pap standard                                           â”‚
â”‚  â€¢ Temps: ~50ms / image                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 1: DÃ‰TECTION CELLULAIRE (YOLOv8/v9)                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Input: Image normalisÃ©e                                                  â”‚
â”‚  â€¢ Output: N bounding boxes + confidence scores                             â”‚
â”‚  â€¢ Classes: {cell, debris, artifact}                                        â”‚
â”‚  â€¢ Temps: ~30-50ms / image                                                  â”‚
â”‚  â€¢ Note: YOLO = "cellular locator", pas segmentateur                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ N crops cellulaires
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 2: SEGMENTATION NUCLEUS (HoVerNet-lite + StarDist fallback)          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ StratÃ©gie hybride basÃ©e sur complexitÃ© estimÃ©e                           â”‚
â”‚  â€¢ StarDist: noyaux isolÃ©s (complexity < 0.3)                               â”‚
â”‚  â€¢ HoVerNet-lite: clusters 3D (complexity >= 0.3)                           â”‚
â”‚  â€¢ Output: Masques instance + centroÃ¯des                                    â”‚
â”‚  â€¢ Temps: ~50-100ms / crop                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ N masques nucleus
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 3: EXTRACTION PATCH 224Ã—224 (centrÃ© sur nucleus)                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Crop centrÃ© sur centroÃ¯de du masque                                      â”‚
â”‚  â€¢ Padding blanc si proche du bord                                          â”‚
â”‚  â€¢ Conservation du masque pour morphomÃ©trie                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VISUAL ENCODER             â”‚  â”‚   MORPHOMÃ‰TRIE AVANCÃ‰E (20 features)     â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚   HiÃ©rarchie:                â”‚  â”‚   Base (10):                             â”‚
â”‚   1. Encoder dÃ©diÃ© cyto      â”‚  â”‚   â€¢ area, perimeter, circularity         â”‚
â”‚   2. UNI fine-tunÃ© (LoRA)    â”‚  â”‚   â€¢ eccentricity, solidity, extent       â”‚
â”‚   3. Phikon-v2 fine-tunÃ©     â”‚  â”‚   â€¢ major/minor axis, aspect ratio       â”‚
â”‚   4. H-Optimus fine-tunÃ©     â”‚  â”‚   â€¢ compactness                          â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚   Output: 768-1536 dims      â”‚  â”‚   IntensitÃ© H-channel (5):               â”‚
â”‚                              â”‚  â”‚   â€¢ mean, std, max, min intensity        â”‚
â”‚                              â”‚  â”‚   â€¢ integrated_od (proxy ploÃ¯die)        â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚                              â”‚  â”‚   Texture GLCM (5):                      â”‚
â”‚                              â”‚  â”‚   â€¢ contrast, homogeneity, energy        â”‚
â”‚                              â”‚  â”‚   â€¢ correlation, entropy                 â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚                              â”‚  â”‚   AvancÃ©es Pap-spÃ©cifiques:              â”‚
â”‚                              â”‚  â”‚   â€¢ Variance texture nuclÃ©olaire         â”‚
â”‚                              â”‚  â”‚   â€¢ Polychromasie / gradient densitÃ©     â”‚
â”‚                              â”‚  â”‚   â€¢ N/C ratio (si cytoplasme segmentÃ©)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 4: GATED FEATURE FUSION (GFF)                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Remplace concat simple                                                   â”‚
â”‚  â€¢ g = Ïƒ(W_g Â· [f_visual; f_morpho] + b_g)                                 â”‚
â”‚  â€¢ f_fused = g âŠ™ f_visual + (1-g) âŠ™ f_morpho                               â”‚
â”‚  â€¢ PondÃ©ration adaptative par classe                                        â”‚
â”‚  â€¢ Gain attendu: +4-8% F1-score                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 5: CLASSIFICATION MLP                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Input: f_fused (768-1556 dims selon encoder)                             â”‚
â”‚  â€¢ Architecture: Linear â†’ BN â†’ ReLU â†’ Dropout (Ã—3) â†’ Linear(K)             â”‚
â”‚  â€¢ Classes Bethesda: NILM / ASC-US / ASC-H / LSIL / HSIL / SCC             â”‚
â”‚  â€¢ Output: logits + probabilitÃ©s                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 6: COUCHE SÃ‰CURITÃ‰                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Conformal Prediction: intervalles de confiance calibrÃ©s                  â”‚
â”‚  â€¢ OOD Detection: distance Mahalanobis dans espace latent                   â”‚
â”‚  â€¢ Temperature Scaling: calibration des probabilitÃ©s                        â”‚
â”‚  â€¢ Reject Option: seuil de confiance pour review humaine                    â”‚
â”‚  â€¢ Output: {Fiable / Ã€ revoir / Hors domaine}                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 7: AGRÃ‰GATION SLIDE-LEVEL (V2)                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  V1: Top-K anomalies pooling + heatmap simple                               â”‚
â”‚  V2: Attention-based MIL (CLAM / TransMIL)                                  â”‚
â”‚  â€¢ Output: Classification lame Bethesda + ROIs prioritaires                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Composant 1: Stain Normalization

### Justification

> **ProblÃ¨me:** Les images Pap/LBC varient Ã©normÃ©ment entre laboratoires (colorants, scanners, protocoles).
> **Impact:** Un modÃ¨le entraÃ®nÃ© sur un labo peut s'effondrer sur un autre (-20-30% accuracy).

### ImplÃ©mentation

```python
# src/preprocessing/stain_normalization.py

import numpy as np
from skimage import color

class MacenkoPapNormalizer:
    """
    Normalisation Macenko adaptÃ©e Pap-stain.

    DiffÃ©rences vs Macenko H&E:
    - Vecteurs stain diffÃ©rents (Papanicolaou vs H&E)
    - 3 colorants: HÃ©matoxyline, Orange G, EA (vs 2 pour H&E)
    - Reference target calibrÃ© sur lames LBC standard
    """

    # Reference vectors pour Pap-stain (Ã  calibrer sur dataset)
    PAP_REFERENCE = {
        'hematoxylin': np.array([0.65, 0.70, 0.29]),
        'orange_g': np.array([0.07, 0.99, 0.11]),
        'ea': np.array([0.27, 0.57, 0.78])
    }

    def __init__(self, target_image=None):
        """
        Args:
            target_image: Image de rÃ©fÃ©rence pour normalisation.
                         Si None, utilise PAP_REFERENCE.
        """
        if target_image is not None:
            self.target_stains = self._extract_stain_vectors(target_image)
        else:
            self.target_stains = self.PAP_REFERENCE

    def normalize(self, image: np.ndarray) -> np.ndarray:
        """
        Normalise une image Pap vers la rÃ©fÃ©rence.

        Args:
            image: Image RGB (H, W, 3), uint8

        Returns:
            normalized: Image normalisÃ©e, mÃªme shape
        """
        # 1. Conversion RGB â†’ OD (Optical Density)
        od = self._rgb_to_od(image)

        # 2. Extraction vecteurs stain de l'image source
        source_stains = self._extract_stain_vectors(image)

        # 3. DÃ©convolution
        concentrations = self._deconvolve(od, source_stains)

        # 4. Reconvolution avec vecteurs cibles
        normalized_od = self._reconvolve(concentrations, self.target_stains)

        # 5. Conversion OD â†’ RGB
        normalized = self._od_to_rgb(normalized_od)

        return normalized

    def _rgb_to_od(self, image: np.ndarray) -> np.ndarray:
        """Conversion RGB vers Optical Density."""
        image = image.astype(np.float32) / 255.0
        image = np.clip(image, 1e-6, 1.0)  # Ã‰vite log(0)
        od = -np.log(image)
        return od

    def _od_to_rgb(self, od: np.ndarray) -> np.ndarray:
        """Conversion Optical Density vers RGB."""
        rgb = np.exp(-od)
        rgb = np.clip(rgb * 255, 0, 255).astype(np.uint8)
        return rgb

    def _extract_stain_vectors(self, image: np.ndarray) -> dict:
        """
        Extrait les vecteurs stain via SVD.
        AdaptÃ© pour Pap-stain (3 composantes vs 2 pour H&E).
        """
        # ImplÃ©mentation simplifiÃ©e - Ã  affiner
        od = self._rgb_to_od(image)
        od_flat = od.reshape(-1, 3)

        # Filtrer pixels non-background
        od_mask = np.sum(od_flat, axis=1) > 0.15
        od_filtered = od_flat[od_mask]

        # SVD pour extraire composantes principales
        _, _, Vt = np.linalg.svd(od_filtered, full_matrices=False)

        # Les 2-3 premiÃ¨res composantes = vecteurs stain
        return {
            'hematoxylin': Vt[0],
            'orange_g': Vt[1] if len(Vt) > 1 else self.PAP_REFERENCE['orange_g'],
            'ea': Vt[2] if len(Vt) > 2 else self.PAP_REFERENCE['ea']
        }

    def _deconvolve(self, od, stain_vectors):
        """DÃ©convolution pour obtenir concentrations."""
        # TODO: ImplÃ©mentation complÃ¨te
        pass

    def _reconvolve(self, concentrations, target_stains):
        """Reconvolution avec vecteurs cibles."""
        # TODO: ImplÃ©mentation complÃ¨te
        pass
```

### MÃ©triques de Validation

| MÃ©trique | Calcul | Cible |
|----------|--------|-------|
| Variance couleur inter-labo | std(mean_color) avant/aprÃ¨s | RÃ©duction > 50% |
| Performance downstream | Accuracy classification | Pas de rÃ©gression |
| Artefacts visuels | Inspection manuelle | Aucun |

---

## Composant 2: DÃ©tection YOLO

### Justification

> **RÃ´le:** YOLO = "cellular locator" rapide. Il dÃ©tecte les cellules, pas les segmente.
> **Approche industrielle:** Techcyte, BD UroPath, Hologic Genius utilisent tous YOLO + segmentateur.

### Configuration

```python
# src/detection/yolo_detector.py

from ultralytics import YOLO

class CellDetector:
    """
    DÃ©tecteur de cellules basÃ© sur YOLOv8.

    Classes:
    - 0: cell (noyau + cytoplasme)
    - 1: debris
    - 2: artifact
    """

    def __init__(self, model_path: str = "models/yolo/cell_detector.pt"):
        self.model = YOLO(model_path)
        self.conf_threshold = 0.5
        self.iou_threshold = 0.45

    def detect(self, image: np.ndarray) -> list[dict]:
        """
        DÃ©tecte les cellules dans une image.

        Args:
            image: Image RGB (H, W, 3)

        Returns:
            detections: Liste de {bbox, confidence, class}
        """
        results = self.model(
            image,
            conf=self.conf_threshold,
            iou=self.iou_threshold,
            verbose=False
        )[0]

        detections = []
        for box in results.boxes:
            if box.cls == 0:  # Classe "cell" uniquement
                detections.append({
                    'bbox': box.xyxy[0].cpu().numpy(),  # [x1, y1, x2, y2]
                    'confidence': box.conf.item(),
                    'class': 'cell'
                })

        return detections

    def extract_crops(self, image: np.ndarray, detections: list,
                      margin: int = 20) -> list[np.ndarray]:
        """
        Extrait les crops autour des dÃ©tections.

        Args:
            image: Image source
            detections: Liste de dÃ©tections YOLO
            margin: Marge autour de la bbox

        Returns:
            crops: Liste de crops RGB
        """
        crops = []
        for det in detections:
            x1, y1, x2, y2 = det['bbox'].astype(int)

            # Ajouter marge
            x1 = max(0, x1 - margin)
            y1 = max(0, y1 - margin)
            x2 = min(image.shape[1], x2 + margin)
            y2 = min(image.shape[0], y2 + margin)

            crop = image[y1:y2, x1:x2]
            crops.append(crop)

        return crops
```

### EntraÃ®nement YOLO

**Dataset requis:**
- APCData avec bounding boxes (dÃ©jÃ  disponible)
- Annotations debris/artifacts (Ã  crÃ©er)

**Commande d'entraÃ®nement:**
```bash
yolo train model=yolov8m.pt data=data/yolo/cell_detection.yaml epochs=100 imgsz=640
```

---

## Composant 3: HoVerNet-lite

### Justification

> **ProblÃ¨me:** Les clusters cellulaires (HSIL, ASC-H) ont des noyaux qui se chevauchent.
> **Solution:** HoVerNet utilise des HV maps (gradients horizontaux/verticaux) pour sÃ©parer les instances.
> **Avantage:** L'Ã©quipe a dÃ©jÃ  l'expertise HoVer-Net (V13 histologie).

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HOVERNET-LITE CYTOLOGIE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ENCODER: ResNet18 (pretrained ImageNet)                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  â€¢ Conv1: 64 channels                                                       â”‚
â”‚  â€¢ Layer1: 64 channels, stride 1                                            â”‚
â”‚  â€¢ Layer2: 128 channels, stride 2                                           â”‚
â”‚  â€¢ Layer3: 256 channels, stride 2                                           â”‚
â”‚  â€¢ Layer4: 512 channels, stride 2                                           â”‚
â”‚                                                                             â”‚
â”‚  FPN: 3 niveaux (simplifiÃ© vs 5 niveaux V13)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  â€¢ P3: 256 channels, 1/8 resolution                                         â”‚
â”‚  â€¢ P4: 256 channels, 1/16 resolution                                        â”‚
â”‚  â€¢ P5: 256 channels, 1/32 resolution                                        â”‚
â”‚                                                                             â”‚
â”‚  TÃŠTES DE PRÃ‰DICTION:                                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚  â€¢ NP (Nucleus Probability): Conv â†’ Sigmoid â†’ (H, W, 1)                    â”‚
â”‚  â€¢ HV (Horizontal-Vertical): Conv â†’ Tanh â†’ (H, W, 2)                       â”‚
â”‚  â€¢ [SUPPRIMÃ‰] NT (Nucleus Type): Non nÃ©cessaire pour cytologie              â”‚
â”‚                                                                             â”‚
â”‚  POST-PROCESSING: hv_guided_watershed() [Code V13 rÃ©utilisÃ©]               â”‚
â”‚                                                                             â”‚
â”‚  STATS:                                                                     â”‚
â”‚  â€¢ ParamÃ¨tres: ~2.5M (vs ~8M HoVerNet complet)                             â”‚
â”‚  â€¢ InfÃ©rence: ~50ms / crop 224Ã—224                                          â”‚
â”‚  â€¢ VRAM: ~1.5 GB                                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation

```python
# src/models/hovernet_lite.py

import torch
import torch.nn as nn
import torchvision.models as models

class HoVerNetLite(nn.Module):
    """
    HoVerNet allÃ©gÃ© pour segmentation nucleus en cytologie.

    DiffÃ©rences vs HoVerNet V13:
    - Encoder: ResNet18 (vs ResNet50)
    - FPN: 3 niveaux (vs 5)
    - Pas de branche NT (classification noyaux)
    - Pas d'injection H-channel (Pap â‰  H&E)
    """

    def __init__(self, pretrained: bool = True):
        super().__init__()

        # Encoder ResNet18
        resnet = models.resnet18(pretrained=pretrained)
        self.conv1 = resnet.conv1
        self.bn1 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool
        self.layer1 = resnet.layer1  # 64 channels
        self.layer2 = resnet.layer2  # 128 channels
        self.layer3 = resnet.layer3  # 256 channels
        self.layer4 = resnet.layer4  # 512 channels

        # FPN 3 niveaux
        self.fpn = SimpleFPN(
            in_channels=[128, 256, 512],
            out_channels=256
        )

        # TÃªte NP (Nucleus Probability)
        self.np_head = nn.Sequential(
            nn.Conv2d(256, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )

        # TÃªte HV (Horizontal-Vertical gradients)
        self.hv_head = nn.Sequential(
            nn.Conv2d(256, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 2, 1),
            nn.Tanh()
        )

    def forward(self, x: torch.Tensor) -> dict:
        """
        Forward pass.

        Args:
            x: Input tensor (B, 3, H, W)

        Returns:
            dict: {'np': (B, 1, H, W), 'hv': (B, 2, H, W)}
        """
        # Encoder
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        c1 = self.layer1(x)   # 1/4
        c2 = self.layer2(c1)  # 1/8
        c3 = self.layer3(c2)  # 1/16
        c4 = self.layer4(c3)  # 1/32

        # FPN
        fpn_out = self.fpn([c2, c3, c4])  # (B, 256, H/4, W/4)

        # Upsample to input resolution
        fpn_out = nn.functional.interpolate(
            fpn_out, scale_factor=4, mode='bilinear', align_corners=False
        )

        # Predictions
        np_pred = self.np_head(fpn_out)
        hv_pred = self.hv_head(fpn_out)

        return {'np': np_pred, 'hv': hv_pred}


class SimpleFPN(nn.Module):
    """FPN simplifiÃ© 3 niveaux."""

    def __init__(self, in_channels: list, out_channels: int):
        super().__init__()

        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(c, out_channels, 1) for c in in_channels
        ])

        self.output_convs = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, 3, padding=1)
            for _ in in_channels
        ])

    def forward(self, features: list) -> torch.Tensor:
        """
        Args:
            features: [c2, c3, c4] from encoder

        Returns:
            Fused feature map at c2 resolution
        """
        # Lateral connections
        laterals = [conv(f) for conv, f in zip(self.lateral_convs, features)]

        # Top-down pathway
        for i in range(len(laterals) - 1, 0, -1):
            laterals[i-1] = laterals[i-1] + nn.functional.interpolate(
                laterals[i], scale_factor=2, mode='nearest'
            )

        # Output convolutions
        outputs = [conv(lat) for conv, lat in zip(self.output_convs, laterals)]

        # Return finest resolution
        return outputs[0]
```

### StratÃ©gie Hybride (StarDist fallback)

```python
# src/segmentation/hybrid_segmenter.py

from stardist.models import StarDist2D
from src.models.hovernet_lite import HoVerNetLite
from src.postprocessing.watershed import hv_guided_watershed

class HybridNucleusSegmenter:
    """
    Segmentation hybride: StarDist pour noyaux isolÃ©s, HoVerNet-lite pour clusters.
    """

    def __init__(
        self,
        hovernet_checkpoint: str,
        stardist_model: str = '2D_versatile_fluo',
        complexity_threshold: float = 0.3
    ):
        self.hovernet = HoVerNetLite()
        self.hovernet.load_state_dict(torch.load(hovernet_checkpoint))
        self.hovernet.eval()

        self.stardist = StarDist2D.from_pretrained(stardist_model)
        self.complexity_threshold = complexity_threshold

    def segment(self, image: np.ndarray) -> np.ndarray:
        """
        Segmente les noyaux avec stratÃ©gie adaptative.

        Args:
            image: Image RGB (H, W, 3)

        Returns:
            masks: Image labelisÃ©e (H, W) avec instances
        """
        complexity = self._estimate_complexity(image)

        if complexity < self.complexity_threshold:
            # Noyaux isolÃ©s â†’ StarDist (rapide)
            masks, _ = self.stardist.predict_instances(image)
        else:
            # Clusters â†’ HoVerNet-lite (prÃ©cis)
            masks = self._hovernet_segment(image)

        return masks

    def _estimate_complexity(self, image: np.ndarray) -> float:
        """
        Estime la complexitÃ© (prÃ©sence de clusters).

        BasÃ© sur:
        - DensitÃ© de noyaux estimÃ©e
        - Variance des intensitÃ©s
        """
        # Quick pass avec StarDist
        masks_quick, details = self.stardist.predict_instances(
            image,
            prob_thresh=0.3,  # Seuil bas pour tout dÃ©tecter
            nms_thresh=0.1    # NMS agressif
        )

        n_nuclei = masks_quick.max()
        area = image.shape[0] * image.shape[1]
        density = n_nuclei / area * 1e6  # Noyaux par million de pixels

        # ComplexitÃ© basÃ©e sur densitÃ©
        if density > 500:
            return 0.8  # High complexity
        elif density > 200:
            return 0.5  # Medium complexity
        else:
            return 0.2  # Low complexity

    def _hovernet_segment(self, image: np.ndarray) -> np.ndarray:
        """Segmentation via HoVerNet-lite."""
        # Preprocessing
        tensor = self._preprocess(image)

        # Inference
        with torch.no_grad():
            outputs = self.hovernet(tensor)

        # Post-processing (rÃ©utilise code V13)
        np_pred = outputs['np'][0, 0].cpu().numpy()
        hv_pred = outputs['hv'][0].permute(1, 2, 0).cpu().numpy()

        masks = hv_guided_watershed(
            np_pred=np_pred,
            hv_pred=hv_pred,
            np_threshold=0.5,
            min_size=50,
            min_distance=3,
            beta=1.0
        )

        return masks
```

### GÃ©nÃ©ration de Pseudo-Masques (pour entraÃ®nement)

> **ProblÃ¨me:** APCData n'a que des points nucleus, pas de masques pixel-level.
> **Solution:** GÃ©nÃ©rer pseudo-masques via SAM + watershed.

```python
# scripts/cytology/generate_pseudo_masks.py

from segment_anything import SamPredictor, sam_model_registry
import numpy as np

def generate_pseudo_masks(image: np.ndarray, nucleus_points: list) -> np.ndarray:
    """
    GÃ©nÃ¨re pseudo-masques Ã  partir de points nucleus.

    Pipeline:
    1. SAM avec points comme prompts
    2. Watershed pour sÃ©parer instances qui se touchent
    3. Filtrage par taille

    Args:
        image: Image RGB
        nucleus_points: Liste de (x, y) pour chaque noyau

    Returns:
        masks: Image labelisÃ©e avec instances
    """
    # Init SAM
    sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b.pth")
    predictor = SamPredictor(sam)
    predictor.set_image(image)

    all_masks = np.zeros(image.shape[:2], dtype=np.int32)
    instance_id = 1

    for x, y in nucleus_points:
        # Point prompt
        point_coords = np.array([[x, y]])
        point_labels = np.array([1])  # Foreground

        # SAM prediction
        masks, scores, _ = predictor.predict(
            point_coords=point_coords,
            point_labels=point_labels,
            multimask_output=True
        )

        # Prendre le meilleur masque
        best_mask = masks[np.argmax(scores)]

        # Ajouter Ã  l'image labelisÃ©e
        # (gÃ©rer les chevauchements par prioritÃ©)
        overlap = (all_masks > 0) & best_mask
        if overlap.sum() / best_mask.sum() < 0.5:  # < 50% overlap
            all_masks[best_mask & (all_masks == 0)] = instance_id
            instance_id += 1

    return all_masks
```

---

## Composant 4: Gated Feature Fusion

### Justification

> **ProblÃ¨me:** La concatÃ©nation simple [visual; morpho] pondÃ¨re Ã©galement toutes les features.
> **Solution:** GFF apprend Ã  pondÃ©rer dynamiquement selon la classe.
> **Gain attendu:** +4-8% F1-score selon littÃ©rature.

### ImplÃ©mentation

```python
# src/models/gated_fusion.py

import torch
import torch.nn as nn

class GatedFeatureFusion(nn.Module):
    """
    Gated Feature Fusion pour fusion visual + morphomÃ©trique.

    g = Ïƒ(W_g Â· [f_visual; f_morpho] + b_g)
    f_fused = g âŠ™ f_visual + (1-g) âŠ™ f_morpho

    Le gate apprend Ã  pondÃ©rer diffÃ©remment selon le contexte:
    - SCC: priorise area/circularity (morpho)
    - HSIL: priorise texture nuclÃ©olaire (visual)
    """

    def __init__(self, visual_dim: int, morpho_dim: int, output_dim: int = None):
        super().__init__()

        if output_dim is None:
            output_dim = visual_dim

        # Projection des features vers mÃªme dimension
        self.visual_proj = nn.Linear(visual_dim, output_dim)
        self.morpho_proj = nn.Linear(morpho_dim, output_dim)

        # Gate network
        self.gate = nn.Sequential(
            nn.Linear(visual_dim + morpho_dim, output_dim),
            nn.Sigmoid()
        )

        # Output projection
        self.output_proj = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU()
        )

    def forward(self, f_visual: torch.Tensor, f_morpho: torch.Tensor) -> torch.Tensor:
        """
        Args:
            f_visual: (B, visual_dim) - Features du visual encoder
            f_morpho: (B, morpho_dim) - Features morphomÃ©triques

        Returns:
            f_fused: (B, output_dim) - Features fusionnÃ©es
        """
        # Projections
        v_proj = self.visual_proj(f_visual)
        m_proj = self.morpho_proj(f_morpho)

        # Gate
        concat = torch.cat([f_visual, f_morpho], dim=1)
        g = self.gate(concat)

        # Fusion pondÃ©rÃ©e
        f_fused = g * v_proj + (1 - g) * m_proj

        # Output
        return self.output_proj(f_fused)

    def get_gate_weights(self, f_visual: torch.Tensor, f_morpho: torch.Tensor) -> torch.Tensor:
        """Retourne les poids du gate pour analyse."""
        concat = torch.cat([f_visual, f_morpho], dim=1)
        return self.gate(concat)
```

### Analyse des Poids de Gate

```python
def analyze_gate_by_class(model, dataloader):
    """
    Analyse les poids de gate par classe Bethesda.

    Permet de comprendre quelles features le modÃ¨le privilÃ©gie
    pour chaque classe.
    """
    gate_weights_by_class = {c: [] for c in BETHESDA_CLASSES}

    for batch in dataloader:
        f_visual, f_morpho, labels = batch
        gates = model.fusion.get_gate_weights(f_visual, f_morpho)

        for i, label in enumerate(labels):
            class_name = BETHESDA_CLASSES[label]
            gate_weights_by_class[class_name].append(gates[i].cpu().numpy())

    # Moyenne par classe
    for class_name, weights in gate_weights_by_class.items():
        mean_gate = np.mean(weights, axis=0).mean()
        print(f"{class_name}: visual weight = {mean_gate:.2f}, morpho weight = {1-mean_gate:.2f}")
```

---

## Composant 5: Couche SÃ©curitÃ©

### Justification

> **Contexte clinique:** Un diagnostic cytologique erronÃ© peut avoir des consÃ©quences graves.
> **Objectif:** Le systÃ¨me doit savoir dire "je ne suis pas sÃ»r" et demander une review humaine.

### ImplÃ©mentation

```python
# src/safety/uncertainty.py

import torch
import numpy as np
from scipy.spatial.distance import mahalanobis

class SafetyLayer:
    """
    Couche de sÃ©curitÃ© pour classification cytologique.

    Composants:
    1. Temperature Scaling - Calibration des probabilitÃ©s
    2. Conformal Prediction - Intervalles de confiance
    3. OOD Detection - DÃ©tection hors distribution
    """

    def __init__(
        self,
        temperature: float = 1.0,
        conformal_alpha: float = 0.1,
        ood_threshold: float = 0.95
    ):
        self.temperature = temperature
        self.conformal_alpha = conformal_alpha
        self.ood_threshold = ood_threshold

        # Statistiques pour OOD (Ã  calculer sur train set)
        self.class_means = None
        self.class_covs = None
        self.global_cov_inv = None

    def calibrate_temperature(self, logits: np.ndarray, labels: np.ndarray) -> float:
        """
        Calibre la tempÃ©rature sur un validation set.

        Optimise NLL pour trouver T optimal.
        """
        from scipy.optimize import minimize

        def nll_loss(T):
            scaled_logits = logits / T
            probs = softmax(scaled_logits, axis=1)
            log_probs = np.log(probs[np.arange(len(labels)), labels] + 1e-10)
            return -log_probs.mean()

        result = minimize(nll_loss, x0=1.0, bounds=[(0.1, 10.0)])
        self.temperature = result.x[0]
        return self.temperature

    def fit_ood_detector(self, features: np.ndarray, labels: np.ndarray):
        """
        Fit le dÃ©tecteur OOD sur les features du train set.

        Calcule moyennes et covariances par classe pour Mahalanobis.
        """
        n_classes = len(np.unique(labels))

        self.class_means = []
        self.class_covs = []

        for c in range(n_classes):
            class_features = features[labels == c]
            self.class_means.append(class_features.mean(axis=0))
            self.class_covs.append(np.cov(class_features.T))

        # Covariance globale (tied covariance)
        global_cov = np.mean(self.class_covs, axis=0)
        self.global_cov_inv = np.linalg.inv(global_cov + 1e-6 * np.eye(global_cov.shape[0]))

    def compute_ood_score(self, features: np.ndarray) -> np.ndarray:
        """
        Calcule le score OOD via distance de Mahalanobis.

        Score Ã©levÃ© = potentiellement OOD.
        """
        scores = []

        for feat in features:
            min_dist = float('inf')
            for mean in self.class_means:
                dist = mahalanobis(feat, mean, self.global_cov_inv)
                min_dist = min(min_dist, dist)
            scores.append(min_dist)

        return np.array(scores)

    def predict_with_safety(
        self,
        logits: torch.Tensor,
        features: np.ndarray
    ) -> dict:
        """
        PrÃ©diction avec couche de sÃ©curitÃ©.

        Returns:
            dict: {
                'prediction': int,
                'confidence': float,
                'calibrated_probs': np.ndarray,
                'ood_score': float,
                'status': str  # 'reliable' / 'review' / 'ood'
            }
        """
        # Temperature scaling
        scaled_logits = logits / self.temperature
        probs = torch.softmax(scaled_logits, dim=-1).numpy()

        # PrÃ©diction
        pred = probs.argmax()
        confidence = probs.max()

        # OOD score
        ood_score = self.compute_ood_score(features.reshape(1, -1))[0]

        # DÃ©terminer statut
        if ood_score > self.ood_threshold:
            status = 'ood'
        elif confidence < 0.7:
            status = 'review'
        else:
            status = 'reliable'

        return {
            'prediction': int(pred),
            'confidence': float(confidence),
            'calibrated_probs': probs,
            'ood_score': float(ood_score),
            'status': status
        }
```

---

## Protocoles de Benchmark

### Protocole 1: Validation H-Optimus vs UNI vs Phikon

> **Objectif:** DÃ©montrer que H-Optimus n'est pas optimal pour cytologie Pap.

```bash
# Benchmark sur APCData (images rÃ©elles LBC)
python scripts/cytology/benchmark_encoders.py \
    --dataset apcdata \
    --encoders h-optimus,uni,phikon-v2 \
    --task bethesda_6class \
    --output_dir reports/encoder_benchmark
```

**MÃ©triques Ã  collecter:**

| MÃ©trique | H-Optimus | UNI | Phikon-v2 |
|----------|-----------|-----|-----------|
| Balanced Accuracy | ? | ? | ? |
| F1-score (macro) | ? | ? | ? |
| ASC-H Recall | ? | ? | ? |
| HSIL Recall | ? | ? | ? |
| Confidence calibration (ECE) | ? | ? | ? |

### Protocole 2: Validation Clusters HSIL/ASC-H

> **Objectif:** Mesurer la performance de segmentation sur noyaux chevauchants.

```bash
# Benchmark sur clusters annotÃ©s manuellement
python scripts/cytology/benchmark_segmentation.py \
    --dataset clusters_hsil \
    --methods stardist,hovernet-lite,cellpose \
    --metric dice,aji \
    --output_dir reports/segmentation_benchmark
```

### Protocole 3: Stain Normalization Impact

> **Objectif:** Mesurer l'impact de la normalisation sur la robustesse inter-labo.

```bash
# Benchmark avant/aprÃ¨s normalisation
python scripts/cytology/benchmark_stain_normalization.py \
    --source_lab labA \
    --target_lab labB \
    --normalizers macenko,reinhard,none \
    --output_dir reports/stain_benchmark
```

---

## Roadmap 12 Semaines

### Phase 1: Fondations (Semaines 1-4)

| Semaine | TÃ¢che | Livrable |
|---------|-------|----------|
| S1 | Stain Normalization Macenko | `src/preprocessing/stain_normalization.py` |
| S2 | Benchmark normalisation sur APCData | Rapport variance couleur |
| S3 | HoVerNet-lite architecture | `src/models/hovernet_lite.py` |
| S4 | GÃ©nÃ©ration pseudo-masques (SAM) | Dataset pseudo-annotÃ©s |

### Phase 2: Encoders & Segmentation (Semaines 5-8)

| Semaine | TÃ¢che | Livrable |
|---------|-------|----------|
| S5 | Benchmark H-Optimus vs UNI vs Phikon | Rapport comparatif |
| S6 | Fine-tuning UNI (LoRA) sur APCData | Checkpoint `uni_cytology.pth` |
| S7 | EntraÃ®nement HoVerNet-lite | Checkpoint `hovernet_lite_cytology.pth` |
| S8 | Validation segmentation clusters | Dice/AJI sur HSIL/ASC-H |

### Phase 3: Fusion & SÃ©curitÃ© (Semaines 9-12)

| Semaine | TÃ¢che | Livrable |
|---------|-------|----------|
| S9 | Gated Feature Fusion | `src/models/gated_fusion.py` |
| S10 | Temperature Scaling + calibration | Validation ECE < 0.05 |
| S11 | OOD Detection (Mahalanobis) | Seuils cliniques dÃ©finis |
| S12 | IntÃ©gration pipeline complet | `scripts/cytology/pipeline_v15.py` |

---

## KPIs de SuccÃ¨s

| Phase | KPI | Cible | PrioritÃ© |
|-------|-----|-------|----------|
| **P1** | Variance couleur inter-labo | RÃ©duction > 50% | ğŸŸ¡ |
| **P1** | Dice score clusters | > 0.80 | ğŸ”´ |
| **P2** | Balanced Accuracy (6 classes) | > 75% | ğŸ”´ |
| **P2** | ASC-H Recall | > 90% | ğŸ”´ |
| **P3** | ECE (calibration) | < 0.05 | ğŸŸ¡ |
| **P3** | OOD AUC-ROC | > 0.90 | ğŸŸ¡ |
| **Global** | Sensitivity abnormal | > 98% | ğŸ”´ CRITIQUE |

---

## Risques et Mitigations

| Risque | Impact | ProbabilitÃ© | Mitigation |
|--------|--------|-------------|------------|
| Pseudo-masques de mauvaise qualitÃ© | Ã‰levÃ© | Moyenne | Validation manuelle 10%, itÃ©ration |
| UNI pas meilleur que H-Optimus | Moyen | Faible | Fallback sur H-Optimus fine-tunÃ© |
| HoVerNet-lite s'effondre sur clusters | Ã‰levÃ© | Faible | Benchmark early, ajuster architecture |
| 12 semaines insuffisant | Moyen | Moyenne | Scope flexible Phase 3, prioritÃ©s claires |

---

*SpÃ©cification gÃ©nÃ©rÃ©e le 2026-01-22*
*Version: Draft 1.0*
