# V15.2 Cytology Architecture — Specification

> **Version:** 15.2-Lite
> **Date:** 2026-01-22
> **Statut:** ✅ CONSENSUS FINAL
> **Auteurs:** Equipe CellViT-Optimus + Expert Review
> **Timeline:** 12 semaines

---

## Stratégie Unifiée (Consensus Final)

> **Ce document représente le consensus final entre l'équipe interne et l'expert externe.**
> **Aucun aller-retour supplémentaire nécessaire.**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    STRATÉGIE V15.2 — CONSENSUS FINAL                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  PHASE 0: BENCHMARK ENCODER (7-10 jours) ◄── NOUVEAU                       │
│  ────────────────────────────────────────                                   │
│  • Encoders: H-Optimus / UNI / Phikon / ConvNeXt / ResNet50                │
│  • Méthode: Linear probe 5-fold CV sur APCData                             │
│  • Métrique: Balanced Accuracy (pas AUC)                                    │
│  • Décision: Data-driven, pas de dogme                                      │
│  • Règle fine-tuning: écart frozen vs fine-tuned >5% → Full FT, sinon LoRA │
│                                                                             │
│  PHASE 1: SEGMENTATION (3-4 semaines)                                      │
│  ─────────────────────────────────────                                      │
│  • HoVerNet-lite (clusters) + StarDist fallback (isolés)                   │
│  • YOLO detection → HoVerNet-lite segmentation                             │
│  • Domain randomization (couleur, luminosité)                              │
│                                                                             │
│  PHASE 2: ENCODER + FUSION (3-4 semaines)                                  │
│  ────────────────────────────────────────                                   │
│  • Encoder sélectionné par benchmark Phase 0                               │
│  • Si écart >5% → Full fine-tuning, sinon LoRA                             │
│  • Gated Feature Fusion (visual + morpho)                                  │
│                                                                             │
│  PHASE 3: SÉCURITÉ CLINIQUE (2-3 semaines)                                 │
│  ─────────────────────────────────────────                                  │
│  • PCA 128 dims → Mahalanobis (stabilité numérique)                        │
│  • Conformal Prediction (coverage 95%)                                      │
│  • OOD reject 98% en V1 (conservateur)                                     │
│  • Session calibration avec cytopathologistes                              │
│                                                                             │
│  PHASE 4: SLIDE-LEVEL (optionnel V2)                                       │
│  ───────────────────────────────────                                        │
│  • V1: Top-K anomalies                                                      │
│  • V2: Attention MIL                                                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Datasets (Stratégie Simplifiée — POC)

| Usage | Dataset | Cellules | Notes |
|-------|---------|----------|-------|
| **Training + Validation** | **APCData uniquement** | 3,619 | LBC, multi-cellules, Bethesda |

> **⚠️ DÉCISION POC:** APCData est le **seul dataset exploitable** pour V15.2 POC.
> Les autres datasets (SIPaKMeD, Herlev, CRICVA) sont exclus car:
> - Cellules isolées (≠ scénario production)
> - Pas d'annotations exploitables
> - Perte de temps pour le POC

### Ce qu'APCData permet (suffisant pour POC)

| Étape | Faisabilité | Source |
|-------|-------------|--------|
| YOLO détection | ✅ Direct | Bounding boxes fournies |
| Segmentation nucleus | ✅ Via pseudo-masques | Points nucleus → StarDist/circular approx |
| Extraction patch 224×224 | ✅ Direct | Bounding boxes |
| Feature visuel | ✅ Direct | Backbone frozen (pas de FT pour POC) |
| Morphométrie | ✅ Direct | Masques approximatifs suffisants |
| Classification Bethesda | ✅ Direct | Labels fournis |

### Ce qu'on N'AURA PAS (acceptable pour POC)

- ❌ HoVerNet pixel-perfect
- ❌ Performance optimale sur clusters serrés
- ❌ Généralisation production
- ❌ Robustesse scanner-to-scanner

**Objectif POC:** Démontrer l'architecture V15.2 fonctionne, pas performance optimale.

### Points de Consensus

| Point | Accord | Source |
|-------|--------|--------|
| HoVerNet-lite + StarDist fallback | ✅ Total | Équipe + Expert |
| Benchmark encoder obligatoire | ✅ Total | Équipe + Expert |
| Règle >5% écart → Full FT | ✅ Total | Littérature Stanford/Brigham |
| Conformal Prediction | ✅ Total | Équipe + Expert |
| PCA 128 dims avant Mahalanobis | ✅ Total | Stabilité numérique |
| CRICVA limité à image-level | ✅ Clarification | Documentation interne |

---

## Executive Summary

V15.2 est une refonte majeure du pipeline cytologie, passant d'une approche "foundation model brut" (V14) à une architecture **industrielle validée** combinant:

1. **Stain Normalization** — Robustesse inter-laboratoires
2. **YOLO Detection** — Localisation cellulaire rapide
3. **HoVerNet-lite** — Segmentation nucleus sur clusters
4. **Gated Feature Fusion** — Fusion adaptative visual + morpho
5. **Couche Sécurité** — Conformal Prediction + OOD

### Pourquoi V15.2?

| Limitation V14 | Solution V15.2 |
|----------------|----------------|
| H-Optimus jamais vu Pap-stain | Fine-tuning + benchmark UNI |
| 99% SIPaKMeD ≠ performance LBC | Validation sur APCData/LBC réels |
| CellPose inadapté clusters 3D | HoVerNet-lite avec HV maps |
| Concat simple features | Gated Feature Fusion |
| Pas de gestion incertitude | Conformal + OOD + Reject option |

---

## Architecture Globale

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         V15.2 CYTOLOGY PIPELINE                             │
└─────────────────────────────────────────────────────────────────────────────┘

                         IMAGE LBC / PAP SMEAR
                        (ex: 2048×1532 pixels)
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 0: STAIN NORMALIZATION (Macenko/Reinhard Pap)                        │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Normalisation couleur inter-laboratoires                                 │
│  • Correction illumination                                                  │
│  • Target: reference Pap standard                                           │
│  • Temps: ~50ms / image                                                     │
└─────────────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 1: DÉTECTION CELLULAIRE (YOLOv8/v9)                                  │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Input: Image normalisée                                                  │
│  • Output: N bounding boxes + confidence scores                             │
│  • Classes: {cell, debris, artifact}                                        │
│  • Temps: ~30-50ms / image                                                  │
│  • Note: YOLO = "cellular locator", pas segmentateur                        │
└─────────────────────────────────────────────────────────────────────────────┘
                              │
                              │ N crops cellulaires
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 2: SEGMENTATION NUCLEUS (HoVerNet-lite + StarDist fallback)          │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Stratégie hybride basée sur complexité estimée                           │
│  • StarDist: noyaux isolés (complexity < 0.3)                               │
│  • HoVerNet-lite: clusters 3D (complexity >= 0.3)                           │
│  • Output: Masques instance + centroïdes                                    │
│  • Temps: ~50-100ms / crop                                                  │
└─────────────────────────────────────────────────────────────────────────────┘
                              │
                              │ N masques nucleus
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 3: EXTRACTION PATCH 224×224 (centré sur nucleus)                     │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Crop centré sur centroïde du masque                                      │
│  • Padding blanc si proche du bord                                          │
│  • Conservation du masque pour morphométrie                                 │
└─────────────────────────────────────────────────────────────────────────────┘
                              │
              ┌───────────────┴───────────────┐
              ▼                               ▼
┌──────────────────────────────┐  ┌──────────────────────────────────────────┐
│   VISUAL ENCODER             │  │   MORPHOMÉTRIE AVANCÉE (20 features)     │
│                              │  │                                          │
│   ⚠️ SÉLECTION PAR BENCHMARK │  │   Base (10):                             │
│   (Phase 0 - Data-driven)    │  │   • area, perimeter, circularity         │
│                              │  │   • eccentricity, solidity, extent       │
│   Candidats:                 │  │   • major/minor axis, aspect ratio       │
│   • H-Optimus (baseline)     │  │   • compactness                          │
│   • UNI                      │  │                                          │
│   • Phikon-v2                │  │   Intensité H-channel (5):               │
│   • ConvNeXt-Base            │  │   • mean, std, max, min intensity        │
│   • ResNet50                 │  │   • integrated_od (proxy ploïdie)        │
│                              │  │                                          │
│   Output: 768-1536 dims      │  │   Texture GLCM (5):               │
│                              │  │   • mean, std, max, min intensity        │
│                              │  │   • integrated_od (proxy ploïdie)        │
│                              │  │                                          │
│                              │  │   Texture GLCM (5):                      │
│                              │  │   • contrast, homogeneity, energy        │
│                              │  │   • correlation, entropy                 │
│                              │  │                                          │
│                              │  │   Avancées Pap-spécifiques:              │
│                              │  │   • Variance texture nucléolaire         │
│                              │  │   • Polychromasie / gradient densité     │
│                              │  │   • N/C ratio (si cytoplasme segmenté)   │
└──────────────────────────────┘  └──────────────────────────────────────────┘
              │                               │
              └───────────────┬───────────────┘
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 4: GATED FEATURE FUSION (GFF)                                        │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Remplace concat simple                                                   │
│  • g = σ(W_g · [f_visual; f_morpho] + b_g)                                 │
│  • f_fused = g ⊙ f_visual + (1-g) ⊙ f_morpho                               │
│  • Pondération adaptative par classe                                        │
│  • Gain attendu: +4-8% F1-score                                             │
└─────────────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 5: CLASSIFICATION MLP                                                │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Input: f_fused (768-1556 dims selon encoder)                             │
│  • Architecture: Linear → BN → ReLU → Dropout (×3) → Linear(K)             │
│  • Classes Bethesda: NILM / ASC-US / ASC-H / LSIL / HSIL / SCC             │
│  • Output: logits + probabilités                                            │
└─────────────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 6: COUCHE SÉCURITÉ                                                   │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Conformal Prediction: intervalles de confiance calibrés                  │
│  • OOD Detection: distance Mahalanobis dans espace latent                   │
│  • Temperature Scaling: calibration des probabilités                        │
│  • Reject Option: seuil de confiance pour review humaine                    │
│  • Output: {Fiable / À revoir / Hors domaine}                              │
└─────────────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  ÉTAPE 7: AGRÉGATION SLIDE-LEVEL (V2)                                       │
│  ─────────────────────────────────────────────────────────────────────────  │
│  V1: Top-K anomalies pooling + heatmap simple                               │
│  V2: Attention-based MIL (CLAM / TransMIL)                                  │
│  • Output: Classification lame Bethesda + ROIs prioritaires                 │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Composant 1: Stain Normalization

### Justification

> **Problème:** Les images Pap/LBC varient énormément entre laboratoires (colorants, scanners, protocoles).
> **Impact:** Un modèle entraîné sur un labo peut s'effondrer sur un autre (-20-30% accuracy).

### Implémentation

```python
# src/preprocessing/stain_normalization.py

import numpy as np
from skimage import color

class MacenkoPapNormalizer:
    """
    Normalisation Macenko adaptée Pap-stain.

    Différences vs Macenko H&E:
    - Vecteurs stain différents (Papanicolaou vs H&E)
    - 3 colorants: Hématoxyline, Orange G, EA (vs 2 pour H&E)
    - Reference target calibré sur lames LBC standard
    """

    # Reference vectors pour Pap-stain (à calibrer sur dataset)
    PAP_REFERENCE = {
        'hematoxylin': np.array([0.65, 0.70, 0.29]),
        'orange_g': np.array([0.07, 0.99, 0.11]),
        'ea': np.array([0.27, 0.57, 0.78])
    }

    def __init__(self, target_image=None):
        """
        Args:
            target_image: Image de référence pour normalisation.
                         Si None, utilise PAP_REFERENCE.
        """
        if target_image is not None:
            self.target_stains = self._extract_stain_vectors(target_image)
        else:
            self.target_stains = self.PAP_REFERENCE

    def normalize(self, image: np.ndarray) -> np.ndarray:
        """
        Normalise une image Pap vers la référence.

        Args:
            image: Image RGB (H, W, 3), uint8

        Returns:
            normalized: Image normalisée, même shape
        """
        # 1. Conversion RGB → OD (Optical Density)
        od = self._rgb_to_od(image)

        # 2. Extraction vecteurs stain de l'image source
        source_stains = self._extract_stain_vectors(image)

        # 3. Déconvolution
        concentrations = self._deconvolve(od, source_stains)

        # 4. Reconvolution avec vecteurs cibles
        normalized_od = self._reconvolve(concentrations, self.target_stains)

        # 5. Conversion OD → RGB
        normalized = self._od_to_rgb(normalized_od)

        return normalized

    def _rgb_to_od(self, image: np.ndarray) -> np.ndarray:
        """Conversion RGB vers Optical Density."""
        image = image.astype(np.float32) / 255.0
        image = np.clip(image, 1e-6, 1.0)  # Évite log(0)
        od = -np.log(image)
        return od

    def _od_to_rgb(self, od: np.ndarray) -> np.ndarray:
        """Conversion Optical Density vers RGB."""
        rgb = np.exp(-od)
        rgb = np.clip(rgb * 255, 0, 255).astype(np.uint8)
        return rgb

    def _extract_stain_vectors(self, image: np.ndarray) -> dict:
        """
        Extrait les vecteurs stain via SVD.
        Adapté pour Pap-stain (3 composantes vs 2 pour H&E).
        """
        # Implémentation simplifiée - à affiner
        od = self._rgb_to_od(image)
        od_flat = od.reshape(-1, 3)

        # Filtrer pixels non-background
        od_mask = np.sum(od_flat, axis=1) > 0.15
        od_filtered = od_flat[od_mask]

        # SVD pour extraire composantes principales
        _, _, Vt = np.linalg.svd(od_filtered, full_matrices=False)

        # Les 2-3 premières composantes = vecteurs stain
        return {
            'hematoxylin': Vt[0],
            'orange_g': Vt[1] if len(Vt) > 1 else self.PAP_REFERENCE['orange_g'],
            'ea': Vt[2] if len(Vt) > 2 else self.PAP_REFERENCE['ea']
        }

    def _deconvolve(self, od, stain_vectors):
        """Déconvolution pour obtenir concentrations."""
        # TODO: Implémentation complète
        pass

    def _reconvolve(self, concentrations, target_stains):
        """Reconvolution avec vecteurs cibles."""
        # TODO: Implémentation complète
        pass
```

### Métriques de Validation

| Métrique | Calcul | Cible |
|----------|--------|-------|
| Variance couleur inter-labo | std(mean_color) avant/après | Réduction > 50% |
| Performance downstream | Accuracy classification | Pas de régression |
| Artefacts visuels | Inspection manuelle | Aucun |

---

## Composant 2: Détection YOLO

### Justification

> **Rôle:** YOLO = "cellular locator" rapide. Il détecte les cellules, pas les segmente.
> **Approche industrielle:** Techcyte, BD UroPath, Hologic Genius utilisent tous YOLO + segmentateur.

### Configuration

```python
# src/detection/yolo_detector.py

from ultralytics import YOLO

class CellDetector:
    """
    Détecteur de cellules basé sur YOLOv8.

    Classes:
    - 0: cell (noyau + cytoplasme)
    - 1: debris
    - 2: artifact
    """

    def __init__(self, model_path: str = "models/yolo/cell_detector.pt"):
        self.model = YOLO(model_path)
        self.conf_threshold = 0.5
        self.iou_threshold = 0.45

    def detect(self, image: np.ndarray) -> list[dict]:
        """
        Détecte les cellules dans une image.

        Args:
            image: Image RGB (H, W, 3)

        Returns:
            detections: Liste de {bbox, confidence, class}
        """
        results = self.model(
            image,
            conf=self.conf_threshold,
            iou=self.iou_threshold,
            verbose=False
        )[0]

        detections = []
        for box in results.boxes:
            if box.cls == 0:  # Classe "cell" uniquement
                detections.append({
                    'bbox': box.xyxy[0].cpu().numpy(),  # [x1, y1, x2, y2]
                    'confidence': box.conf.item(),
                    'class': 'cell'
                })

        return detections

    def extract_crops(self, image: np.ndarray, detections: list,
                      margin: int = 20) -> list[np.ndarray]:
        """
        Extrait les crops autour des détections.

        Args:
            image: Image source
            detections: Liste de détections YOLO
            margin: Marge autour de la bbox

        Returns:
            crops: Liste de crops RGB
        """
        crops = []
        for det in detections:
            x1, y1, x2, y2 = det['bbox'].astype(int)

            # Ajouter marge
            x1 = max(0, x1 - margin)
            y1 = max(0, y1 - margin)
            x2 = min(image.shape[1], x2 + margin)
            y2 = min(image.shape[0], y2 + margin)

            crop = image[y1:y2, x1:x2]
            crops.append(crop)

        return crops
```

### Entraînement YOLO

**Dataset requis:**
- APCData avec bounding boxes (déjà disponible)
- Annotations debris/artifacts (à créer)

**Commande d'entraînement:**
```bash
yolo train model=yolov8m.pt data=data/yolo/cell_detection.yaml epochs=100 imgsz=640
```

---

## Composant 3: HoVerNet-lite

### Justification

> **Problème:** Les clusters cellulaires (HSIL, ASC-H) ont des noyaux qui se chevauchent.
> **Solution:** HoVerNet utilise des HV maps (gradients horizontaux/verticaux) pour séparer les instances.
> **Avantage:** L'équipe a déjà l'expertise HoVer-Net (V13 histologie).

### Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         HOVERNET-LITE CYTOLOGIE                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ENCODER: ResNet18 (pretrained ImageNet)                                    │
│  ────────────────────────────────────────                                   │
│  • Conv1: 64 channels                                                       │
│  • Layer1: 64 channels, stride 1                                            │
│  • Layer2: 128 channels, stride 2                                           │
│  • Layer3: 256 channels, stride 2                                           │
│  • Layer4: 512 channels, stride 2                                           │
│                                                                             │
│  FPN: 3 niveaux (simplifié vs 5 niveaux V13)                               │
│  ─────────────────────────────────────────                                  │
│  • P3: 256 channels, 1/8 resolution                                         │
│  • P4: 256 channels, 1/16 resolution                                        │
│  • P5: 256 channels, 1/32 resolution                                        │
│                                                                             │
│  TÊTES DE PRÉDICTION:                                                       │
│  ────────────────────                                                       │
│  • NP (Nucleus Probability): Conv → Sigmoid → (H, W, 1)                    │
│  • HV (Horizontal-Vertical): Conv → Tanh → (H, W, 2)                       │
│  • [SUPPRIMÉ] NT (Nucleus Type): Non nécessaire pour cytologie              │
│                                                                             │
│  POST-PROCESSING: hv_guided_watershed() [Code V13 réutilisé]               │
│                                                                             │
│  STATS:                                                                     │
│  • Paramètres: ~2.5M (vs ~8M HoVerNet complet)                             │
│  • Inférence: ~50ms / crop 224×224                                          │
│  • VRAM: ~1.5 GB                                                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Implémentation

```python
# src/models/hovernet_lite.py

import torch
import torch.nn as nn
import torchvision.models as models

class HoVerNetLite(nn.Module):
    """
    HoVerNet allégé pour segmentation nucleus en cytologie.

    Différences vs HoVerNet V13:
    - Encoder: ResNet18 (vs ResNet50)
    - FPN: 3 niveaux (vs 5)
    - Pas de branche NT (classification noyaux)
    - Pas d'injection H-channel (Pap ≠ H&E)
    """

    def __init__(self, pretrained: bool = True):
        super().__init__()

        # Encoder ResNet18
        resnet = models.resnet18(pretrained=pretrained)
        self.conv1 = resnet.conv1
        self.bn1 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool
        self.layer1 = resnet.layer1  # 64 channels
        self.layer2 = resnet.layer2  # 128 channels
        self.layer3 = resnet.layer3  # 256 channels
        self.layer4 = resnet.layer4  # 512 channels

        # FPN 3 niveaux
        self.fpn = SimpleFPN(
            in_channels=[128, 256, 512],
            out_channels=256
        )

        # Tête NP (Nucleus Probability)
        self.np_head = nn.Sequential(
            nn.Conv2d(256, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )

        # Tête HV (Horizontal-Vertical gradients)
        self.hv_head = nn.Sequential(
            nn.Conv2d(256, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 2, 1),
            nn.Tanh()
        )

    def forward(self, x: torch.Tensor) -> dict:
        """
        Forward pass.

        Args:
            x: Input tensor (B, 3, H, W)

        Returns:
            dict: {'np': (B, 1, H, W), 'hv': (B, 2, H, W)}
        """
        # Encoder
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        c1 = self.layer1(x)   # 1/4
        c2 = self.layer2(c1)  # 1/8
        c3 = self.layer3(c2)  # 1/16
        c4 = self.layer4(c3)  # 1/32

        # FPN
        fpn_out = self.fpn([c2, c3, c4])  # (B, 256, H/4, W/4)

        # Upsample to input resolution
        fpn_out = nn.functional.interpolate(
            fpn_out, scale_factor=4, mode='bilinear', align_corners=False
        )

        # Predictions
        np_pred = self.np_head(fpn_out)
        hv_pred = self.hv_head(fpn_out)

        return {'np': np_pred, 'hv': hv_pred}


class SimpleFPN(nn.Module):
    """FPN simplifié 3 niveaux."""

    def __init__(self, in_channels: list, out_channels: int):
        super().__init__()

        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(c, out_channels, 1) for c in in_channels
        ])

        self.output_convs = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, 3, padding=1)
            for _ in in_channels
        ])

    def forward(self, features: list) -> torch.Tensor:
        """
        Args:
            features: [c2, c3, c4] from encoder

        Returns:
            Fused feature map at c2 resolution
        """
        # Lateral connections
        laterals = [conv(f) for conv, f in zip(self.lateral_convs, features)]

        # Top-down pathway
        for i in range(len(laterals) - 1, 0, -1):
            laterals[i-1] = laterals[i-1] + nn.functional.interpolate(
                laterals[i], scale_factor=2, mode='nearest'
            )

        # Output convolutions
        outputs = [conv(lat) for conv, lat in zip(self.output_convs, laterals)]

        # Return finest resolution
        return outputs[0]
```

### Stratégie Hybride (StarDist fallback)

```python
# src/segmentation/hybrid_segmenter.py

from stardist.models import StarDist2D
from src.models.hovernet_lite import HoVerNetLite
from src.postprocessing.watershed import hv_guided_watershed

class HybridNucleusSegmenter:
    """
    Segmentation hybride: StarDist pour noyaux isolés, HoVerNet-lite pour clusters.
    """

    def __init__(
        self,
        hovernet_checkpoint: str,
        stardist_model: str = '2D_versatile_fluo',
        complexity_threshold: float = 0.3
    ):
        self.hovernet = HoVerNetLite()
        self.hovernet.load_state_dict(torch.load(hovernet_checkpoint))
        self.hovernet.eval()

        self.stardist = StarDist2D.from_pretrained(stardist_model)
        self.complexity_threshold = complexity_threshold

    def segment(self, image: np.ndarray) -> np.ndarray:
        """
        Segmente les noyaux avec stratégie adaptative.

        Args:
            image: Image RGB (H, W, 3)

        Returns:
            masks: Image labelisée (H, W) avec instances
        """
        complexity = self._estimate_complexity(image)

        if complexity < self.complexity_threshold:
            # Noyaux isolés → StarDist (rapide)
            masks, _ = self.stardist.predict_instances(image)
        else:
            # Clusters → HoVerNet-lite (précis)
            masks = self._hovernet_segment(image)

        return masks

    def _estimate_complexity(self, image: np.ndarray) -> float:
        """
        Estime la complexité (présence de clusters).

        Basé sur:
        - Densité de noyaux estimée
        - Variance des intensités
        """
        # Quick pass avec StarDist
        masks_quick, details = self.stardist.predict_instances(
            image,
            prob_thresh=0.3,  # Seuil bas pour tout détecter
            nms_thresh=0.1    # NMS agressif
        )

        n_nuclei = masks_quick.max()
        area = image.shape[0] * image.shape[1]
        density = n_nuclei / area * 1e6  # Noyaux par million de pixels

        # Complexité basée sur densité
        if density > 500:
            return 0.8  # High complexity
        elif density > 200:
            return 0.5  # Medium complexity
        else:
            return 0.2  # Low complexity

    def _hovernet_segment(self, image: np.ndarray) -> np.ndarray:
        """Segmentation via HoVerNet-lite."""
        # Preprocessing
        tensor = self._preprocess(image)

        # Inference
        with torch.no_grad():
            outputs = self.hovernet(tensor)

        # Post-processing (réutilise code V13)
        np_pred = outputs['np'][0, 0].cpu().numpy()
        hv_pred = outputs['hv'][0].permute(1, 2, 0).cpu().numpy()

        masks = hv_guided_watershed(
            np_pred=np_pred,
            hv_pred=hv_pred,
            np_threshold=0.5,
            min_size=50,
            min_distance=3,
            beta=1.0
        )

        return masks
```

### Génération de Pseudo-Masques (pour entraînement)

> **Problème:** APCData n'a que des points nucleus, pas de masques pixel-level.
> **Solution:** Générer pseudo-masques via SAM + watershed.

```python
# scripts/cytology/generate_pseudo_masks.py

from segment_anything import SamPredictor, sam_model_registry
import numpy as np

def generate_pseudo_masks(image: np.ndarray, nucleus_points: list) -> np.ndarray:
    """
    Génère pseudo-masques à partir de points nucleus.

    Pipeline:
    1. SAM avec points comme prompts
    2. Watershed pour séparer instances qui se touchent
    3. Filtrage par taille

    Args:
        image: Image RGB
        nucleus_points: Liste de (x, y) pour chaque noyau

    Returns:
        masks: Image labelisée avec instances
    """
    # Init SAM
    sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b.pth")
    predictor = SamPredictor(sam)
    predictor.set_image(image)

    all_masks = np.zeros(image.shape[:2], dtype=np.int32)
    instance_id = 1

    for x, y in nucleus_points:
        # Point prompt
        point_coords = np.array([[x, y]])
        point_labels = np.array([1])  # Foreground

        # SAM prediction
        masks, scores, _ = predictor.predict(
            point_coords=point_coords,
            point_labels=point_labels,
            multimask_output=True
        )

        # Prendre le meilleur masque
        best_mask = masks[np.argmax(scores)]

        # Ajouter à l'image labelisée
        # (gérer les chevauchements par priorité)
        overlap = (all_masks > 0) & best_mask
        if overlap.sum() / best_mask.sum() < 0.5:  # < 50% overlap
            all_masks[best_mask & (all_masks == 0)] = instance_id
            instance_id += 1

    return all_masks
```

---

## Composant 4: Gated Feature Fusion

### Justification

> **Problème:** La concaténation simple [visual; morpho] pondère également toutes les features.
> **Solution:** GFF apprend à pondérer dynamiquement selon la classe.
> **Gain attendu:** +4-8% F1-score selon littérature.

### Implémentation

```python
# src/models/gated_fusion.py

import torch
import torch.nn as nn

class GatedFeatureFusion(nn.Module):
    """
    Gated Feature Fusion pour fusion visual + morphométrique.

    g = σ(W_g · [f_visual; f_morpho] + b_g)
    f_fused = g ⊙ f_visual + (1-g) ⊙ f_morpho

    Le gate apprend à pondérer différemment selon le contexte:
    - SCC: priorise area/circularity (morpho)
    - HSIL: priorise texture nucléolaire (visual)
    """

    def __init__(self, visual_dim: int, morpho_dim: int, output_dim: int = None):
        super().__init__()

        if output_dim is None:
            output_dim = visual_dim

        # Projection des features vers même dimension
        self.visual_proj = nn.Linear(visual_dim, output_dim)
        self.morpho_proj = nn.Linear(morpho_dim, output_dim)

        # Gate network
        self.gate = nn.Sequential(
            nn.Linear(visual_dim + morpho_dim, output_dim),
            nn.Sigmoid()
        )

        # Output projection
        self.output_proj = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU()
        )

    def forward(self, f_visual: torch.Tensor, f_morpho: torch.Tensor) -> torch.Tensor:
        """
        Args:
            f_visual: (B, visual_dim) - Features du visual encoder
            f_morpho: (B, morpho_dim) - Features morphométriques

        Returns:
            f_fused: (B, output_dim) - Features fusionnées
        """
        # Projections
        v_proj = self.visual_proj(f_visual)
        m_proj = self.morpho_proj(f_morpho)

        # Gate
        concat = torch.cat([f_visual, f_morpho], dim=1)
        g = self.gate(concat)

        # Fusion pondérée
        f_fused = g * v_proj + (1 - g) * m_proj

        # Output
        return self.output_proj(f_fused)

    def get_gate_weights(self, f_visual: torch.Tensor, f_morpho: torch.Tensor) -> torch.Tensor:
        """Retourne les poids du gate pour analyse."""
        concat = torch.cat([f_visual, f_morpho], dim=1)
        return self.gate(concat)
```

### Analyse des Poids de Gate

```python
def analyze_gate_by_class(model, dataloader):
    """
    Analyse les poids de gate par classe Bethesda.

    Permet de comprendre quelles features le modèle privilégie
    pour chaque classe.
    """
    gate_weights_by_class = {c: [] for c in BETHESDA_CLASSES}

    for batch in dataloader:
        f_visual, f_morpho, labels = batch
        gates = model.fusion.get_gate_weights(f_visual, f_morpho)

        for i, label in enumerate(labels):
            class_name = BETHESDA_CLASSES[label]
            gate_weights_by_class[class_name].append(gates[i].cpu().numpy())

    # Moyenne par classe
    for class_name, weights in gate_weights_by_class.items():
        mean_gate = np.mean(weights, axis=0).mean()
        print(f"{class_name}: visual weight = {mean_gate:.2f}, morpho weight = {1-mean_gate:.2f}")
```

---

## Composant 5: Couche Sécurité

### Justification

> **Contexte clinique:** Un diagnostic cytologique erroné peut avoir des conséquences graves.
> **Objectif:** Le système doit savoir dire "je ne suis pas sûr" et demander une review humaine.

### Architecture Sécurité (Consensus)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    COUCHE SÉCURITÉ V15.2 (CONSENSUS)                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. RÉDUCTION DIMENSIONNELLE (Stabilité numérique)                         │
│     • Embeddings 1024-1536 dims → PCA 128 dims                             │
│     • Évite matrice covariance mal conditionnée                            │
│     • Mahalanobis stable sur 128 dims                                      │
│                                                                             │
│  2. TEMPERATURE SCALING (Calibration)                                      │
│     • Optimise T sur validation set (NLL minimization)                     │
│     • Cible: ECE < 0.05                                                    │
│                                                                             │
│  3. CONFORMAL PREDICTION (Coverage garantie)                               │
│     • Prédit un SET de classes, pas une seule                              │
│     • Garantie: P(vraie classe ∈ set) ≥ 95%                               │
│     • Si set trop grand (>3 classes) → review humaine                      │
│                                                                             │
│  4. OOD DETECTION (Mahalanobis)                                            │
│     • Distance aux centroïdes de classe                                    │
│     • Seuil V1: reject 98% (conservateur)                                  │
│     • Affiné avec pathologistes                                            │
│                                                                             │
│  OUTPUTS:                                                                   │
│  • "Fiable" — Confiance haute, in-distribution                            │
│  • "À revoir" — Set conformal large OU confiance basse                    │
│  • "Hors domaine" — OOD détecté                                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Seuils V1 (Conservateurs)

| Classe | Sensibilité Min | Spécificité Min |
|--------|-----------------|-----------------|
| NILM | 98% | 85% |
| ASC-US | 95% | 90% |
| HSIL | **99.5%** | 95% |
| SCC | **100%** | 95% |
| OOD Reject | — | **98%** |

> **Philosophie V1:** Préférer sur-rejeter (review humaine) que rater un cancer.

### Implémentation

```python
# src/safety/uncertainty.py

import torch
import numpy as np
from scipy.spatial.distance import mahalanobis

class SafetyLayer:
    """
    Couche de sécurité pour classification cytologique.

    Composants:
    1. Temperature Scaling - Calibration des probabilités
    2. Conformal Prediction - Intervalles de confiance
    3. OOD Detection - Détection hors distribution
    """

    def __init__(
        self,
        temperature: float = 1.0,
        conformal_alpha: float = 0.1,
        ood_threshold: float = 0.95
    ):
        self.temperature = temperature
        self.conformal_alpha = conformal_alpha
        self.ood_threshold = ood_threshold

        # Statistiques pour OOD (à calculer sur train set)
        self.class_means = None
        self.class_covs = None
        self.global_cov_inv = None

    def calibrate_temperature(self, logits: np.ndarray, labels: np.ndarray) -> float:
        """
        Calibre la température sur un validation set.

        Optimise NLL pour trouver T optimal.
        """
        from scipy.optimize import minimize

        def nll_loss(T):
            scaled_logits = logits / T
            probs = softmax(scaled_logits, axis=1)
            log_probs = np.log(probs[np.arange(len(labels)), labels] + 1e-10)
            return -log_probs.mean()

        result = minimize(nll_loss, x0=1.0, bounds=[(0.1, 10.0)])
        self.temperature = result.x[0]
        return self.temperature

    def fit_ood_detector(self, features: np.ndarray, labels: np.ndarray):
        """
        Fit le détecteur OOD sur les features du train set.

        Calcule moyennes et covariances par classe pour Mahalanobis.
        """
        n_classes = len(np.unique(labels))

        self.class_means = []
        self.class_covs = []

        for c in range(n_classes):
            class_features = features[labels == c]
            self.class_means.append(class_features.mean(axis=0))
            self.class_covs.append(np.cov(class_features.T))

        # Covariance globale (tied covariance)
        global_cov = np.mean(self.class_covs, axis=0)
        self.global_cov_inv = np.linalg.inv(global_cov + 1e-6 * np.eye(global_cov.shape[0]))

    def compute_ood_score(self, features: np.ndarray) -> np.ndarray:
        """
        Calcule le score OOD via distance de Mahalanobis.

        Score élevé = potentiellement OOD.
        """
        scores = []

        for feat in features:
            min_dist = float('inf')
            for mean in self.class_means:
                dist = mahalanobis(feat, mean, self.global_cov_inv)
                min_dist = min(min_dist, dist)
            scores.append(min_dist)

        return np.array(scores)

    def predict_with_safety(
        self,
        logits: torch.Tensor,
        features: np.ndarray
    ) -> dict:
        """
        Prédiction avec couche de sécurité.

        Returns:
            dict: {
                'prediction': int,
                'confidence': float,
                'calibrated_probs': np.ndarray,
                'ood_score': float,
                'status': str  # 'reliable' / 'review' / 'ood'
            }
        """
        # Temperature scaling
        scaled_logits = logits / self.temperature
        probs = torch.softmax(scaled_logits, dim=-1).numpy()

        # Prédiction
        pred = probs.argmax()
        confidence = probs.max()

        # OOD score
        ood_score = self.compute_ood_score(features.reshape(1, -1))[0]

        # Déterminer statut
        if ood_score > self.ood_threshold:
            status = 'ood'
        elif confidence < 0.7:
            status = 'review'
        else:
            status = 'reliable'

        return {
            'prediction': int(pred),
            'confidence': float(confidence),
            'calibrated_probs': probs,
            'ood_score': float(ood_score),
            'status': status
        }
```

---

## Protocoles de Benchmark

### Protocole 0: Benchmark Encoder (PHASE 0 — OBLIGATOIRE)

> **Objectif:** Sélection data-driven de l'encoder, pas de dogme.
> **Durée:** 7-10 jours
> **Priorité:** 🔴 CRITIQUE — Doit être fait AVANT toute autre phase

```bash
# Benchmark sur APCData (images réelles LBC)
python scripts/cytology/benchmark_encoders.py \
    --dataset apcdata \
    --encoders h-optimus,uni,phikon-v2,convnext-base,resnet50 \
    --method linear_probe \
    --cv_folds 5 \
    --output_dir reports/encoder_benchmark
```

**Métriques à collecter:**

| Métrique | H-Optimus | UNI | Phikon-v2 | ConvNeXt | ResNet50 |
|----------|-----------|-----|-----------|----------|----------|
| Balanced Accuracy | ? | ? | ? | ? | ? |
| F1-score (macro) | ? | ? | ? | ? | ? |
| ASC-H Recall | ? | ? | ? | ? | ? |
| HSIL Recall | ? | ? | ? | ? | ? |
| ECE (calibration) | ? | ? | ? | ? | ? |

**Règle de décision:**
1. Sélectionner l'encoder avec **meilleure Balanced Accuracy**
2. Si écart entre frozen et fine-tuned > 5% → Full fine-tuning
3. Sinon → LoRA

**Attendus (basé sur littérature):**
- ResNet50: 70-80% (baseline)
- H-Optimus: 75-85%
- UNI: 78-88%
- Phikon: 80-90%
- ConvNeXt: 80-92%

### Protocole 2: Validation Clusters HSIL/ASC-H

> **Objectif:** Mesurer la performance de segmentation sur noyaux chevauchants.

```bash
# Benchmark sur clusters annotés manuellement
python scripts/cytology/benchmark_segmentation.py \
    --dataset clusters_hsil \
    --methods stardist,hovernet-lite,cellpose \
    --metric dice,aji \
    --output_dir reports/segmentation_benchmark
```

### Protocole 3: Stain Normalization Impact

> **Objectif:** Mesurer l'impact de la normalisation sur la robustesse inter-labo.

```bash
# Benchmark avant/après normalisation
python scripts/cytology/benchmark_stain_normalization.py \
    --source_lab labA \
    --target_lab labB \
    --normalizers macenko,reinhard,none \
    --output_dir reports/stain_benchmark
```

---

## Roadmap 12 Semaines (Mise à Jour Consensus)

### Phase 0: Benchmark Encoder (Semaine 1) ◄── NOUVEAU

> **Objectif:** Décision data-driven sur l'encoder, pas de dogme.

| Jour | Tâche | Livrable |
|------|-------|----------|
| J1-2 | Setup benchmark infrastructure | Script `benchmark_encoders.py` |
| J3-5 | Linear probe 5-fold CV | Résultats par encoder |
| J6-7 | Analyse + décision | Rapport + encoder sélectionné |

**Encoders testés:**
- H-Optimus (frozen) — Baseline V14
- UNI (frozen) — Généralisation large
- Phikon-v2 (frozen) — Robuste OOD
- ConvNeXt-Base (frozen) — Textures locales
- ResNet50 (frozen) — Baseline CNN

**Règle de décision fine-tuning:**
```
SI écart (frozen vs fine-tuned) > 5% → Full fine-tuning
SINON → LoRA
```

### Phase 1: Fondations (Semaines 2-4)

| Semaine | Tâche | Livrable |
|---------|-------|----------|
| S2 | Stain Normalization Macenko | `src/preprocessing/stain_normalization.py` |
| S3 | HoVerNet-lite architecture | `src/models/hovernet_lite.py` |
| S4 | Génération pseudo-masques (SAM) | Dataset pseudo-annotés |

### Phase 2: Segmentation & Encoder (Semaines 5-8)

| Semaine | Tâche | Livrable |
|---------|-------|----------|
| S5 | Entraînement HoVerNet-lite | Checkpoint `hovernet_lite_cytology.pth` |
| S6 | Validation segmentation clusters | Dice/AJI sur HSIL/ASC-H |
| S7 | Fine-tuning encoder (selon règle) | Checkpoint encoder cytology |
| S8 | Validation encoder fine-tuné | Rapport performance |

### Phase 3: Fusion & Sécurité (Semaines 9-12)

| Semaine | Tâche | Livrable |
|---------|-------|----------|
| S9 | Gated Feature Fusion | `src/models/gated_fusion.py` |
| S10 | Temperature Scaling + calibration | Validation ECE < 0.05 |
| S11 | OOD Detection (PCA 128 + Mahalanobis) | Seuils cliniques définis |
| S12 | Intégration pipeline complet | `scripts/cytology/pipeline_v15.py` |

### Phase 4: Slide-Level (Optionnel, Post-V1)

| Version | Approche | Priorité |
|---------|----------|----------|
| V1 | Top-K anomalies + heatmap simple | 🟡 Suffisant pour déploiement |
| V2 | Attention MIL (CLAM/TransMIL) | 🟢 Amélioration future |

---

## KPIs de Succès

| Phase | KPI | Cible | Priorité |
|-------|-----|-------|----------|
| **P1** | Variance couleur inter-labo | Réduction > 50% | 🟡 |
| **P1** | Dice score clusters | > 0.80 | 🔴 |
| **P2** | Balanced Accuracy (6 classes) | > 75% | 🔴 |
| **P2** | ASC-H Recall | > 90% | 🔴 |
| **P3** | ECE (calibration) | < 0.05 | 🟡 |
| **P3** | OOD AUC-ROC | > 0.90 | 🟡 |
| **Global** | Sensitivity abnormal | > 98% | 🔴 CRITIQUE |

---

## Risques et Mitigations

| Risque | Impact | Probabilité | Mitigation |
|--------|--------|-------------|------------|
| Pseudo-masques de mauvaise qualité | Élevé | Moyenne | Validation manuelle 10%, itération |
| UNI pas meilleur que H-Optimus | Moyen | Faible | Fallback sur H-Optimus fine-tuné |
| HoVerNet-lite s'effondre sur clusters | Élevé | Faible | Benchmark early, ajuster architecture |
| 12 semaines insuffisant | Moyen | Moyenne | Scope flexible Phase 3, priorités claires |

---

## Changelog

| Date | Version | Changements |
|------|---------|-------------|
| 2026-01-22 | 1.0 | Draft initial |
| 2026-01-22 | 2.0 | Consensus final — Ajout Phase 0 Benchmark, règle >5% fine-tuning, clarification CRICVA, PCA 128 dims |
| 2026-01-22 | **2.1** | **Simplification POC** — APCData uniquement, exclusion autres datasets |

---

*Spécification mise à jour le 2026-01-22*
*Version: 2.0 — CONSENSUS FINAL*
