# Plan de V√©rification M√©thodique : HoVer-Net vs Notre Syst√®me

**Date** : 2025-12-23
**Objectif** : Identifier EXACTEMENT pourquoi notre syst√®me (entra√Æn√© sur PanNuke) n'obtient pas les r√©sultats HoVer-Net baseline
**M√©thodologie** : Comparaison syst√©matique INPUT ‚Üí TRAINING ‚Üí MODEL ‚Üí OUTPUT

---

## ‚ùì Question Centrale

> **HoVer-Net original (Graham et al. 2019) entra√Æn√© sur PanNuke ‚Üí AJI ~0.68**
> **Notre syst√®me (HoVer-Net sur H-optimus-0) entra√Æn√© sur PanNuke ‚Üí AJI 0.0863**
>
> Diff√©rence : **8√ó pire** ‚Äî Pourquoi ?

---

## üîç √âtape 1 : V√©rifier les Donn√©es d'Entra√Ænement UTILIS√âES

### Objectif
D√©terminer **quelles donn√©es ont r√©ellement √©t√© utilis√©es** pour entra√Æner les mod√®les actuels.

### Tests √† R√©aliser

#### Test 1.1 : Chercher les donn√©es d'entra√Ænement existantes
```bash
# Chercher TOUS les fichiers de donn√©es par famille
find . -name "*glandular*" -o -name "*digestive*" -o -name "*urologic*" | grep -E "\.(npz|npy)$"

# V√©rifier les timestamps (quand ont-ils √©t√© cr√©√©s ?)
ls -lh --time-style=full-iso <fichiers_trouv√©s>
```

**Questions √† r√©pondre** :
- [ ] Des fichiers `*_data.npz` existent-ils ?
- [ ] Des fichiers `*_data_FIXED.npz` existent-ils ?
- [ ] Quelles sont leurs dates de cr√©ation ?
- [ ] Quelle est leur taille (coh√©rente avec nb d'√©chantillons attendus) ?

#### Test 1.2 : Inspecter le format des instances dans les donn√©es
```bash
# Script √† cr√©er : inspect_training_instances.py
# Charge un fichier .npz et affiche :
# - Nombre d'instances par image
# - Distribution des tailles d'instances
# - Exemple de inst_map avec IDs
```

**Questions √† r√©pondre** :
- [ ] Les inst_map contiennent-ils des IDs s√©quentiels (1, 2, 3...) ou des IDs PanNuke natifs ?
- [ ] Y a-t-il des instances fusionn√©es (grandes blobs) ou des instances s√©par√©es (petits blobs) ?
- [ ] Combien d'instances par image en moyenne ?

**Crit√®re de validation** :
- ‚úÖ Si inst_map a 50-100 instances/image ‚Üí Probablement FIXED (vraies instances)
- ‚ùå Si inst_map a 5-15 instances/image ‚Üí Probablement OLD (connectedComponents fusionn√©es)

---

## üîç √âtape 2 : Comparer Preprocessing PanNuke (HoVer-Net vs Nous)

### Objectif
V√©rifier si notre preprocessing PanNuke est **identique** √† HoVer-Net original.

### Tests √† R√©aliser

#### Test 2.1 : Lire le paper HoVer-Net (Graham et al. 2019)
- [ ] Section "Dataset" : Comment PanNuke est-il pr√©trait√© ?
- [ ] Utilisent-ils les IDs natifs ou connectedComponents ?
- [ ] Quelle est la distribution d'instances par image rapport√©e ?

**Ressource** :
- Paper : "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images"
- Lien probable : https://arxiv.org/abs/1812.06499

#### Test 2.2 : Comparer avec leur code officiel
- [ ] Repo GitHub : https://github.com/vqdang/hover_net
- [ ] Inspecter `process.py` ou √©quivalent : Comment extraient-ils les instances PanNuke ?
- [ ] Comparer avec notre `prepare_family_data.py` ligne par ligne

**Questions √† r√©pondre** :
- [ ] HoVer-Net utilise-t-il `connectedComponents` ou IDs natifs ?
- [ ] Quel est le format exact de leurs HV targets ?
- [ ] Y a-t-il des diff√©rences de normalisation [-1, 1] ?

**Crit√®re de validation** :
- ‚úÖ Si HoVer-Net utilise aussi connectedComponents ‚Üí Notre OLD data est correcte
- ‚ùå Si HoVer-Net utilise IDs natifs ‚Üí Nous devons utiliser FIXED data

---

## üîç √âtape 3 : Comparer Architecture Mod√®le

### Objectif
V√©rifier si notre d√©codeur HoVer-Net est **identique** √† l'original.

### Tests √† R√©aliser

#### Test 3.1 : Comparer architectures
```python
# Notre d√©codeur : src/models/hovernet_decoder.py
# HoVer-Net original : hover_net/models/hovernet/net_desc.py (repo GitHub)
```

**Questions √† r√©pondre** :
- [ ] Nombre de couches identique ?
- [ ] Skip connections identiques ?
- [ ] Fonctions d'activation (ReLU vs autre) ?
- [ ] Poids de loss (Œª_np, Œª_hv, Œª_nt) identiques ?

#### Test 3.2 : V√©rifier la loss function
```python
# HoVer-Net original : MSE pour HV, CrossEntropy pour NP/NT
# Notre impl√©mentation : SmoothL1Loss pour HV (depuis 2025-12-20)
```

**Diff√©rence critique identifi√©e** :
- HoVer-Net original : `MSE` pour HV
- Notre syst√®me : `SmoothL1Loss` pour HV (moins sensible aux outliers)

**Question** : Est-ce que SmoothL1Loss peut causer des gradients plus faibles ?

**Test √† faire** :
- [ ] Comparer MSE vs SmoothL1Loss sur un batch
- [ ] R√©-entra√Æner UNE famille avec MSE pour comparer

---

## üîç √âtape 4 : Comparer Post-Processing Watershed

### Objectif
V√©rifier si notre impl√©mentation watershed est **identique** √† HoVer-Net original.

### Tests √† R√©aliser

#### Test 4.1 : Comparer impl√©mentations watershed
```python
# HoVer-Net original : hover_net/infer/post_proc.py
# Notre syst√®me : src/inference/optimus_gate_inference_multifamily.py (m√©thode watershed)
```

**Questions √† r√©pondre** :
- [ ] M√™mes seuils `edge_threshold` ? (notre 0.3 vs leur ?)
- [ ] M√™mes seuils `dist_threshold` ? (notre 2 vs leur ?)
- [ ] M√™me algorithme de d√©tection de markers ?
- [ ] Utilisent-ils un pr√©-traitement des HV maps (smoothing, etc.) ?

#### Test 4.2 : Tester avec leurs param√®tres exacts
```bash
# Une fois les param√®tres HoVer-Net identifi√©s, tester sur nos donn√©es
python scripts/evaluation/test_watershed_params.py \
    --edge_threshold <valeur_hovernet> \
    --dist_threshold <valeur_hovernet>
```

---

## üîç √âtape 5 : Reproduire HoVer-Net Baseline sur PanNuke

### Objectif
**Preuve ultime** : Reproduire exactement les r√©sultats HoVer-Net paper.

### Tests √† R√©aliser

#### Test 5.1 : Utiliser le mod√®le HoVer-Net pr√©-entra√Æn√© officiel
```bash
# T√©l√©charger leur checkpoint pr√©-entra√Æn√©
# Source : https://github.com/vqdang/hover_net (releases)

# √âvaluer sur notre subset PanNuke fold2
python hover_net/run_infer.py \
    --checkpoint hovernet_pannuke_official.pth \
    --input_dir data/evaluation/pannuke_fold2_converted
```

**Questions √† r√©pondre** :
- [ ] Quel AJI obtiennent-ils sur fold2 ?
- [ ] Combien d'instances d√©tect√©es par image en moyenne ?
- [ ] Recall/Precision compar√©s aux n√¥tres ?

**Crit√®re de validation** :
- ‚úÖ Si leur mod√®le obtient aussi AJI ~0.09 sur fold2 ‚Üí Probl√®me dans les donn√©es GT
- ‚ùå Si leur mod√®le obtient AJI ~0.60-0.70 ‚Üí Notre impl√©mentation a un bug

---

## üìä Matrice de Diagnostic

| √âtape | Test | R√©sultat Attendu | Action si ‚ùå |
|-------|------|------------------|--------------|
| 1.1 | Donn√©es utilis√©es | Fichiers *_data.npz trouv√©s | G√©n√©rer donn√©es manquantes |
| 1.2 | Format instances | 50-100 inst/image (FIXED) | V√©rifier connectedComponents vs natif |
| 2.1 | Paper HoVer-Net | M√©thode extraction instances | Comparer avec notre script |
| 2.2 | Code GitHub HoVer-Net | Ligne par ligne identique | Corriger diff√©rences |
| 3.1 | Architecture | D√©codeur identique | Ajuster couches/activations |
| 3.2 | Loss function | MSE vs SmoothL1Loss | Tester avec MSE |
| 4.1 | Watershed params | Seuils identiques | Ajuster nos seuils |
| 5.1 | Mod√®le officiel | AJI ~0.60-0.70 | Reproduire leur pipeline |

---

## üéØ Crit√®res de D√©cision APR√àS Tests

### Sc√©nario A : Preprocessing Diff√©rent
**Si** : HoVer-Net utilise IDs natifs ET nous utilisons connectedComponents
**Action** : G√©n√©rer donn√©es FIXED + r√©-entra√Æner (10h)
**Gain estim√©** : AJI 0.09 ‚Üí 0.60-0.70

### Sc√©nario B : Architecture/Loss Diff√©rente
**Si** : SmoothL1Loss cause gradients faibles vs MSE
**Action** : R√©-entra√Æner UNE famille avec MSE (2h test)
**Gain estim√©** : AJI 0.09 ‚Üí 0.30-0.40 (si confirm√©)

### Sc√©nario C : Watershed Diff√©rent
**Si** : Param√®tres watershed tr√®s diff√©rents
**Action** : Ajuster param√®tres (1h)
**Gain estim√©** : AJI 0.09 ‚Üí 0.15-0.25

### Sc√©nario D : Combination
**Si** : Plusieurs diff√©rences identifi√©es
**Action** : Corriger dans l'ordre : Preprocessing ‚Üí Architecture ‚Üí Watershed
**Gain estim√©** : AJI 0.09 ‚Üí 0.70-0.80 (cumulatif)

---

## üìù Scripts √† Cr√©er

### Script 1 : `inspect_training_instances.py`
```python
"""Inspecte les instances dans les fichiers de donn√©es d'entra√Ænement."""
# Charge un .npz
# Affiche nombre d'instances par image
# Visualise quelques inst_map
```

### Script 2 : `compare_hovernet_preprocessing.py`
```python
"""Compare notre preprocessing avec HoVer-Net officiel."""
# Lit le m√™me batch PanNuke
# Applique les deux pipelines
# Compare les inst_map r√©sultants
```

### Script 3 : `test_loss_functions.py`
```python
"""Compare MSE vs SmoothL1Loss sur un batch."""
# Charge un batch
# Calcule loss avec les deux m√©thodes
# Compare magnitudes de gradients
```

---

## ‚è±Ô∏è Timeline Estim√©e

| √âtape | Temps | D√©pendances |
|-------|-------|-------------|
| 1. V√©rifier donn√©es | 30 min | Acc√®s filesystem |
| 2. Lire paper + code | 2h | Internet, lecture |
| 3. Comparer architecture | 1h | Code review |
| 4. Comparer watershed | 1h | Code review |
| 5. Tester mod√®le officiel | 1h | T√©l√©chargement checkpoint |
| **TOTAL** | **5.5h** | Avant toute d√©cision |

---

## ‚úÖ Checklist de Validation

Avant de proposer TOUTE solution, v√©rifier :

- [ ] **√âtape 1 compl√®te** : Nous savons quelles donn√©es ont √©t√© utilis√©es
- [ ] **√âtape 2 compl√®te** : Nous avons compar√© avec HoVer-Net preprocessing
- [ ] **√âtape 3 compl√®te** : Nous avons compar√© architecture et loss
- [ ] **√âtape 4 compl√®te** : Nous avons compar√© watershed
- [ ] **√âtape 5 compl√®te** : Nous avons test√© leur mod√®le officiel
- [ ] **Rapport √©crit** : Diff√©rences identifi√©es document√©es
- [ ] **Consensus** : Solution valid√©e avec l'utilisateur

---

## üö´ INTERDICTIONS

- ‚ùå NE PAS g√©n√©rer de nouvelles donn√©es avant √âtape 1 et 2
- ‚ùå NE PAS r√©-entra√Æner avant √âtape 3
- ‚ùå NE PAS modifier watershed avant √âtape 4
- ‚ùå NE PAS proposer de solution avant √âtape 5

**Principe** : COMPRENDRE avant AGIR
